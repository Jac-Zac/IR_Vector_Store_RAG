{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store Implementation\n",
    "\n",
    "This is an implementation of a vector store that can leverage embedding models to create our vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "- Numpy\n",
    "- Pands\n",
    "\n",
    "##### Using files from:\n",
    "\n",
    "http://ir.dcs.gla.ac.uk/resources/test_collections/time/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index (Helper class)\n",
    "\n",
    "Implementation of an Helper class index which is going to be used in my vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index:\n",
    "    def __init__(self, dim=None):\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Dictionary to store the vectors\n",
    "        self.stored_vectors = {}\n",
    "\n",
    "    def add_items(self, vectors, vectors_id: int):\n",
    "        \"\"\"\n",
    "        Update the indexing structure for the vector store\n",
    "        \"\"\"\n",
    "        for vector_id, vector in zip(vectors_id, vectors):\n",
    "            if vector.shape != (self.dim,):\n",
    "                raise ValueError(\"Vectors must have shape (dim,)\")\n",
    "            self.stored_vectors[vector_id] = vector\n",
    "\n",
    "    def knn_query(self, query_vector: np.ndarray, top_n: int = 5):\n",
    "        \"\"\"\n",
    "        Find the top n similar vectors to the query vector using cosine similarity.\n",
    "\n",
    "        Args:\n",
    "            query_vector (numpy.ndarray): The query vector.\n",
    "            top_n (int): The number of top similar vectors to return.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of two numpy arrays: the first array contains the indices of the top n similar vectors,\n",
    "            and the second array contains the corresponding cosine similarity scores.\n",
    "        \"\"\"\n",
    "        similarities = [(index, self._cosine_similarity(query_vector, vector)) for index, vector in self.stored_vectors.items()]\n",
    "\n",
    "        # Sort based on the similarity (second element of the vector) and take the first top_n elements\n",
    "        # Then unpack it into indices and distances\n",
    "        top_n_indices, top_n_similarities = zip(*sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n])\n",
    "\n",
    "        return top_n_indices, top_n_similarities\n",
    "        \n",
    "    def _cosine_similarity(self, query_vector, vector) -> float:\n",
    "        \"\"\"\n",
    "        Compute the similarity between two vectors\n",
    "\n",
    "        Args:\n",
    "            query_vector (numpy.ndarray): The query vector\n",
    "            vector (numpy.ndarray): The vector to compare\n",
    "\n",
    "        Returns:\n",
    "            The dot product of the vectors, normalized by the product of their norms\n",
    "        \"\"\"\n",
    "\n",
    "        dot_product = np.dot(query_vector, vector)\n",
    "        \n",
    "        query_vector_norm = np.linalg.norm(query_vector)\n",
    "        vector_norm = np.linalg.norm(vector)\n",
    "\n",
    "        # Return the similarity\n",
    "        return dot_product / (query_vector_norm * vector_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, vector_dimension=None, persist=True, persist_path=\"vector_store\"):\n",
    "\n",
    "        if vector_dimension is None:\n",
    "            raise ValueError(\"You should pass the vector size\")\n",
    "\n",
    "        # Initialize our index our index\n",
    "        self.index = Index(dim=vector_dimension)\n",
    "        self.persist = persist\n",
    "        self.persist_path = persist_path\n",
    "        self.vector_dimension = vector_dimension\n",
    "    \n",
    "        # Counter to then store the ids of vectors\n",
    "        self.id_counter = 0\n",
    "        \n",
    "        # Dictionary to store sentences corresponding to vectors\n",
    "        self.sentences = {}\n",
    "\n",
    "    def _load_vector_store(self):\n",
    "        index_file = os.path.join(self.persist_path, \"index.pkl\")\n",
    "        sentences_file = os.path.join(self.persist_path, \"sentences.pkl\")\n",
    "        \n",
    "        if not os.path.exists(index_file) or not os.path.exists(sentences_file):\n",
    "            raise FileNotFoundError(\"Index and sentences files not found in the specified directory.\")\n",
    "\n",
    "        with open(index_file, \"rb\") as f:\n",
    "            self.index = pickle.load(f)\n",
    "        with open(sentences_file, \"rb\") as f:\n",
    "            self.sentences = pickle.load(f)\n",
    "\n",
    "        return self.index, self.sentences\n",
    "\n",
    "    def save_vector_store(self):\n",
    "        # Save the index and corresponding sentences\n",
    "        \n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(self.persist_path, exist_ok=True)\n",
    "        \n",
    "        # Serialize and save the index\n",
    "        with open(os.path.join(self.persist_path, \"index.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.index, f)\n",
    "\n",
    "        # Serialize and save the sentences\n",
    "        with open(os.path.join(self.persist_path, \"sentences.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.sentences, f)\n",
    "            \n",
    "    def create_vector_store(self, new_sentence_vectors):\n",
    "        \"\"\"\n",
    "        Add vectors to the vector store\n",
    "\n",
    "        id: the unique id for the vector\n",
    "        vecotor: the vector to be added\n",
    "        \"\"\"\n",
    "        try:\n",
    "            vectors = []\n",
    "            ids = []\n",
    "            for sentence, vector in new_sentence_vectors.items():\n",
    "                # Append the new vector\n",
    "                vectors.append(vector)\n",
    "                # Assign a unique integer id to every vector\n",
    "                ids.append(self.id_counter)\n",
    "                # Store the sentence\n",
    "                self.sentences[self.id_counter] = sentence\n",
    "                # Increment the counter for the next vector\n",
    "                self.id_counter += 1\n",
    "\n",
    "            # Adding the items to the index\n",
    "            self.index.add_items(vectors, ids)\n",
    "    \n",
    "            if self.persist:                \n",
    "                self.save_vector_store()\n",
    "                \n",
    "            print(\"Vector store created successfully\", end=\"\\n\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def update_vector_store(self, new_sentence_vectors):\n",
    "        \"\"\"\n",
    "        Update the existing vector store with new vectors\n",
    "\n",
    "        new_id_vectors: Dictionary containing new vectors to be added\n",
    "        persist_path: Path to the directory where the existing vector store is saved\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load existing index and sentences\n",
    "            self.index, self.sentences = self._load_vector_store()\n",
    "\n",
    "            # Update the id counter\n",
    "            self.id_counter = max(self.sentences.keys()) + 1\n",
    "\n",
    "            # Add new vectors to the index and sentences\n",
    "            vectors = []\n",
    "            ids = []\n",
    "            for sentence, vector in new_sentence_vectors.items():\n",
    "                vectors.append(vector)\n",
    "                ids.append(self.id_counter)\n",
    "                self.sentences[self.id_counter] = sentence\n",
    "                self.id_counter += 1\n",
    "                \n",
    "            # Adding the vectors, index to the our index\n",
    "            self.index.add_items(vectors, ids)\n",
    "            \n",
    "            print(\"Vector store updated successfully\", end=\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def delete_vector_store(self) -> None:\n",
    "        \"\"\"\n",
    "        Delete a persistent vector store that was craeted\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Check if the directory exists\n",
    "            if os.path.exists(self.persist_path):\n",
    "                # Delete index and sentences files\n",
    "                os.remove(os.path.join(self.persist_path, \"index.pkl\"))\n",
    "                os.remove(os.path.join(self.persist_path, \"sentences.pkl\"))\n",
    "                print(\"Vector store deleted successfully\", end=\"\\n\\n\")\n",
    "            else:\n",
    "                print(\"Vector store does not exist\", end=\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def get_similar_vectors(self, query_vector, top_n=5) -> list:\n",
    "        \"\"\"\n",
    "        Find similar vectors to the query vector\n",
    "\n",
    "        Args:\n",
    "            query_vector (numpy.ndarray): The query vector to compare with the vecotr in the store\n",
    "            num_results (int): The number of similar vectors to return\n",
    "\n",
    "        Returns:\n",
    "            A list of tuples, each containing a vector id and its similarity to the query vector\n",
    "        \"\"\"\n",
    "        if self.persist:\n",
    "            # Load existing index and sentences\n",
    "            self._load_vector_store()\n",
    "        \n",
    "        result = []\n",
    "        labels, distances = self.index.knn_query(query_vector, top_n=top_n)\n",
    "\n",
    "        similar_vectors = [(self.sentences[label], distance) for label, distance in zip(labels, distances)]\n",
    "        \n",
    "        return similar_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo of The vector store\n",
    "\n",
    "Using nomic embed for the demo and a custom index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextSplitter and Tetriver classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'termcolor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Pritty print\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtermcolor\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTextSplitter\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_path, split_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'termcolor'"
     ]
    }
   ],
   "source": [
    "# Pritty print\n",
    "import termcolor\n",
    "\n",
    "class TextSplitter:\n",
    "    def __init__(self, data_path, split_ratio=0.8, random_state=200):\n",
    "        self.data = pd.read_csv(data_path, delimiter=\";\")\n",
    "        self.data_1 = self.data.sample(frac=split_ratio, random_state=random_state)\n",
    "        self.data_2 = self.data.drop(self.data_1.index)\n",
    "\n",
    "    def get_data_split(self):\n",
    "        return self.data_1[\"text\"].tolist(), self.data_2[\"text\"].tolist()\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self, model_name=\"nomic-ai/nomic-embed-text-v1.5\", persist=True, persist_path=\"test\"):\n",
    "        self.model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "        self.vector_store = VectorStore(persist=persist, persist_path=persist_path)\n",
    "\n",
    "    def create_vector_store(self, data):\n",
    "        vectors = self.model.encode(data)\n",
    "        vector_dimension = len(vectors[0])\n",
    "        new_sentence_vectors = {data[i]: vectors[i] for i in range(len(data))}\n",
    "        self.vector_store.create_vector_store(new_sentence_vectors)\n",
    "\n",
    "    def update_vector_store_data(self, data):\n",
    "        vectors = self.model.encode(data)\n",
    "        new_sentence_vectors = {data[i]: vectors[i] for i in range(len(data))}\n",
    "        self.vector_store.update_vector_store(new_sentence_vectors)\n",
    "        return new_sentence_vectors\n",
    "\n",
    "    def query_similar_vectors(self, query, top_n=5):\n",
    "        query_vector = self.model.encode(query)\n",
    "        similar_vectors = self.vector_store.get_similar_vectors(query_vector, top_n=top_n)\n",
    "        return similar_vectors\n",
    "\n",
    "    def print_similar_vectors(self, similar_vectors):\n",
    "        print(\"Similarity Vectors:\")\n",
    "        for sentence, similarity_score in similar_vectors:\n",
    "            print(termcolor.colored(f\"- Sentence: {sentence}\", \"green\", \"on_grey\", [\"bold\"]))\n",
    "            print(termcolor.colored(f\"  Similarity Score: {similarity_score}\", \"yellow\", \"on_grey\", [\"bold\"]))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TextSplitter(\"data.csv\")\n",
    "data_1, data_2 = text_splitter.get_data_split()\n",
    "\n",
    "retriever = Retriever()\n",
    "retriever.create_vector_store(data_1)\n",
    "\n",
    "query = \"I want to buy a car\"\n",
    "similar_vectors = retriever.query_similar_vectors(query)\n",
    "\n",
    "retriever.print_similar_vectors(similar_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding update of the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the vector store\n",
    "data_2 = data_2[\"text\"].tolist()\n",
    "new_sentence_vectors_2 = retriever.update_vector_store_data(data_2)\n",
    "\n",
    "# Query the vector store\n",
    "query = \"I want to buy a cycle\"\n",
    "query_vector = retriever.model.encode(query)\n",
    "similar_vectors = retriever.query_similar_vectors(query_vector, top_n=5)\n",
    "retriever.print_similar_vectors(similar_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding persistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete saved vector store\n",
    "retriver.vector_store.delete_vector_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the system on a set of test queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old way of using it\n",
    "\n",
    "Might be helpfull as a look up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VectorStore instance\n",
    "vector_store = VectorStore()\n",
    "\n",
    "# Define your sentences\n",
    "sentences = [\n",
    "    \"I eat mango\",\n",
    "    \"mango is my favorite fruit\",\n",
    "    \"mango, apple, oranges are fruits\",\n",
    "    \"fruits are good for health\",\n",
    "]\n",
    "\n",
    "# Tokenization and Vocabulary Creation\n",
    "vocabulary = set()\n",
    "for sentence in sentences:\n",
    "    tokens = sentence.lower().split()\n",
    "    vocabulary.update(tokens)\n",
    "\n",
    "# Assign unique indices to words in the vocabulary\n",
    "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "\n",
    "# Vectorization\n",
    "sentence_vectors = {}\n",
    "for sentence in sentences:\n",
    "    tokens = sentence.lower().split()\n",
    "    vector = np.zeros(len(vocabulary))\n",
    "    for token in tokens:\n",
    "        vector[word_to_index[token]] += 1\n",
    "    sentence_vectors[sentence] = vector\n",
    "\n",
    "# Storing in VectorStore\n",
    "for sentence, vector in sentence_vectors.items():\n",
    "    vector_store.add_vector(sentence, vector)\n",
    "\n",
    "# Searching for Similarity\n",
    "query_sentence = \"Mango is the best fruit\"\n",
    "query_vector = np.zeros(len(vocabulary))\n",
    "query_tokens = query_sentence.lower().split()\n",
    "for token in query_tokens:\n",
    "    if token in word_to_index:\n",
    "        query_vector[word_to_index[token]] += 1\n",
    "\n",
    "similar_sentences = vector_store.find_similar_vectors(query_vector, num_results=2)\n",
    "\n",
    "# Print similar sentences\n",
    "print(\"Query Sentence:\", query_sentence)\n",
    "print(\"Similar Sentences:\")\n",
    "for sentence, similarity in similar_sentences:\n",
    "    print(f\"{sentence}: Similarity = {similarity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv-kernel",
   "language": "python",
   "name": "local-venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "139e7c84632f54486abb9d698f2a5412a324e85ce1b1331ea63d3255168fb27f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
