{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store Implementation\n",
    "\n",
    "The following code is an implementation made for the final exam of the Information Retrival course.\n",
    "\n",
    "**Author:** *Jacopo Zacchigna*\n",
    "\n",
    "---\n",
    "\n",
    "The notebook is an implementation of a vector store.\n",
    "The code is structured in multiple class:\n",
    "\n",
    "- Index\n",
    "- VectorStore\n",
    "\n",
    "And there is also the implementation of additional classes that are helpfull to load the data and retrive interesting informations:\n",
    "\n",
    "- TextSplitter\n",
    "- Retriever\n",
    "\n",
    "---\n",
    "\n",
    "##### The text for the demo is from:\n",
    "\n",
    "http://ir.dcs.gla.ac.uk/resources/test_collections/time/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Imports external libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Pritty print\n",
    "import termcolor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index (Helper class)\n",
    "\n",
    "Implementation of an Helper class index which is going to be used in my vector Store\n",
    "\n",
    "- **Add items:** to add all of the vectors with the relative indices to the stored_vectors dictionary\n",
    "- **knn_query:** to get the `top_n` most similar vectors inside a vector store with the relative\n",
    "- **_cosine_similarity:** helper function to compute the cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index:\n",
    "    def __init__(self, dim=None):\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Dictionary to store the vectors\n",
    "        self.stored_vectors = {}\n",
    "\n",
    "    def add_items(self, vectors, vectors_id: int):\n",
    "        \"\"\"\n",
    "        Update the indexing structure for the vector store\n",
    "        \"\"\"\n",
    "        for vector_id, vector in zip(vectors_id, vectors):\n",
    "            if vector.shape != (self.dim,):\n",
    "                raise ValueError(\"Vectors must have shape (dim,)\")\n",
    "            self.stored_vectors[vector_id] = vector\n",
    "\n",
    "    def knn_query(self, query_vector: np.ndarray, top_n: int = 5):\n",
    "        \"\"\"\n",
    "        Find the top n similar vectors to the query vector using cosine similarity.\n",
    "\n",
    "        Args:\n",
    "            query_vector (numpy.ndarray): The query vector.\n",
    "            top_n (int): The number of top similar vectors to return.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of two numpy arrays: the first array contains the indices of the top n similar vectors,\n",
    "            and the second array contains the corresponding cosine similarity scores.\n",
    "        \"\"\"\n",
    "        similarities = [(index, self._cosine_similarity(query_vector, vector)) for index, vector in self.stored_vectors.items()]\n",
    "\n",
    "        # Sort based on the similarity (second element of the vector) and take the first top_n elements\n",
    "        # Then unpack it into indices and distances\n",
    "        top_n_indices, top_n_similarities = zip(*sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n])\n",
    "\n",
    "        return top_n_indices, top_n_similarities\n",
    "        \n",
    "    def _cosine_similarity(self, query_vector, vector) -> float:\n",
    "        \"\"\"\n",
    "        Compute the similarity between two vectors\n",
    "\n",
    "        Args:\n",
    "            query_vector (numpy.ndarray): The query vector\n",
    "            vector (numpy.ndarray): The vector to compare\n",
    "\n",
    "        Returns:\n",
    "            The dot product of the vectors, normalized by the product of their norms\n",
    "        \"\"\"\n",
    "\n",
    "        dot_product = np.dot(query_vector, vector)\n",
    "        \n",
    "        query_vector_norm = np.linalg.norm(query_vector)\n",
    "        vector_norm = np.linalg.norm(vector)\n",
    "\n",
    "        # Return the similarity\n",
    "        return dot_product / (query_vector_norm * vector_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store class\n",
    "\n",
    "This is the main part of the code which implements the vector store.\n",
    "\n",
    "*It focueses on implementing the following function with some additional functionality and some basic error handling:*\n",
    "\n",
    "- **_load_vector_store:** loads the index and sentences\n",
    "\n",
    "- **save_vector_store:** saves the index and sentences to the specified directory\n",
    "\n",
    "- **create_vector_store:** adds vectors to the vector store\n",
    "\n",
    "- **update_vector_store:** updates the existing vector store with new vectors\n",
    "\n",
    "- **delete_vector_store:** deletes a persistent vector store\n",
    "\n",
    "- **get_similar_vectors:** finds similar vectors to the query vector based on cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, model_name=\"nomic-ai/nomic-embed-text-v1.5\", persist=True, persist_path=\"vector_store\"):\n",
    "        self.persist = persist\n",
    "        self.persist_path = persist_path\n",
    "        \n",
    "        # Initialize our index our index\n",
    "        self.model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "        \n",
    "        # Counter to then store the ids of vectors\n",
    "        self.id_counter = 0\n",
    "\n",
    "        # Dictionary to store sentences corresponding to vectors\n",
    "        self.sentences = {}\n",
    "\n",
    "    # Add type hinting for documents\n",
    "    @classmethod\n",
    "    def from_documents(cls, documents, model_name=\"nomic-ai/nomic-embed-text-v1.5\", persist=True, persist_path=\"vector_store\"):\n",
    "        \"\"\"\n",
    "        From documents create the vector store using a defined model, with optional persistence\n",
    "        \"\"\"\n",
    "        # Create an instance of the class\n",
    "        vector_store = cls(model_name, persist, persist_path)\n",
    "\n",
    "        # Get the embeddings\n",
    "        embeddings = vector_store.model.encode(documents)\n",
    "        vector_store.embeddings_dimension = len(embeddings[0])\n",
    "\n",
    "        # Create the index\n",
    "        vector_store.index = Index(dim=vector_store.embeddings_dimension)\n",
    "\n",
    "        \n",
    "        # Create a dictionary with the documents and the relative embeddings\n",
    "        new_documents_embeddings = {documents[i]: embeddings[i] for i in range(len(documents))}\n",
    "\n",
    "        # Create the vector store\n",
    "        vector_store.create_vector_store(new_documents_embeddings)\n",
    "\n",
    "        return vector_store\n",
    "\n",
    "    def _load_vector_store(self):\n",
    "        index_file = os.path.join(self.persist_path, \"index.pkl\")\n",
    "        sentences_file = os.path.join(self.persist_path, \"sentences.pkl\")\n",
    "\n",
    "        if not os.path.exists(index_file) or not os.path.exists(sentences_file):\n",
    "            raise FileNotFoundError(\"Index and sentences files not found in the specified directory.\")\n",
    "\n",
    "        with open(index_file, \"rb\") as f:\n",
    "            self.index = pickle.load(f)\n",
    "        with open(sentences_file, \"rb\") as f:\n",
    "            self.sentences = pickle.load(f)\n",
    "\n",
    "        return self.index, self.sentences\n",
    "\n",
    "    def _save_vector_store(self):\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(self.persist_path, exist_ok=True)\n",
    "\n",
    "        # Serialize and save the index\n",
    "        with open(os.path.join(self.persist_path, \"index.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.index, f)\n",
    "\n",
    "        # Serialize and save the sentences\n",
    "        with open(os.path.join(self.persist_path, \"sentences.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.sentences, f)\n",
    "\n",
    "    def create_vector_store(self, new_documents_embeddings):\n",
    "        try:\n",
    "            vectors = []\n",
    "            ids = []\n",
    "            for sentence, vector in new_documents_embeddings.items():\n",
    "                # Append the new vector\n",
    "                vectors.append(vector)\n",
    "                # Assign a unique integer id to every vector\n",
    "                ids.append(self.id_counter)\n",
    "                # Store the sentence\n",
    "                self.sentences[self.id_counter] = sentence\n",
    "                # Increment the counter for the next vector\n",
    "                self.id_counter += 1\n",
    "\n",
    "            # Adding the items to the index\n",
    "            self.index.add_items(vectors, ids)\n",
    "\n",
    "            if self.persist:\n",
    "                self._save_vector_store()\n",
    "\n",
    "            print(\"Vector store created successfully\", end=\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def update_vector_store(self, documents):\n",
    "        \"\"\"\n",
    "        Update the existing vector store with new documents\n",
    "\n",
    "        documents: List of documents to add to my vector store\n",
    "        \"\"\"\n",
    "        embeddings = self.model.encode(documents)\n",
    "        new_documents_embeddings = {documents[i]: embeddings[i] for i in range(len(documents))}\n",
    "\n",
    "        try:\n",
    "            # Load existing index and sentences\n",
    "            self.index, self.sentences = self._load_vector_store()\n",
    "\n",
    "            # Update the id counter\n",
    "            self.id_counter = max(self.sentences.keys()) + 1\n",
    "\n",
    "            # Add new vectors to the index and sentences\n",
    "            vectors = []\n",
    "            ids = []\n",
    "            for sentence, vector in new_documents_embeddings.items():\n",
    "                vectors.append(vector)\n",
    "                ids.append(self.id_counter)\n",
    "                self.sentences[self.id_counter] = sentence\n",
    "                self.id_counter += 1\n",
    "\n",
    "            # Adding the vectors, index to the our index\n",
    "            self.index.add_items(vectors, ids)\n",
    "\n",
    "            print(\"Vector store updated successfully\", end=\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def delete_vector_store(self) -> None:\n",
    "        \"\"\"\n",
    "        Delete a persistent vector store that was craeted\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if the directory exists\n",
    "            if os.path.exists(self.persist_path):\n",
    "                # Delete index and sentences files\n",
    "                os.remove(os.path.join(self.persist_path, \"index.pkl\"))\n",
    "                os.remove(os.path.join(self.persist_path, \"sentences.pkl\"))\n",
    "                print(\"Vector store deleted successfully\", end=\"\\n\\n\")\n",
    "            else:\n",
    "                print(\"Vector store does not exist\", end=\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def query_similar_vectors(self, query: str, top_n=5):\n",
    "        \"\"\"\n",
    "        Find similar vectors to the query\n",
    "\n",
    "        Args:\n",
    "            query (str): The query that is going to be searched for inside my vector store\n",
    "            num_results (int): The number of similar vectors to return\n",
    "\n",
    "        Returns:\n",
    "            A list of tuples, each containing a document and its similarity to the query vector\n",
    "        \"\"\"\n",
    "        if self.persist:\n",
    "            # Load existing index and sentences\n",
    "            self._load_vector_store()\n",
    "\n",
    "        # Use the same model to encode the query\n",
    "        query_vector = self.model.encode(query)\n",
    "        \n",
    "        # Querry for the top_n most similar vectors to my querry vector\n",
    "        labels, distances = self.index.knn_query(query_vector, top_n=top_n)\n",
    "\n",
    "        # Return the most similar documents in a list of tuples with (sentence, similarity_score)\n",
    "        return [(self.sentences[label], distance) for label, distance in zip(labels, distances)]\n",
    "        \n",
    "    def print_similar_vectors(self, similar_vectors) -> None:\n",
    "        \"\"\"\n",
    "        Helper function to print the most similar vector with the relative similarity score in a nice way\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Similarity Vectors:\")\n",
    "        for sentence, similarity_score in similar_vectors:\n",
    "            print(termcolor.colored(f\"- Sentence: {sentence}\", \"green\", \"on_grey\", [\"bold\"]))\n",
    "            print(termcolor.colored(f\"  Similarity Score: {similarity_score}\", \"yellow\", \"on_grey\", [\"bold\"]))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo of The vector store\n",
    "\n",
    "Using nomic embed for the demo and a custom index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextSplitter and Retriver class\n",
    "\n",
    "Implementation of the text splitter with different options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLoader:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load the data from a CSV file\n",
    "        return pd.read_csv(self.data_path, delimiter=\";\")\n",
    "\n",
    "    def split_data(self, split_ratio=0.8, random_state=1337):\n",
    "        # Load the data\n",
    "        data = self.load_data()\n",
    "\n",
    "        # Split them in two splits\n",
    "        data_1 = data.sample(frac=split_ratio, random_state=random_state)\n",
    "        data_2 = data.drop(data_1.index)\n",
    "        \n",
    "        return data_1[\"text\"].tolist(), data_2[\"text\"].tolist()\n",
    "\n",
    "class CharacterTextSplitter:\n",
    "    def __init__(self, chunk_size=100, chunk_overlap=0):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def split_documents(self, raw_documents):\n",
    "        self.data = raw_documents\n",
    "        self.split_text()\n",
    "\n",
    "    \"\"\"\n",
    "    def split_text(self):\n",
    "        self.chunks = (text[i:i+self.chunk_size] for i in range(0, len(self.data), self.chunk_size))\n",
    "        self.chunks = (chunk for chunk in self.chunks if len(chunk) > self.chunk_overlap)\n",
    "    \"\"\"\n",
    "\n",
    "    def split_text(self):\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created successfully\n",
      "\n",
      "Similarity Vectors:\n",
      "\u001b[1m\u001b[40m\u001b[32m- Sentence: A vintage car gleamed in the sunlight, its polished chrome catching the eye of passersby.\u001b[0m\n",
      "\u001b[1m\u001b[40m\u001b[33m  Similarity Score: 0.5708912014961243\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[40m\u001b[32m- Sentence: The sleek, silver sports car raced down the winding mountain road, its engine roaring with power.\u001b[0m\n",
      "\u001b[1m\u001b[40m\u001b[33m  Similarity Score: 0.5229447484016418\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[40m\u001b[32m- Sentence: A family sedan cruised along the highway, its occupants singing along to their favorite songs on the radio.\u001b[0m\n",
      "\u001b[1m\u001b[40m\u001b[33m  Similarity Score: 0.5017635822296143\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[40m\u001b[32m- Sentence: The Harley-Davidson motorcycle rumbled to life, its deep, throaty growl announcing its presence on the road.\u001b[0m\n",
      "\u001b[1m\u001b[40m\u001b[33m  Similarity Score: 0.4918859899044037\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[40m\u001b[32m- Sentence: The sound of engines filled the air, a symphony of power and speed that echoed through the streets.\u001b[0m\n",
      "\u001b[1m\u001b[40m\u001b[33m  Similarity Score: 0.4788406491279602\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the raw text\n",
    "# Split to then use it to update the vector store\n",
    "data_1, data_2 = TextLoader('data.csv').split_data()\n",
    "\n",
    "# Craete a tecxt splitter\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "# Split the documents recursivly\n",
    "# documents_1 = text_splitter.split_documents(data_1)\n",
    "\n",
    "# Create the vecotr store from documents\n",
    "db = VectorStore.from_documents(data_1, model_name=\"nomic-ai/nomic-embed-text-v1.5\")\n",
    "\n",
    "# Define a querry and searcah for it in my vector stor\n",
    "query = \"I want to buy a car\"\n",
    "similar_vectors = db.query_similar_vectors(query)\n",
    "\n",
    "db.print_similar_vectors(similar_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding update of the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store updated successfully\n",
      "\n",
      "Similarity Vectors:\n",
      "\u001b[1m\u001b[40m\u001b[32m- Sentence: And as the sun rises once again, the cycle begins anew, a testament to the beauty and resilience of life.\u001b[0m\n",
      "\u001b[1m\u001b[40m\u001b[33m  Similarity Score: 0.5324117541313171\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[40m\u001b[32m- Sentence: The Harley-Davidson motorcycle rumbled to life, its deep, throaty growl announcing its presence on the road.\u001b[0m\n",
      "\u001b[1m\u001b[40m\u001b[33m  Similarity Score: 0.521259605884552\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[40m\u001b[32m- Sentence: A vintage car gleamed in the sunlight, its polished chrome catching the eye of passersby.\u001b[0m\n",
      "\u001b[1m\u001b[40m\u001b[33m  Similarity Score: 0.4940513074398041\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update the vector store\n",
    "db.update_vector_store(data_2)\n",
    "\n",
    "# Query the vector store\n",
    "query = \"I want to buy a cycle\"\n",
    "\n",
    "similar_vectors = db.query_similar_vectors(query, top_n=3)\n",
    "db.print_similar_vectors(similar_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding persistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store deleted successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Delete saved vector store\n",
    "db.delete_vector_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the system on a set of test queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv-kernel",
   "language": "python",
   "name": "local-venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "139e7c84632f54486abb9d698f2a5412a324e85ce1b1331ea63d3255168fb27f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
