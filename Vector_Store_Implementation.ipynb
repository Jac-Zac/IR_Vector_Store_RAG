{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store Implementation\n",
    "\n",
    "The following code is an implementation made for the final exam of the Information Retrival course.\n",
    "\n",
    "**Author:** *Jacopo Zacchigna*\n",
    "\n",
    "---\n",
    "\n",
    "The notebook is an implementation of a vector store.\n",
    "The code is structured in multiple class:\n",
    "\n",
    "- Index\n",
    "- VectorStore\n",
    "\n",
    "And there is also the implementation of additional classes that are helpfull to load the data and retrive interesting informations:\n",
    "\n",
    "- TextLoader\n",
    "- DirectoryReader\n",
    "- TextSplitter\n",
    "\n",
    "---\n",
    "\n",
    "##### The text for the RAG demo is taken from:\n",
    "\n",
    "- Grokking Paper: https://arxiv.org/pdf/2201.02177.pdf\n",
    "- Attention Is All You Need: https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Imports external libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Index (Helper class)\n",
    "\n",
    "Implementation of an Helper class index which is going to be used in my vector Store\n",
    "\n",
    "- **Add items:** to add all of the vectors with the relative indices to the stored_vectors dictionary\n",
    "- **knn_query:** to get the `top_n` most similar vectors inside a vector store with the relative\n",
    "- **_cosine_similarity:** helper function to compute the cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index:\n",
    "    def __init__(self, dim=None):\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Dictionary to store the vectors\n",
    "        self.stored_vectors = {}\n",
    "\n",
    "    def add_items(self, vectors, vectors_id: int):\n",
    "        \"\"\"\n",
    "        Update the indexing structure for the vector store\n",
    "        \"\"\"\n",
    "        for vector_id, vector in zip(vectors_id, vectors):\n",
    "            if vector.shape != (self.dim,):\n",
    "                raise ValueError(\"Vectors must have shape (dim,)\")\n",
    "            self.stored_vectors[vector_id] = vector\n",
    "\n",
    "    def knn_query(self, query_vector: np.ndarray, top_n: int = 5):\n",
    "        \"\"\"\n",
    "        Find the top n similar vectors to the query vector using cosine similarity.\n",
    "\n",
    "        Args:\n",
    "            query_vector (numpy.ndarray): The query vector.\n",
    "            top_n (int): The number of top similar vectors to return.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of two numpy arrays: the first array contains the indices of the top n similar vectors,\n",
    "            and the second array contains the corresponding cosine similarity scores.\n",
    "        \"\"\"\n",
    "        similarities = [(index, self._cosine_similarity(query_vector, vector)) for index, vector in self.stored_vectors.items()]\n",
    "\n",
    "        # Sort based on the similarity (second element of the vector) and take the first top_n elements\n",
    "        # Then unpack it into indices and distances\n",
    "        top_n_indices, top_n_similarities = zip(*sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n])\n",
    "\n",
    "        return top_n_indices, top_n_similarities\n",
    "        \n",
    "    def _cosine_similarity(self, query_vector, vector) -> float:\n",
    "        \"\"\"\n",
    "        Compute the similarity between two vectors\n",
    "\n",
    "        Args:\n",
    "            query_vector (numpy.ndarray): The query vector\n",
    "            vector (numpy.ndarray): The vector to compare\n",
    "\n",
    "        Returns:\n",
    "            The dot product of the vectors, normalized by the product of their norms\n",
    "        \"\"\"\n",
    "\n",
    "        dot_product = np.dot(query_vector, vector)\n",
    "        \n",
    "        query_vector_norm = np.linalg.norm(query_vector)\n",
    "        vector_norm = np.linalg.norm(vector)\n",
    "\n",
    "        # Return the similarity\n",
    "        return dot_product / (query_vector_norm * vector_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Vector Store class\n",
    "\n",
    "This is the main part of the code which implements the vector store.\n",
    "\n",
    "*It focueses on implementing the following function with some additional functionality and some basic error handling:*\n",
    "\n",
    "- **_load_vector_store:** loads the index and sentences\n",
    "\n",
    "- **save_vector_store:** saves the index and sentences to the specified directory\n",
    "\n",
    "- **create_vector_store:** adds vectors to the vector store\n",
    "\n",
    "- **update_vector_store:** updates the existing vector store with new vectors\n",
    "\n",
    "- **delete_vector_store:** deletes a persistent vector store\n",
    "\n",
    "- **get_similar_vectors:** finds similar vectors to the query vector based on cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, model_name=\"nomic-ai/nomic-embed-text-v1.5\", persist=True, persist_path=\"vector_store\"):\n",
    "        self.persist = persist\n",
    "        self.persist_path = persist_path\n",
    "        \n",
    "        # Initialize our index our index\n",
    "        self.model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "        \n",
    "        # Counter to then store the ids of vectors\n",
    "        self.id_counter = 0\n",
    "\n",
    "        # Dictionary to store sentences corresponding to vectors\n",
    "        self.sentences = {}\n",
    "\n",
    "    def _load_vector_store(self):\n",
    "        index_file = os.path.join(self.persist_path, \"index.pkl\")\n",
    "        sentences_file = os.path.join(self.persist_path, \"sentences.pkl\")\n",
    "\n",
    "        if not os.path.exists(index_file) or not os.path.exists(sentences_file):\n",
    "            raise FileNotFoundError(\"Index and sentences files not found in the specified directory.\")\n",
    "\n",
    "        with open(index_file, \"rb\") as f:\n",
    "            self.index = pickle.load(f)\n",
    "        with open(sentences_file, \"rb\") as f:\n",
    "            self.sentences = pickle.load(f)\n",
    "\n",
    "        return self.index, self.sentences\n",
    "\n",
    "    def _save_vector_store(self):\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(self.persist_path, exist_ok=True)\n",
    "\n",
    "        # Serialize and save the index\n",
    "        with open(os.path.join(self.persist_path, \"index.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.index, f)\n",
    "\n",
    "        # Serialize and save the sentences\n",
    "        with open(os.path.join(self.persist_path, \"sentences.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.sentences, f)\n",
    "\n",
    "    def create_vector_store(self, documents):\n",
    "        # Get the embeddings\n",
    "        embeddings = self.model.encode(documents)\n",
    "        self.embeddings_dimension = len(embeddings[0])\n",
    "\n",
    "        # Create the index\n",
    "        self.index = Index(dim=self.embeddings_dimension)\n",
    "\n",
    "        # Create a dictionary with the documents and the relative embeddings\n",
    "        new_documents_embeddings = {documents[i]: embeddings[i] for i in range(len(documents))}\n",
    "        \n",
    "        try:\n",
    "            vectors = []\n",
    "            ids = []\n",
    "            \n",
    "            for sentence, vector in new_documents_embeddings.items():\n",
    "                # Append the new vector\n",
    "                vectors.append(vector)\n",
    "                # Assign a unique integer id to every vector\n",
    "                ids.append(self.id_counter)\n",
    "                # Store the sentence\n",
    "                self.sentences[self.id_counter] = sentence\n",
    "                # Increment the counter for the next vector\n",
    "                self.id_counter += 1\n",
    "                \n",
    "            # Adding the items to the index\n",
    "            self.index.add_items(vectors, ids)\n",
    "\n",
    "            if self.persist:\n",
    "                self._save_vector_store()\n",
    "\n",
    "            print(\"\\033[32mVector store created successfully\\033[0m\", end=\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def update_vector_store(self, documents):\n",
    "        \"\"\"\n",
    "        Update the existing vector store with new documents\n",
    "\n",
    "        documents: List of documents to add to my vector store\n",
    "        \"\"\"\n",
    "        embeddings = self.model.encode(documents)\n",
    "        new_documents_embeddings = {documents[i]: embeddings[i] for i in range(len(documents))}\n",
    "\n",
    "        try:\n",
    "            # Load existing index and sentences\n",
    "            self.index, self.sentences = self._load_vector_store()\n",
    "\n",
    "            # Update the id counter\n",
    "            self.id_counter = max(self.sentences.keys()) + 1\n",
    "\n",
    "            # Add new vectors to the index and sentences\n",
    "            vectors = []\n",
    "            ids = []\n",
    "            for sentence, vector in new_documents_embeddings.items():\n",
    "                vectors.append(vector)\n",
    "                ids.append(self.id_counter)\n",
    "                self.sentences[self.id_counter] = sentence\n",
    "                self.id_counter += 1\n",
    "\n",
    "            # Adding the vectors, index to the our index\n",
    "            self.index.add_items(vectors, ids)\n",
    "\n",
    "            print(\"\\033[32mVector store updated successfully\\033[0m\", end=\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def delete_vector_store(self) -> None:\n",
    "        \"\"\"\n",
    "        Delete a persistent vector store that was craeted\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if the directory exists\n",
    "            if os.path.exists(self.persist_path):\n",
    "                # Delete index and sentences files\n",
    "                os.remove(os.path.join(self.persist_path, \"index.pkl\"))\n",
    "                os.remove(os.path.join(self.persist_path, \"sentences.pkl\"))\n",
    "                os.rmdir(self.persist_path)\n",
    "                print(\"\\033[32mVector store deleted successfully\\033[0m\", end=\"\\n\\n\")\n",
    "            else:\n",
    "                print(\"Vector store does not exist\", end=\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def query_similar_vectors(self, query: str, top_n=5):\n",
    "        \"\"\"\n",
    "        Find similar vectors to the query\n",
    "\n",
    "        Args:\n",
    "            query (str): The query that is going to be searched for inside my vector store\n",
    "            num_results (int): The number of similar vectors to return\n",
    "\n",
    "        Returns:\n",
    "            A list of tuples, each containing a document and its similarity to the query vector\n",
    "        \"\"\"\n",
    "        if self.persist:\n",
    "            # Load existing index and sentences\n",
    "            self._load_vector_store()\n",
    "\n",
    "        # Use the same model to encode the query\n",
    "        query_vector = self.model.encode(query)\n",
    "        \n",
    "        # Querry for the top_n most similar vectors to my querry vector\n",
    "        labels, distances = self.index.knn_query(query_vector, top_n=top_n)\n",
    "\n",
    "        # Return the most similar documents in a list of tuples with (sentence, similarity_score)\n",
    "        return [(self.sentences[label], distance) for label, distance in zip(labels, distances)]\n",
    "        \n",
    "    def print_similar_vectors(self, similar_vectors) -> None:\n",
    "        \"\"\"\n",
    "        Helper function to print the most similar vector with the relative similarity score in a nice way\n",
    "        \"\"\"\n",
    "        print(\"\\033[1mSimilar Text Retrived:\\033[0m\")\n",
    "        print(\"___________________________________\\n\")\n",
    "        for sentence, similarity_score in similar_vectors:\n",
    "            print(\"\\033[1m- Retrieved Text:\\033[0m\", sentence)\n",
    "            print(\"\\033[1m    Similarity Score:\\033[0m\", similarity_score)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo of The vector store\n",
    "\n",
    "Using nomic embed for the demo and a custom index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### TextSplitter and Retriver class\n",
    "\n",
    "Implementation of the text splitter with different options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLoader:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load the data from a CSV file\n",
    "        return pd.read_csv(self.data_path, delimiter=\";\")\n",
    "\n",
    "    def split_data(self, split_ratio=0.8, random_state=1337):\n",
    "        # Load the data\n",
    "        data = self.load_data()\n",
    "\n",
    "        # Split them in two splits\n",
    "        data_1 = data.sample(frac=split_ratio, random_state=random_state)\n",
    "        data_2 = data.drop(data_1.index)\n",
    "        \n",
    "        return data_1[\"text\"].tolist(), data_2[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store Creatoion\n",
    "\n",
    "Creation of the vector store, and simple query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mVector store created successfully\u001b[0m\n",
      "\n",
      "\u001b[1mSimilar Text Retrived:\u001b[0m\n",
      "___________________________________\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m A vintage car gleamed in the sunlight, its polished chrome catching the eye of passersby.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.5708912\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m The sleek, silver sports car raced down the winding mountain road, its engine roaring with power.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.52294475\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m A family sedan cruised along the highway, its occupants singing along to their favorite songs on the radio.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.5017636\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m The Harley-Davidson motorcycle rumbled to life, its deep, throaty growl announcing its presence on the road.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.491886\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m The sound of engines filled the air, a symphony of power and speed that echoed through the streets.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.47884065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the raw text\n",
    "# Split to then use it to update the vector store\n",
    "data_1, data_2 = TextLoader('data/sample.csv').split_data()\n",
    "\n",
    "# Create the vecotr store from documents\n",
    "db = VectorStore(model_name=\"nomic-ai/nomic-embed-text-v1.5\", persist=True, persist_path=\"demo\")\n",
    "\n",
    "# Create the vector store\n",
    "db.create_vector_store(data_1)\n",
    "\n",
    "# Define a querry and searcah for it in my vector stor\n",
    "query = \"I want to buy a car\"\n",
    "similar_vectors = db.query_similar_vectors(query)\n",
    "\n",
    "db.print_similar_vectors(similar_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showcase The Vector Store Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mVector store updated successfully\u001b[0m\n",
      "\n",
      "\u001b[1mSimilar Text Retrived:\u001b[0m\n",
      "___________________________________\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m And as the sun rises once again, the cycle begins anew, a testament to the beauty and resilience of life.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.53241175\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m The Harley-Davidson motorcycle rumbled to life, its deep, throaty growl announcing its presence on the road.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.5212596\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m A vintage car gleamed in the sunlight, its polished chrome catching the eye of passersby.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.4940513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update the vector store\n",
    "db.update_vector_store(data_2)\n",
    "\n",
    "# Query the vector store\n",
    "query = \"I want to buy a cycle\"\n",
    "\n",
    "similar_vectors = db.query_similar_vectors(query, top_n=3)\n",
    "db.print_similar_vectors(similar_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mVector store deleted successfully\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Delete saved vector store\n",
    "db.delete_vector_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showcase RAG Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DirectoryReader, TextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectoryReader:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        \n",
    "    def load_data(self):\n",
    "        # List all files in the data directory\n",
    "        files = os.listdir(self.data_path)\n",
    "    \n",
    "        # Read the contents of each file\n",
    "        text = ''\n",
    "        for file in files:\n",
    "            file_path = os.path.join(self.data_path, file)\n",
    "            if file.endswith('.pdf'):\n",
    "                with open(file_path, 'rb') as f:  # Open the file in binary mode\n",
    "                    pdf = PdfReader(f)  # Create a PdfReader object\n",
    "                    for page in pdf.pages:\n",
    "                        text += page.extract_text()\n",
    "            elif file.endswith('.txt'):\n",
    "                with open(file_path, 'r') as txt_file:  # Open the file in text mode\n",
    "                    text += txt_file.read()\n",
    "            else:\n",
    "                print(f\"File type not supported for: {file}\")\n",
    "    \n",
    "        return text\n",
    "        \n",
    "class CharacterSplitter:\n",
    "    def __init__(self, chunk_size=100, chunk_overlap=0):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def split_documents(self, raw_documents):\n",
    "        # Get the rappresentation for every charcter\n",
    "        self.data = list(raw_documents)\n",
    "        self.split_text()\n",
    "        return self.chunks\n",
    "\n",
    "    def split_text(self):\n",
    "        self.chunks = []\n",
    "        chunk_size = self.chunk_size - self.chunk_overlap  # Adjust chunk size to account for overlap\n",
    "    \n",
    "        for i in range(0, len(self.data), chunk_size):\n",
    "            # Ensure the chunk size doesn't exceed the length of the data\n",
    "            if i + self.chunk_size > len(self.data):\n",
    "                chunk = self.data[i:]\n",
    "            else:\n",
    "                chunk = self.data[i:i + self.chunk_size]\n",
    "    \n",
    "            self.chunks.append(\"\".join(chunk))\n",
    "    \n",
    "        # Adjust the last chunk to include the overlap\n",
    "        if self.chunk_overlap > 0:\n",
    "            for i in range(1, len(self.chunks)):\n",
    "                self.chunks[i] = self.chunks[i-1][-self.chunk_overlap:] + self.chunks[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File type not supported for: sample.csv\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experimebased solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, d\n",
      "placing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, dplacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long B\n",
      "ile at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Bile at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 20231 Introduction\n",
      "Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [38, 24, 15].\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "states ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence le\n",
      "ure precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence leure precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "significant improvements in computational efficiency through factorization tricks [ 21] and conditional\n",
      "computation [ 32], while also improving model performance in case of the latter. The fundamental\n",
      "constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer\n",
      "rely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformerrely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to \n",
      "duced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to duced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and o\n",
      " transduction model relying\n",
      "entirely on self-attention to compute representations of its input and o transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\n",
      "of continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\n",
      "sequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\n",
      "[10], consuming the previously generated symbols as additional input when generating the next.\n",
      "2Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder \n",
      "chitecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder chitecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully connected feed-forward network. We employ a residual connection [ 11] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\n",
      "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "layers, produce outputs of dimension dmodel = 512 .\n",
      "Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third s\n",
      "tical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third stical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
      "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
      "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position ican depend only on the known outputs at positions less than i.\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention laye\n",
      "(left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention laye(left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\n",
      "values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "into a matrix Q. The keys and values are also packed together into matrices KandV. We compute\n",
      "the matrix of outputs as:\n",
      "Attention( Q, K, V ) = softmax(QKT\n",
      "√dk)V (1)\n",
      "The two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attent\n",
      "ctions are additive attention [ 2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attentctions are additive attention [ 2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "of1√dk. Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "much faster and more space-efficient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.\n",
      "While for small values of dkthe two mechanisms perform similarly, additive attention outperforms\n",
      "dot product attention without scaling for larger values of dk[3]. We suspect that for large values of\n",
      "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\n",
      "3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and q\n",
      "tention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and qtention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "we found it beneficial to linearly project the queries, keys and values htimes with different, learned\n",
      "linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "4To illustrate why the dot products get large, assume that the components of qandkare independent random\n",
      "variables with mean 0and variance 1. Then their dot product, q·k=Pdk\n",
      "i=1qiki, has mean 0and variance dk.\n",
      "4output values. These are concatenated and once again projected, resulting in the final values, as\n",
      "depicted in Figure 2.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation\n",
      "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "MultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\n",
      "where head i= Atte\n",
      "ead, averaging inhibits this.\n",
      "MultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\n",
      "where head i= Atteead, averaging inhibits this.\n",
      "MultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\n",
      "where head i= Attention( QWQ\n",
      "i, KWK\n",
      "i, V WV\n",
      "i)\n",
      "Where the projections are parameter matrices WQ\n",
      "i∈Rdmodel×dk,WK\n",
      "i∈Rdmodel×dk,WV\n",
      "i∈Rdmodel×dv\n",
      "andWO∈Rhdv×dmodel.\n",
      "In this work we employ h= 8 parallel attention layers, or heads. For each of these we use\n",
      "dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\n",
      "is similar to that of single-head attention with full dimensionality.\n",
      "3.2.3 Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[38, 2, 9].\n",
      "•The encoder contains self-att\n",
      "tention mechanisms in sequence-to-sequence models such as\n",
      "[38, 2, 9].\n",
      "•The encoder contains self-atttention mechanisms in sequence-to-sequence models such as\n",
      "[38, 2, 9].\n",
      "•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in the\n",
      "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder.\n",
      "•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "all positions in the decoder up to and including that position. We need to prevent leftward\n",
      "information flow in the decoder to preserve the auto-regressive property. We implement this\n",
      "inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      "connected feed-forward network, which is applied to ea\n",
      "s in our encoder and decoder contains a fully\n",
      "connected feed-forward network, which is applied to eas in our encoder and decoder contains a fully\n",
      "connected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.\n",
      "FFN( x) = max(0 , xW 1+b1)W2+b2 (2)\n",
      "While the linear transformations are the same across different positions, they use different parameters\n",
      "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      "The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\n",
      "dff= 2048 .\n",
      "3.4 Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
      "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
      "mation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
      "our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
      "linear transfor\n",
      "we share the same weight matrix between the two embedding layers and the pre-softmax\n",
      "linear transforwe share the same weight matrix between the two embedding layers and the pre-softmax\n",
      "linear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\n",
      "5Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for different layer types. nis the sequence length, dis the representation dimension, kis the kernel\n",
      "size of convolutions and rthe size of the neighborhood in restricted self-attention.\n",
      "Layer Type Complexity per Layer Sequential Maximum Path Length\n",
      "Operations\n",
      "Self-Attention O(n2·d) O(1) O(1)\n",
      "Recurrent O(n·d2) O(n) O(n)\n",
      "Convolutional O(k·n·d2) O(1) O(logk(n))\n",
      "Self-Attention (restricted) O(r·n·d) O(1) O(n/r)\n",
      "3.5 Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
      "order of the sequence, we must inject some information about the relative or absolute position of the\n",
      "tokens in the sequence. To this end, we add \"positional encodings\" to the inpu\n",
      "olute position of the\n",
      "tokens in the sequence. To this end, we add \"positional encodings\" to the inpuolute position of the\n",
      "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
      "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
      "learned and fixed [9].\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "PE(pos,2i)=sin(pos/100002i/d model)\n",
      "PE(pos,2i+1)=cos(pos/100002i/d model)\n",
      "where posis the position and iis the dimension. That is, each dimension of the positional encoding\n",
      "corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\n",
      "chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
      "relative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\n",
      "PEpos.\n",
      "We also experimented with using learned positional embeddings [ 9] instead, and found that the two\n",
      "versions produce\n",
      "ented with using learned positional embeddings [ 9] instead, and found that the two\n",
      "versions produceented with using learned positional embeddings [ 9] instead, and found that the two\n",
      "versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\n",
      "because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\n",
      "during training.\n",
      "4 Why Self-Attention\n",
      "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
      "tional layers commonly used for mapping one variable-length sequence of symbol representations\n",
      "(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
      "consider three desiderata.\n",
      "One is the total computational complexity per layer. Another is the amount of computation that can\n",
      "be parallelized, as measured by the minimum number of sequential operations required.\n",
      "The third is the path length between long-range dependencies in the network.\n",
      "al operations required.\n",
      "The third is the path length between long-range dependencies in the network.al operations required.\n",
      "The third is the path length between long-range dependencies in the network. Learning long-range\n",
      "dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\n",
      "ability to learn such dependencies is the length of the paths forward and backward signals have to\n",
      "traverse in the network. The shorter these paths between any combination of positions in the input\n",
      "and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\n",
      "the maximum path length between any two input and output positions in networks composed of the\n",
      "different layer types.\n",
      "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\n",
      "executed operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\n",
      "computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
      "6length nis smaller than the representation dimensionality d, wh\n",
      " recurrent layers when the sequence\n",
      "6length nis smaller than the representation dimensionality d, wh recurrent layers when the sequence\n",
      "6length nis smaller than the representation dimensionality d, which is most often the case with\n",
      "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
      "[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\n",
      "very long sequences, self-attention could be restricted to considering only a neighborhood of size rin\n",
      "the input sequence centered around the respective output position. This would increase the maximum\n",
      "path length to O(n/r). We plan to investigate this approach further in future work.\n",
      "A single convolutional layer with kernel width k < n does not connect all pairs of input and output\n",
      "positions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\n",
      "orO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\n",
      "between any two positions in the network. Convolutional layers are generally more \n",
      "the longest paths\n",
      "between any two positions in the network. Convolutional layers are generally more the longest paths\n",
      "between any two positions in the network. Convolutional layers are generally more expensive than\n",
      "recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\n",
      "considerably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\n",
      "convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\n",
      "the approach we take in our model.\n",
      "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\n",
      "from our models and present and discuss examples in the appendix. Not only do individual attention\n",
      "heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\n",
      "and semantic structure of the sentences.\n",
      "5 Training\n",
      "This section describes the training regime for our models.\n",
      "5.1 Training Data and Batching\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
      "sentence pairs. Sent\n",
      "on the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
      "sentence pairs. Senton the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
      "sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\n",
      "target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n",
      "2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\n",
      "vocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\n",
      "batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\n",
      "target tokens.\n",
      "5.2 Hardware and Schedule\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\n",
      "the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\n",
      "trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\n",
      "bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n",
      "(3.5\n",
      "ttom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n",
      "(3.5ttom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n",
      "(3.5 days).\n",
      "5.3 Optimizer\n",
      "We used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\n",
      "rate over the course of training, according to the formula:\n",
      "lrate =d−0.5\n",
      "model·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\n",
      "This corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\n",
      "and decreasing it thereafter proportionally to the inverse square root of the step number. We used\n",
      "warmup _steps = 4000 .\n",
      "5.4 Regularization\n",
      "We employ three types of regularization during training:\n",
      "7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\n",
      "English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
      "ModelBLEU Training Cost (FLOPs)\n",
      "EN-DE EN-FR EN-DE EN-FR\n",
      "ByteNet [18] 23.75\n",
      "Deep-Att + PosUnk [39] 39.2 1.0·1020\n",
      "GNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\n",
      "ConvS2S [9\n",
      "18] 23.75\n",
      "Deep-Att + PosUnk [39] 39.2 1.0·1020\n",
      "GNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\n",
      "ConvS2S [918] 23.75\n",
      "Deep-Att + PosUnk [39] 39.2 1.0·1020\n",
      "GNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\n",
      "ConvS2S [9] 25.16 40.46 9.6·10181.5·1020\n",
      "MoE [32] 26.03 40.56 2.0·10191.2·1020\n",
      "Deep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\n",
      "GNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\n",
      "ConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\n",
      "Transformer (base model) 27.3 38.1 3.3·1018\n",
      "Transformer (big) 28.4 41.8 2.3·1019\n",
      "Residual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\n",
      "sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\n",
      "positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\n",
      "Pdrop= 0.1.\n",
      "Label Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\n",
      "hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "6 Results\n",
      "6.1 Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transformer mo\n",
      "s\n",
      "6.1 Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transformer mos\n",
      "6.1 Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\n",
      "in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\n",
      "BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\n",
      "listed in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\n",
      "surpasses all previously published models and ensembles, at a fraction of the training cost of any of\n",
      "the competitive models.\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\n",
      "outperforming all of the previously published single models, at less than 1/4the training cost of the\n",
      "previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\n",
      "dropout rate Pdrop= 0.1, instead of 0.3.\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\n",
      "were written at 10-minute in\n",
      "used a single model obtained by averaging the last 5 checkpoints, which\n",
      "were written at 10-minute inused a single model obtained by averaging the last 5 checkpoints, which\n",
      "were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\n",
      "used beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\n",
      "were chosen after experimentation on the development set. We set the maximum output length during\n",
      "inference to input length + 50, but terminate early when possible [38].\n",
      "Table 2 summarizes our results and compares our translation quality and training costs to other model\n",
      "architectures from the literature. We estimate the number of floating point operations used to train a\n",
      "model by multiplying the training time, the number of GPUs used, and an estimate of the sustained\n",
      "single-precision floating-point capacity of each GPU5.\n",
      "6.2 Model Variations\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model\n",
      "in different ways, measuring the change in performance on English-to-German translation on the\n",
      "\n",
      "odel\n",
      "in different ways, measuring the change in performance on English-to-German translation on the\n",
      "odel\n",
      "in different ways, measuring the change in performance on English-to-German translation on the\n",
      "5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n",
      "8Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\n",
      "model. All metrics are on the English-to-German translation development set, newstest2013. Listed\n",
      "perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\n",
      "per-word perplexities.\n",
      "N d model dff h d k dvPdrop ϵlstrain PPL BLEU params\n",
      "steps (dev) (dev) ×106\n",
      "base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n",
      "(A)1 512 512 5.29 24.9\n",
      "4 128 128 5.00 25.5\n",
      "16 32 32 4.91 25.8\n",
      "32 16 16 5.01 25.4\n",
      "(B)16 5.16 25.1 58\n",
      "32 5.01 25.4 60\n",
      "(C)2 6.11 23.7 36\n",
      "4 5.19 25.3 50\n",
      "8 4.88 25.5 80\n",
      "256 32 32 5.75 24.5 28\n",
      "1024 128 128 4.66 26.0 168\n",
      "1024 5.12 25.4 53\n",
      "4096 4.75 26.2 90\n",
      "(D)0.0 5.77 24.6\n",
      "0.2 4.95 25.5\n",
      "0.0 4.67 25.3\n",
      "0.2 5.47 25.7\n",
      "(E) positional embedding instead of sinusoids 4.92\n",
      "77 24.6\n",
      "0.2 4.95 25.5\n",
      "0.0 4.67 25.3\n",
      "0.2 5.47 25.7\n",
      "(E) positional embedding instead of sinusoids 4.9277 24.6\n",
      "0.2 4.95 25.5\n",
      "0.0 4.67 25.3\n",
      "0.2 5.47 25.7\n",
      "(E) positional embedding instead of sinusoids 4.92 25.7\n",
      "big 6 1024 4096 16 0.3 300K 4.33 26.4 213\n",
      "development set, newstest2013. We used beam search as described in the previous section, but no\n",
      "checkpoint averaging. We present these results in Table 3.\n",
      "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\n",
      "keeping the amount of computation constant, as described in Section 3.2.2. While single-head\n",
      "attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\n",
      "suggests that determining compatibility is not easy and that a more sophisticated compatibility\n",
      "function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\n",
      "bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\n",
      "sinu\n",
      "els are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\n",
      "sinuels are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\n",
      "sinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\n",
      "results to the base model.\n",
      "6.3 English Constituency Parsing\n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English\n",
      "constituency parsing. This task presents specific challenges: the output is subject to strong structural\n",
      "constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\n",
      "models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
      "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\n",
      "Penn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\n",
      "using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n",
      "[37]. We used a vocabulary of 16K tokens for the WSJ only set\n",
      " from with approximately 17M sentences\n",
      "[37]. We used a vocabulary of 16K tokens for the WSJ only set from with approximately 17M sentences\n",
      "[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\n",
      "for the semi-supervised setting.\n",
      "We performed only a small number of experiments to select the dropout, both attention and residual\n",
      "(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\n",
      "remained unchanged from the English-to-German base translation model. During inference, we\n",
      "9Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\n",
      "of WSJ)\n",
      "Parser Training WSJ 23 F1\n",
      "Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\n",
      "Petrov et al. (2006) [29] WSJ only, discriminative 90.4\n",
      "Zhu et al. (2013) [40] WSJ only, discriminative 90.4\n",
      "Dyer et al. (2016) [8] WSJ only, discriminative 91.7\n",
      "Transformer (4 layers) WSJ only, discriminative 91.3\n",
      "Zhu et al. (2013) [40] semi-supervised 91.3\n",
      "Huang & Harper (2009) [14] semi-supervised 91.3\n",
      "McClosky et al. (2006) [26] semi\n",
      "emi-supervised 91.3\n",
      "Huang & Harper (2009) [14] semi-supervised 91.3\n",
      "McClosky et al. (2006) [26] semiemi-supervised 91.3\n",
      "Huang & Harper (2009) [14] semi-supervised 91.3\n",
      "McClosky et al. (2006) [26] semi-supervised 92.1\n",
      "Vinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\n",
      "Transformer (4 layers) semi-supervised 92.7\n",
      "Luong et al. (2015) [23] multi-task 93.0\n",
      "Dyer et al. (2016) [8] generative 93.3\n",
      "increased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\n",
      "for both WSJ only and the semi-supervised setting.\n",
      "Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\n",
      "prisingly well, yielding better results than all previously reported models with the exception of the\n",
      "Recurrent Neural Network Grammar [8].\n",
      "In contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\n",
      "Parser [29] even when training only on the WSJ training set of 40K sentences.\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on\n",
      "attention, replacing the recurre\n",
      "ransformer, the first sequence transduction model based entirely on\n",
      "attention, replacing the recurreransformer, the first sequence transduction model based entirely on\n",
      "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "multi-headed self-attention.\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based\n",
      "on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\n",
      "English-to-French translation tasks, we achieve a new state of the art. In the former task our best\n",
      "model outperforms even all previously reported ensembles.\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks. We\n",
      "plan to extend the Transformer to problems involving input and output modalities other than text and\n",
      "to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\n",
      "such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      "The code we used to train and evaluate our models is ava\n",
      "quential is another research goals of ours.\n",
      "The code we used to train and evaluate our models is avaquential is another research goals of ours.\n",
      "The code we used to train and evaluate our models is available at https://github.com/\n",
      "tensorflow/tensor2tensor .\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "References\n",
      "[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n",
      "arXiv:1607.06450 , 2016.\n",
      "[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n",
      "learning to align and translate. CoRR , abs/1409.0473, 2014.\n",
      "[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\n",
      "machine translation architectures. CoRR , abs/1703.03906, 2017.\n",
      "[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\n",
      "reading. arXiv preprint arXiv:1601.06733 , 2016.\n",
      "10[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\n",
      "and Yoshua Bengio. Learning phrase repre\n",
      "rrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\n",
      "and Yoshua Bengio. Learning phrase reprerrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\n",
      "and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\n",
      "machine translation. CoRR , abs/1406.1078, 2014.\n",
      "[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\n",
      "preprint arXiv:1610.02357 , 2016.\n",
      "[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\n",
      "of gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\n",
      "[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\n",
      "network grammars. In Proc. of NAACL , 2016.\n",
      "[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\n",
      "tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\n",
      "[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\n",
      "arXiv:1308.0850 , 2013.\n",
      "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning \n",
      "308.0850 , 2013.\n",
      "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning 308.0850 , 2013.\n",
      "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\n",
      "age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\n",
      "Recognition , pages 770–778, 2016.\n",
      "[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\n",
      "recurrent nets: the difficulty of learning long-term dependencies, 2001.\n",
      "[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\n",
      "9(8):1735–1780, 1997.\n",
      "[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\n",
      "across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\n",
      "Language Processing , pages 832–841. ACL, August 2009.\n",
      "[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\n",
      "the limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\n",
      "[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neur\n",
      ", 2016.\n",
      "[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neur, 2016.\n",
      "[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\n",
      "Information Processing Systems, (NIPS) , 2016.\n",
      "[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\n",
      "on Learning Representations (ICLR) , 2016.\n",
      "[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\n",
      "ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\n",
      "2017.\n",
      "[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      "InInternational Conference on Learning Representations , 2017.\n",
      "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\n",
      "[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\n",
      "arXiv:1703.10722 , 2017.\n",
      "[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
      "Zhou, and Yoshua Bengio. A structured self-attentive \n",
      " Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
      "Zhou, and Yoshua Bengio. A structured self-attentive  Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
      "Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\n",
      "arXiv:1703.03130 , 2017.\n",
      "[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\n",
      "sequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\n",
      "[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\n",
      "based neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\n",
      "11[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\n",
      "corpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\n",
      "[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\n",
      "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\n",
      "pages 152–159. ACL, June 2006.\n",
      "[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      "mod\n",
      "\n",
      "[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      "mod\n",
      "[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      "model. In Empirical Methods in Natural Language Processing , 2016.\n",
      "[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\n",
      "summarization. arXiv preprint arXiv:1705.04304 , 2017.\n",
      "[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\n",
      "and interpretable tree annotation. In Proceedings of the 21st International Conference on\n",
      "Computational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\n",
      "2006.\n",
      "[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\n",
      "preprint arXiv:1608.05859 , 2016.\n",
      "[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\n",
      "with subword units. arXiv preprint arXiv:1508.07909 , 2015.\n",
      "[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\n",
      "and Jeff Dean. Outrageously large \n",
      "hoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\n",
      "and Jeff Dean. Outrageously large hoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\n",
      "and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\n",
      "layer. arXiv preprint arXiv:1701.06538 , 2017.\n",
      "[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\n",
      "nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\n",
      "Learning Research , 15(1):1929–1958, 2014.\n",
      "[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\n",
      "networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\n",
      "Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\n",
      "Inc., 2015.\n",
      "[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\n",
      "networks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\n",
      "[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\n",
      "Rethinki\n",
      "6] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\n",
      "Rethinki6] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\n",
      "Rethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\n",
      "[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\n",
      "Advances in Neural Information Processing Systems , 2015.\n",
      "[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
      "Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\n",
      "translation system: Bridging the gap between human and machine translation. arXiv preprint\n",
      "arXiv:1609.08144 , 2016.\n",
      "[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\n",
      "fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\n",
      "[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\n",
      "shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n",
      "1: Long Papers) , pages \n",
      "tuent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n",
      "1: Long Papers) , pages tuent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n",
      "1: Long Papers) , pages 434–443. ACL, August 2013.\n",
      "12Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "Figure 3: An example of the attention mechanism following long-distance dependencies in the\n",
      "encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\n",
      "the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\n",
      "the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
      "13Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "\n",
      "different heads. Best viewed in color.\n",
      "13Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "different heads. Best viewed in color.\n",
      "13Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
      "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
      "and 6. Note that the attentions are very sharp for this word.\n",
      "14Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "ect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>ect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
      "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
      "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "15GROKKING : G ENERALIZATION BEYOND OVERFIT -\n",
      "TING ON SMALL ALGORITHMIC DATASETS\n",
      "Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin\n",
      "OpenAIVedant Misra∗\n",
      "Google\n",
      "ABSTRACT\n",
      "In this paper we propose to study generalization of neu\n",
      "buschkin\n",
      "OpenAIVedant Misra∗\n",
      "Google\n",
      "ABSTRACT\n",
      "In this paper we propose to study generalization of neubuschkin\n",
      "OpenAIVedant Misra∗\n",
      "Google\n",
      "ABSTRACT\n",
      "In this paper we propose to study generalization of neural networks on small al-\n",
      "gorithmically generated datasets. In this setting, questions about data efﬁciency,\n",
      "memorization, generalization, and speed of learning can be studied in great de-\n",
      "tail. In some situations we show that neural networks learn through a process\n",
      "of “grokking” a pattern in the data, improving generalization performance from\n",
      "random chance level to perfect generalization, and that this improvement in general-\n",
      "ization can happen well past the point of overﬁtting. We also study generalization as\n",
      "a function of dataset size and ﬁnd that smaller datasets require increasing amounts\n",
      "of optimization for generalization. We argue that these datasets provide a fertile\n",
      "ground for studying a poorly understood aspect of deep learning: generalization\n",
      "of overparametrized neural networks beyond memorization of the ﬁnite training\n",
      "dataset.\n",
      "1 I NTRODUCTION\n",
      "The generalization of overparamet\n",
      "beyond memorization of the ﬁnite training\n",
      "dataset.\n",
      "1 I NTRODUCTION\n",
      "The generalization of overparametbeyond memorization of the ﬁnite training\n",
      "dataset.\n",
      "1 I NTRODUCTION\n",
      "The generalization of overparameterized neural networks has long been a source of interest to the\n",
      "machine learning community since it deﬁes intuitions derived from classical learning theory. In\n",
      "this paper we show that training networks on small algorithmically generated datasets can reliably\n",
      "exhibit unusual generalization patterns, clearly decoupled from performance on the training set, in a\n",
      "signiﬁcantly more pronounced way than such effects manifest on datasets derived from natural data\n",
      "(see Figure 1, left, for an example). Such experiments can be quickly reproduced on a single GPU,\n",
      "and this makes them convenient testbeds for theories of generalization.\n",
      "Figure 1: Left. Grokking: A dramatic example of generalization far after overﬁtting on an algorithmic\n",
      "dataset. We train on the binary operation of division mod 97 with 50% of the data in the training set.\n",
      "Each of the 97 residues is presented to the network as a separate\n",
      "% of the data in the training set.\n",
      "Each of the 97 residues is presented to the network as a separate% of the data in the training set.\n",
      "Each of the 97 residues is presented to the network as a separate symbol, similar to the representation\n",
      "in the ﬁgure to the right. The red curves show training accuracy and the green ones show validation\n",
      "accuracy. Training accuracy becomes close to perfect at <103optimization steps, but it takes\n",
      "close to 106steps for validation accuracy to reach that level, and we see very little evidence of any\n",
      "generalization until 105steps. Center . Training time required to reach 99% validation accuracy\n",
      "increases rapidly as the training data fraction decreases. Right . An example of a small binary\n",
      "operation table. We invite the reader to make their guesses as to which elements are missing.\n",
      "∗Vedant was at OpenAI at the time of this work\n",
      "1arXiv:2201.02177v1  [cs.LG]  6 Jan 2022The datasets we consider are binary operation tables of the form a◦b=cwherea,b,c are discrete\n",
      "symbols with no internal structure, and ◦is a binary operation. Examples of binary operations inclu\n",
      "\n",
      "symbols with no internal structure, and ◦is a binary operation. Examples of binary operations inclu\n",
      "symbols with no internal structure, and ◦is a binary operation. Examples of binary operations include\n",
      "addition, composition of permutations, and bivariate polynomials. Training a neural network on a\n",
      "proper subset of all possible equations then amounts to ﬁlling in the blanks of the binary op table,\n",
      "much like solving a Sudoku puzzle. An example is shown on the right in Figure 1. Since we use\n",
      "distinct abstract symbols for all distinct elements a,b,c involved in the equations, the network is not\n",
      "made aware of any internal structure of the elements, and has to learn about their properties only\n",
      "from their interactions with other elements. For example the network doesn’t see numbers in decimal\n",
      "notation, or permutations in line notation.\n",
      "Our contributions are as follows:\n",
      "•We show that neural networks are capable of generalizing to the empty slots in a variety of\n",
      "binary op tables.\n",
      "•We show that, long after severely overﬁtting, validation accuracy sometimes suddenly\n",
      "begins to increase from cha\n",
      ", long after severely overﬁtting, validation accuracy sometimes suddenly\n",
      "begins to increase from cha, long after severely overﬁtting, validation accuracy sometimes suddenly\n",
      "begins to increase from chance level toward perfect generalization. We call this phenomenon\n",
      "‘grokking’. An example is shown in Figure 1.\n",
      "• We present the data efﬁciency curves for a variety of binary operations.\n",
      "•We show empirically that the amount of optimization required for generalization quickly\n",
      "increases as the dataset size decreases.\n",
      "•We compare various optimization details to measure their impact on data efﬁciency. We ﬁnd\n",
      "that weight decay is particularly effective at improving generalization on the tasks we study.\n",
      "•We visualize the symbol embeddings learned by these networks and ﬁnd that they sometimes\n",
      "uncover recognizable structure of the mathematical objects represented by the symbols.\n",
      "2 M ETHOD\n",
      "All of our experiments used a small transformer trained on datasets of equations of the form a◦b=c,\n",
      "where each of “ a”, “◦”, “b”, “=”, and “c” is a separate token. Details of the operations studied, the\n",
      "architect\n",
      "f “ a”, “◦”, “b”, “=”, and “c” is a separate token. Details of the operations studied, the\n",
      "architectf “ a”, “◦”, “b”, “=”, and “c” is a separate token. Details of the operations studied, the\n",
      "architecture, training hyperparameters and tokenization can be found in Appendix A.1.\n",
      "3 E XPERIMENTS\n",
      "3.1 G ENERALIZATION BEYOND OVERFITTING\n",
      "Deep learning practitioners are used to seeing small improvements in validation accuracy after\n",
      "validation loss stops decreasing. A double descent of validation loss has been documented in\n",
      "some circumstances, but is considered unusual among practitioners Nakkiran et al. (2019); Belkin\n",
      "et al. (2018); d’Ascoli et al. (2020). On the small algorithmic datasets that we study, improved\n",
      "generalization after initial overﬁtting occurs for a range of models, optimizers, and dataset sizes,\n",
      "and in some cases these effects are extremely pronounced. A typical example is shown for modular\n",
      "division in Figure 1. There we see that validation accuracy starts increasing beyond chance level\n",
      "only after 1000 times more optimization steps than are required for training accuracy to ge\n",
      "ce level\n",
      "only after 1000 times more optimization steps than are required for training accuracy to gece level\n",
      "only after 1000 times more optimization steps than are required for training accuracy to get close to\n",
      "optimal. In Figure 4 the training/validation losses are also plotted and we see the double descent of\n",
      "the validation loss.\n",
      "We found these behaviors to be typical for all the binary operations for dataset sizes that were close\n",
      "to the minimal dataset size for which the network generalized within the allotted optimization budget.\n",
      "For larger dataset sizes, the training and validation curves tend to track each other more closely.\n",
      "3.1.1 L EARNING TIME CURVES\n",
      "In a typical supervised learning problem, decreasing the amount of training data decreases the\n",
      "converged generalization performance of the model when the optimization procedure is capable of\n",
      "interpolating the training data. In our setting, we observe a different phenomenon: while the converged\n",
      "performance stays constant at 100% within a range of training dataset sizes, the optimization time\n",
      "required to achieve that performance g\n",
      "thin a range of training dataset sizes, the optimization time\n",
      "required to achieve that performance gthin a range of training dataset sizes, the optimization time\n",
      "required to achieve that performance grows quicky as the dataset size is decreased.\n",
      "2Figure 2: Left. Different optimization algorithms lead to different amounts of generalization within\n",
      "an optimization budget of 105steps for the problem of learning the product in the abstract group S5.\n",
      "Weight decay improves generalization the most, but some generalization happens even with full batch\n",
      "optimizers and models without weight or activation noise at high percentages of training data. Subop-\n",
      "timal choice hyperparameters severely limit generalization. Not shown: training accuracy reaches\n",
      "100% after 103-104updates for all optimization methods. Right . Best validation accuracy achieved\n",
      "after105steps on a variety of algorithmic datasets, averaged over 3 seeds. Generalization happens at\n",
      "higher percentages of data for intuitively more complicated and less symmetrical operations.\n",
      "Figure 1 (center) shows median number of optimization steps \n",
      "icated and less symmetrical operations.\n",
      "Figure 1 (center) shows median number of optimization steps icated and less symmetrical operations.\n",
      "Figure 1 (center) shows median number of optimization steps until validation performance ﬁrst\n",
      "reaches 99% for the product in abstract group S5. In the vicinity of 25-30% of data, a decrease of 1%\n",
      "of training data leads to an increase of 40-50% in median time to generalization. While the number\n",
      "of steps until validation accuracy >99% grows quickly as dataset size decreases, the number of steps\n",
      "until the train accuracy ﬁrst reaches 99% generally trends down as dataset size decreases and stays in\n",
      "the range of 103-104optimization steps. We’ve observed a similar pattern of exponential increase in\n",
      "optimization time until reaching generalization as dataset size decreases on all the algorithmic tasks\n",
      "for which we could get the networks to generalize.\n",
      "3.2 G ROKKING ON A VARIETY OF PROBLEMS\n",
      "We’ve measured the mean accuracy across three runs for training datasets consisting of different\n",
      "fractions of all available equations for a variety of binary operations\n",
      "sets consisting of different\n",
      "fractions of all available equations for a variety of binary operationssets consisting of different\n",
      "fractions of all available equations for a variety of binary operations listed in Appendix A.1.1. The\n",
      "results are presented in Figure 2 (right).\n",
      "Since the operands are presented to the neural network as unrelated abstract symbols, the operations\n",
      "x+y(modp−1)andx∗y(modp)with a prime number pand non-zero x,yare indistinguishable\n",
      "from the neural network’s perspective (and similarly x−y(modp−1)andx/y(modp)). This\n",
      "is because every nonzero residue modulo a prime can be represented as a power of a primitive root.\n",
      "This representation shows the equivalence (up to renaming of symbols) of modular addition modulo\n",
      "p−1and modular multiplication modulo p. We see in Figure 2 (right) that x−yandx/yindeed\n",
      "take about the same amount of data for generalization to occur.\n",
      "Some of the operations listed in Figure 2 (right) are symmetric with respect to the order of the\n",
      "operands (x+y,x∗y,x2+y2andx2+xy+y2). Such operations tend to require less data for\n",
      "generalization than closely rel\n",
      "x∗y,x2+y2andx2+xy+y2). Such operations tend to require less data for\n",
      "generalization than closely relx∗y,x2+y2andx2+xy+y2). Such operations tend to require less data for\n",
      "generalization than closely related non-symmetrical counterparts ( x−y,x/y,x2+xy+y2+x).\n",
      "We believe this effect might be partially architecture-dependent, since it’s easy for a transformer to\n",
      "learn a symmetric function of the operands by ignoring positional embedding.\n",
      "Some operations (for example x3+xy2+y(mod 97) ) didn’t lead to generalization within the\n",
      "allowed optimization budget at any percentage of data up to 95%. The converged models effectively\n",
      "just memorized the training dataset without ﬁnding any real patterns in the data. To such a model, the\n",
      "data is effectively random.\n",
      "The operation [x/y(modp)ifyis odd, otherwise x−y(modp)]requires the network to learn a\n",
      "mix of several simple operations - in particular the role of xhas to be interpreted as a residue in the\n",
      "additive group when it’s paired with an even y, and as a residue in the multiplicative group when it’s\n",
      "paired with an odd y. This shows that generalizatio\n",
      " a residue in the multiplicative group when it’s\n",
      "paired with an odd y. This shows that generalizatio a residue in the multiplicative group when it’s\n",
      "paired with an odd y. This shows that generalization can happen even for operations that are not\n",
      "cleanly interpretable via group or ring operations.\n",
      "3.3 A BLATIONS AND TRICKS\n",
      "We’ve tried various forms of regularization to see what can induce networks to generalize better on\n",
      "our datasets. Here we present the data efﬁciency curves on a particular dataset S5for a variety of\n",
      "interventions: full-batch gradient descent, stochastic gradient descent, large or small learning rates,\n",
      "3residual dropout Srivastava et al. (2014), weight decay Loshchilov & Hutter (2017) and gradient\n",
      "noise Neelakantan et al. (2015). The results are shown in Figure 2 (left).\n",
      "We ﬁnd that adding weight decay has a very large effect on data efﬁciency, more than halving the\n",
      "amount of samples needed compared to most other interventions. We found that weight decay towards\n",
      "the initialization of the network is also effective, but not quite as effective as weight decay towards\n",
      "th\n",
      "itialization of the network is also effective, but not quite as effective as weight decay towards\n",
      "thitialization of the network is also effective, but not quite as effective as weight decay towards\n",
      "the origin. This makes us believe that the prior, that approximately zero weights are suitable for small\n",
      "algorithmic tasks, explains part, but not all of the superior performance of weight decay. Adding some\n",
      "noise to the optimization process (e.g. gradient noise from using minibatches, Gaussian noise applied\n",
      "to weights before or after computing the gradients) is beneﬁcial for generalization, consistent with\n",
      "the idea that such noise might induce the optimization to ﬁnd ﬂatter minima that generalize better.\n",
      "We found that learning rate had to be tuned in a relatively narrow window for the generalization to\n",
      "happen (within 1 order of magnitude).\n",
      "3.4 Q UALITATIVE VISUALIZATION OF EMBEDDINGS\n",
      "In order to gain some insight into networks that generalize, we visualized the matrix of the output\n",
      "layer for the case of modular addition and S5. In Figure 3 we show t-SNE plots of the row vectors.\n",
      "For some \n",
      "r the case of modular addition and S5. In Figure 3 we show t-SNE plots of the row vectors.\n",
      "For some r the case of modular addition and S5. In Figure 3 we show t-SNE plots of the row vectors.\n",
      "For some networks we ﬁnd clear reﬂections of the structure of the underlying mathematical objects\n",
      "in the plots. For example the circular topology of modular addition is shown with a ‘number line’\n",
      "formed by adding 8 to each element. The structure is more apparent in networks that were optimized\n",
      "with weight decay.\n",
      "4 D ISCUSSION\n",
      "We have seen that in the datasets we studied, small algorithmic binary operation tables, effects such\n",
      "as double descent or late generalization, and improvements to generalization from interventions like\n",
      "weight decay can be striking. This suggests that these datasets could be a good place to investigate\n",
      "aspects of generalization. For example, we plan to test whether various proposed measures of minima\n",
      "ﬂatness correlate with generalization in our setting.\n",
      "We have also seen that visualizing the embedding spaces of these neural networks can show natural\n",
      "kinds of structure, for e\n",
      "visualizing the embedding spaces of these neural networks can show natural\n",
      "kinds of structure, for evisualizing the embedding spaces of these neural networks can show natural\n",
      "kinds of structure, for example in problems of modular arithmetic the topology of the embeddings\n",
      "tends to be circles or cylinders. We also see that the network tends to idiosyncratically organize the\n",
      "embeddings by various residues. Whilst the properties of these mathematical objects are familiar to\n",
      "us, we speculate that such visualizations could one day be a useful way to gain intuitions about novel\n",
      "mathematical objects.\n",
      "Figure 3: Left. t-SNE projection of the output layer weights from a network trained on S5. We see\n",
      "clusters of permutations, and each cluster is a coset of the subgroup ⟨(0,3)(1,4),(1,2)(3,4)⟩or one\n",
      "of its conjugates. Right . t-SNE projection of the output layer weights from a network trained on\n",
      "modular addition. The lines show the result of adding 8 to each element. The colors show the residue\n",
      "of each element modulo 8.\n",
      "4In addition, we document an interesting phenomenon, where the number of opti\n",
      "each element modulo 8.\n",
      "4In addition, we document an interesting phenomenon, where the number of optieach element modulo 8.\n",
      "4In addition, we document an interesting phenomenon, where the number of optimization steps needed\n",
      "to reach a given level of performance increases quickly as we reduce the size of the training dataset.\n",
      "Since this represents a way trade compute for performance on smaller amounts of data, it would be\n",
      "useful to investigate in future work whether the effect is also present for other datasets.\n",
      "REFERENCES\n",
      "Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning\n",
      "practice and the bias-variance trade-off. arXiv preprint arXiv:1812.11118 , 2018.\n",
      "St´ephane d’Ascoli, Levent Sagun, and Giulio Biroli. Triple descent and the two kinds of overﬁtting:\n",
      "Where & why do they appear? arXiv preprint arXiv:2006.03509 , 2020.\n",
      "Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal\n",
      "transformers. arXiv preprint arXiv:1807.03819 , 2018.\n",
      "Alex Graves. Adaptive computation time for recurrent neural networks. arXiv prepr\n",
      "1807.03819 , 2018.\n",
      "Alex Graves. Adaptive computation time for recurrent neural networks. arXiv prepr1807.03819 , 2018.\n",
      "Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint\n",
      "arXiv:1603.08983 , 2016.\n",
      "Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint\n",
      "arXiv:1410.5401 , 2014.\n",
      "Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to\n",
      "transduce with unbounded memory. Advances in neural information processing systems , 28:\n",
      "1828–1836, 2015.\n",
      "Sepp Hochreiter and J ¨urgen Schmidhuber. Flat minima. Neural computation , 9(1):1–42, 1997.\n",
      "Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic\n",
      "generalization measures and where to ﬁnd them. arXiv preprint arXiv:1912.02178 , 2019.\n",
      "Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228 ,\n",
      "2015.\n",
      "Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter\n",
      "Tang. On large-batch training for deep learning: Generalization gap and sharp minima. CoRR ,\n",
      "ab\n",
      "eter\n",
      "Tang. On large-batch training for deep learning: Generalization gap and sharp minima. CoRR ,\n",
      "abeter\n",
      "Tang. On large-batch training for deep learning: Generalization gap and sharp minima. CoRR ,\n",
      "abs/1609.04836, 2016. URL http://arxiv.org/abs/1609.04836 .\n",
      "Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\n",
      "arXiv:1711.05101 , 2017.\n",
      "Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep\n",
      "double descent: Where bigger models and more data hurt. arXiv preprint arXiv:1912.02292 , 2019.\n",
      "Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and\n",
      "James Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint\n",
      "arXiv:1511.06807 , 2015.\n",
      "Scott Reed and Nando De Freitas. Neural programmer-interpreters. arXiv preprint arXiv:1511.06279 ,\n",
      "2015.\n",
      "David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical\n",
      "reasoning abilities of neural models. arXiv preprint arXiv:1904.01557 , 2019.\n",
      "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\n",
      "models. arXiv preprint arXiv:1904.01557 , 2019.\n",
      "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,models. arXiv preprint arXiv:1904.01557 , 2019.\n",
      "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\n",
      "Dropout: a simple way to prevent neural networks from overﬁtting. The journal of machine\n",
      "learning research , 15(1):1929–1958, 2014.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\n",
      "Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762 , 2017.\n",
      "Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv preprint\n",
      "arXiv:1410.3916 , 2014.\n",
      "5Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merri ¨enboer, Armand\n",
      "Joulin, and Tomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy\n",
      "tasks. arXiv preprint arXiv:1502.05698 , 2015.\n",
      "Wojciech Zaremba and Ilya Sutskever. Reinforcement learning neural turing machines-revised. arXiv\n",
      "preprint arXiv:1505.00521 , 2015.\n",
      "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and O\n",
      "iv\n",
      "preprint arXiv:1505.00521 , 2015.\n",
      "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oiv\n",
      "preprint arXiv:1505.00521 , 2015.\n",
      "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding\n",
      "deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530 , 2016.\n",
      "A A PPENDIX\n",
      "A.1 A DDITIONAL EXPERIMENTAL DETAILS\n",
      "A.1.1 B INARY OPERATIONS\n",
      "The following are the binary operations that we have tried (for a prime number p= 97 ):\n",
      "x◦y=x+y(modp)for0≤x,y<p\n",
      "x◦y=x−y(modp)for0≤x,y<p\n",
      "x◦y=x/y(modp)for0≤x<p ,0<y<p\n",
      "x◦y= [x/y(modp)ifyis odd, otherwise x−y(modp)] for0≤x,y<p\n",
      "x◦y=x2+y2(modp)for0≤x,y<p\n",
      "x◦y=x2+xy+y2(modp)for0≤x,y<p\n",
      "x◦y=x2+xy+y2+x(modp)for0≤x,y<p\n",
      "x◦y=x3+xy(modp)for0≤x,y<p\n",
      "x◦y=x3+xy2+y(modp)for0≤x,y<p\n",
      "x◦y=x·yforx,y∈S5\n",
      "x◦y=x·y·x−1forx,y∈S5\n",
      "x◦y=x·y·xforx,y∈S5\n",
      "For each binary operation we constructed a dataset of equations of the form ⟨x⟩⟨op⟩⟨y⟩⟨=⟩⟨x◦y⟩,\n",
      "where⟨a⟩stands for the token corresponding to element a.\n",
      "For each training run, we chose a fraction of all available equations at random and declared them to\n",
      "be the training set, with the res\n",
      "fraction of all available equations at random and declared them to\n",
      "be the training set, with the resfraction of all available equations at random and declared them to\n",
      "be the training set, with the rest of equations being the validation set.\n",
      "A.1.2 M ODEL AND OPTIMIZATION\n",
      "We trained a standard decoder-only transformer Vaswani et al. (2017) with causal attention masking,\n",
      "and calculated loss and accuracy only on the answer part of the equation. For all experiments we\n",
      "used a transformer with 2 layers, width 128, and 4 attention heads, with a total of about 4·105\n",
      "non-embedding parameters.\n",
      "We have tuned optimization hyperparameters by running experiments on modular addition and\n",
      "product inS5. For ﬁnal conﬁguration of hyperparameters we have chosen a balance of performance\n",
      "we saw onS5and simplicity (for example we chose not to anneal the learning rate for the experiments\n",
      "in the paper even though it performed better in some situations). For most experiments we used\n",
      "AdamW optimizer with learning rate 10−3, weight decay 1,β1= 0.9,β2= 0.98, linear learning rate\n",
      "warmup over the ﬁrst 10 updates, mi\n",
      "rate 10−3, weight decay 1,β1= 0.9,β2= 0.98, linear learning rate\n",
      "warmup over the ﬁrst 10 updates, mirate 10−3, weight decay 1,β1= 0.9,β2= 0.98, linear learning rate\n",
      "warmup over the ﬁrst 10 updates, minibatch size 512 or half of training dataset size (whichever was\n",
      "smaller) and optimization budget of 105gradient updates.\n",
      "In section 3.3 we have also tried the following variants (listed in the reading order for Figure 2 left):\n",
      "•Adam optimizer with full batch (i.e. exact gradient of the loss on the whole training dataset)\n",
      "• Adam optimizer\n",
      "6•Adam optimizer with full batch and Gaussian noise added to the update direction for each\n",
      "parameter (W←W+lr·(∆W+ϵ), whereϵis sampled from unit Gaussian, ∆Wis the\n",
      "standard Adam weight update, and lr is the learning rate)\n",
      "• Adam optimizer on model with residual dropout 0.1 added\n",
      "• AdamW optimizer with weight decay 1 (default setting in most other experiments)\n",
      "• AdamW optimizer with weight decay 1 towards the initialization instead of the origin\n",
      "• Adam optimizer with learning rate 3·10−4\n",
      "• Adam optimizer with learning rate 3·10−3\n",
      "•Adam optimizer on model \n",
      "mizer with learning rate 3·10−4\n",
      "• Adam optimizer with learning rate 3·10−3\n",
      "•Adam optimizer on model mizer with learning rate 3·10−4\n",
      "• Adam optimizer with learning rate 3·10−3\n",
      "•Adam optimizer on model with Gaussian weight noise of standard deviation 0.01 (i.e. each\n",
      "parameterWreplaced by W+ 0.01·ϵin the model, with ϵsampled from unit Gaussian).\n",
      "For experiments reported in Section 3.1.1 we increased the optimization budget to 5·105optimization\n",
      "steps in order to capture the increase of time to perfect generalization better.\n",
      "For the experiments reported in Section 3.1 we increased the optimization budget to 106, and used\n",
      "Adam optimizer with no weight decay, for emphasizing how late into the optimization process the\n",
      "generalization can begin.\n",
      "We’ve repeated each experiment for each dataset size with 3 random seeds, with the exception of\n",
      "experiments in section 3.1.1, where we’ve aggregated results over 7 random seeds.\n",
      "A.2 A DDITIONAL FIGURES\n",
      "In Figure 4 we show the loss curves that correspond to the accuracy curves in Figure 1.\n",
      "In Figure 5 we show an example of a binary operation table that \n",
      "to the accuracy curves in Figure 1.\n",
      "In Figure 5 we show an example of a binary operation table that to the accuracy curves in Figure 1.\n",
      "In Figure 5 we show an example of a binary operation table that the network can actually solve.\n",
      "Figure 4: The loss curves for modular division, train and validation. We see the validation loss\n",
      "increases from 102to about 105optimization steps before it begins a second descent.\n",
      "A.3 R ELATED WORK\n",
      "In this paper we study training and generalization dynamics on small simple algorithmic datasets. In\n",
      "the past, algorithmic datasets have been used to probe the capability of neural networks to perform\n",
      "symbolic and algorithmic reasoning. For example the tasks of copying, reversing, and sorting\n",
      "randomly generated sequences, and performing arithmetic operations of multi-digit numbers, have\n",
      "been used as standard benchmarks for sequence-to-sequence models Graves et al. (2014), Weston\n",
      "et al. (2014) Kaiser & Sutskever (2015) Reed & De Freitas (2015), Grefenstette et al. (2015), Zaremba\n",
      "& Sutskever (2015), Graves (2016), Dehghani et al. (2018). Typically in these works\n",
      " (2015), Zaremba\n",
      "& Sutskever (2015), Graves (2016), Dehghani et al. (2018). Typically in these works (2015), Zaremba\n",
      "& Sutskever (2015), Graves (2016), Dehghani et al. (2018). Typically in these works however the\n",
      "emphasis is on the performance in the unlimited data regime, with generalization often studied with\n",
      "respect to input sequence length. Some papers study the sample complexity on algorithmic tasks\n",
      "Reed & De Freitas (2015), but mostly focus on the impact of architectural choices. In contrast we\n",
      "study the phenomenon of generalization in data-limited regime, with an emphasis on phenomena that\n",
      "we believe to be architecture-agnostic.\n",
      "7Figure 5: One of the binary operation tables presented to the networks that the network can perfectly\n",
      "ﬁll in. Each symbol is represented as a letter in English, Hebrew, or Greek alphabet for reader’s\n",
      "convenience. We invite the reader to guess which operation is represented here.\n",
      "Algorithmically generated reasoning datasets like bAbI Weston et al. (2015) encourage work on\n",
      "studying generalization in data-limited regime. Most results on such datasets how\n",
      " encourage work on\n",
      "studying generalization in data-limited regime. Most results on such datasets how encourage work on\n",
      "studying generalization in data-limited regime. Most results on such datasets however focus on a\n",
      "point estimate of performance of a particular architecture or training technique, whereas our main\n",
      "interest is in pointing out the change in generalization past the point where a particular architecture\n",
      "can memorize the training data completely.\n",
      "Neelakantan et al. (2015) has a “grok-like” learning curve on an algorithmic task, but it is related to\n",
      "optimization difﬁculty, whereas our phenomenon is speciﬁcally about generalization.\n",
      "In Saxton et al. (2019) they study generalization on procedurally generated math problems such as\n",
      "arithmetic and differentiation, but for the most part these tasks are more involved than the simple\n",
      "binary op problems we have studied and as such do not lend themselves to observing the kinds of\n",
      "phenomena we describe in this paper, since they would require an extremely large number of samples\n",
      "to master.\n",
      "In Jiang et al. (2019) they studied a large nu\n",
      "quire an extremely large number of samples\n",
      "to master.\n",
      "In Jiang et al. (2019) they studied a large nuquire an extremely large number of samples\n",
      "to master.\n",
      "In Jiang et al. (2019) they studied a large number of generalization or complexity measures on\n",
      "convolutional neural networks to see which, if any, are predictive of generalization performance.\n",
      "They ﬁnd that ﬂatness based measures that aim to quantify the sensitivity of the trained neural network\n",
      "to parameter perturbations are the most predictive. We conjectured that the grokking phenomena\n",
      "8we report in this work may be due to the noise from SGD driving the optimization to ﬂatter/simpler\n",
      "solutions that generalize better and hope to investigate in future work whether any of these measures\n",
      "are predictive of grokking.\n",
      "Zhang et al. (2016) ﬁnds that neural networks of sizes typically used in deep learning can interpolate\n",
      "arbitrary training data, and yet generalize when trained with semantically meaningful labels using\n",
      "appropriate optimization procedures. Our work shows a related phenomenon where neural networks\n",
      "can interpolate a small alg\n",
      "on procedures. Our work shows a related phenomenon where neural networks\n",
      "can interpolate a small algon procedures. Our work shows a related phenomenon where neural networks\n",
      "can interpolate a small algorithmic training dataset without generalizing, but start generalizing when\n",
      "trained with SGD for longer.\n",
      "Nakkiran et al. (2019); Belkin et al. (2018) focus on the phenomenon of double descent in loss as\n",
      "a function of model and optimization procedure capacity. They ﬁnd that the classical U-shaped\n",
      "validation loss curve is followed in some settings (including neural network training) by a second\n",
      "descent of loss that starts around the minimal capacity that is needed to interpolate any training data.\n",
      "We observe a second descent in validation loss (though not accuracy) as a function of the amount of\n",
      "training in some of our experiments, and it happens past the point of interpolating the training data.\n",
      "We believe that the phenomenon we describe might be distinct from the double descent phenomena\n",
      "described in Nakkiran et al. (2019); Belkin et al. (2018) because we observe the second descent\n",
      "in lo\n",
      "escribed in Nakkiran et al. (2019); Belkin et al. (2018) because we observe the second descent\n",
      "in loescribed in Nakkiran et al. (2019); Belkin et al. (2018) because we observe the second descent\n",
      "in loss far past the ﬁrst time the training loss becomes very small (tens of thousands of epochs in\n",
      "some of our experiments), and we don’t observe a non-monotonic behavior of accuracy. The setting\n",
      "of small algorithmic datasets that we study also provides a smaller, more tractable playground for\n",
      "studying subtle generalization phenomena than natural datasets studied in Nakkiran et al. (2019).\n",
      "A.4 G ENERALIZATION WITH MEMORIZING SEVERAL OUTLIERS\n",
      "Figure 6: Effect on data efﬁciency of introducing k∈[0,10,100,1000,2000,3000] outliers (exam-\n",
      "ples with random labels) into the training data. Small number of outliers doesn’t noticeably impact\n",
      "generalization performance, but a large number hinders it signiﬁcantly.\n",
      "In this section we show data efﬁciency curves for a modiﬁed version of a binary op by introducing\n",
      "koutliers to the training dataset. More precisely, at the beginning of the experiment we rando\n",
      "ucing\n",
      "koutliers to the training dataset. More precisely, at the beginning of the experiment we randoucing\n",
      "koutliers to the training dataset. More precisely, at the beginning of the experiment we randomly\n",
      "samplekequations from the training set and replace their answers with answers to other kequations\n",
      "randomly sampled from the training data. The rest of the equations in the training data and all the\n",
      "equations in the validation data are kept as before.\n",
      "In this situation one could imagine one of the following scenarios unfolding. If the model class of\n",
      "neural networks optimized and regularized as before was not large enough to interpolate such “noisy”\n",
      "dataset, one could imagine the procedure converging to a solution that generalizes well, but denoises\n",
      "the training data (i.e. predicts c=a◦bas an answer even for the outlier equations a,b→c′with\n",
      "c′̸=c). On the other extreme it could be that the optimization procedure can ﬁnd networks that\n",
      "interpolate the data, but the resulting models don’t generalize, because they are forced to represent a\n",
      "considerably more complicated function than before\n",
      "eneralize, because they are forced to represent a\n",
      "considerably more complicated function than beforeeneralize, because they are forced to represent a\n",
      "considerably more complicated function than before (a simple function + kexceptions encoded in\n",
      "the training data).\n",
      "In our experiments we ﬁnd that the ﬁrst option doesn’t happen - all experiments reach 100% training\n",
      "accuracy at some point, and this point is not considerably affected by changing the number of outliers\n",
      "k. The second phenomenon happens in a range of training data percentages and number of outliers k\n",
      "- increasing kdecreases the range of training data percentages for which the optimization procedure\n",
      "converges to models that generalize. However the effect of introducing a small number of outliers\n",
      "(up to 1000) is not very pronounced - see Figure 6. We interpret this as additional evidence that\n",
      "the capacity of the network and optimization procedure is well beyond the capacity needed for\n",
      "9memorizing all the labels on the training data, and that generalization happening at all requires a\n",
      "non-trivial explanation.\n",
      "A.5 G ENERALIZATIO\n",
      "ata, and that generalization happening at all requires a\n",
      "non-trivial explanation.\n",
      "A.5 G ENERALIZATIOata, and that generalization happening at all requires a\n",
      "non-trivial explanation.\n",
      "A.5 G ENERALIZATION MEASURES\n",
      "We believe it is useful to explore how predictive common generalization measures are of generalization\n",
      "on small algorithmic datasets presented in this paper. In a preliminary investigation we found that\n",
      "sharpness Hochreiter & Schmidhuber (1997) of the minimum found by a trained network measure\n",
      "seems to be predictive of generalization on one of these datasets. We trained multiple networks\n",
      "with different initialization seeds for a ﬁxed number a steps on the S5composition objective, until\n",
      "approximately half of them achieved high validation accuracy. We then used the method described in\n",
      "Keskar et al. (2016) to calculate the sharpness approximation value, φ. We found that the validation\n",
      "accuracy and the φscore across our trained networks had Spearman correlation coefﬁcient of\n",
      "−0.79548 (signiﬁcant with p<0.000014 ). This is suggestive that grokking may only happen after\n",
      "the network’\n",
      "8 (signiﬁcant with p<0.000014 ). This is suggestive that grokking may only happen after\n",
      "the network’8 (signiﬁcant with p<0.000014 ). This is suggestive that grokking may only happen after\n",
      "the network’s parameters are in ﬂatter regions of the loss landscape. It would be valuable for future\n",
      "work to explore this hypothesis, as well as test other generalization measures.\n",
      "Figure 7: Networks trained on the S5composition objective appear to only grok in relatively ﬂat\n",
      "regions of the loss landscape.\n",
      "10\n",
      "ata, and that generalization happening at all requires a\n",
      "non-trivial explanation.\n",
      "A.5 G ENERALIZATIOata, and that generalization happening at all requires a\n",
      "non-trivial explanation.\n",
      "A.5 G ENERALIZATION MEASURES\n",
      "We believe it is useful to explore how predictive common generalization measures are of generalization\n",
      "on small algorithmic datasets presented in this paper. In a preliminary investigation we found that\n",
      "sharpness Hochreiter & Schmidhuber (1997) of the minimum found by a trained network measure\n",
      "seems to be predictive of generalization on one of these datasets. We trained multiple networks\n",
      "with different initialization seeds for a ﬁxed number a steps on the S5composition objective, until\n",
      "approximately half of them achieved high validation accuracy. We then used the method described in\n",
      "Keskar et al. (2016) to calculate the sharpness approximation value, φ. We found that the validation\n",
      "accuracy and the φscore across our trained networks had Spearman correlation coefﬁcient of\n",
      "−0.79548 (signiﬁcant with p<0.000014 ). This is suggestive that grokking may only happen after\n",
      "the network’\n",
      "\n",
      "____________________________________________________\n",
      "\n",
      "8 (signiﬁcant with p<0.000014 ). This is suggestive that grokking may only happen after\n",
      "the network’8 (signiﬁcant with p<0.000014 ). This is suggestive that grokking may only happen after\n",
      "the network’s parameters are in ﬂatter regions of the loss landscape. It would be valuable for future\n",
      "work to explore this hypothesis, as well as test other generalization measures.\n",
      "Figure 7: Networks trained on the S5composition objective appear to only grok in relatively ﬂat\n",
      "regions of the loss landscape.\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mVector store created successfully\u001b[0m\n",
      "\n",
      "\u001b[1mSimilar Text Retrived:\u001b[0m\n",
      "___________________________________\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m 8 (signiﬁcant with p<0.000014 ). This is suggestive that grokking may only happen after\n",
      "the network’8 (signiﬁcant with p<0.000014 ). This is suggestive that grokking may only happen after\n",
      "the network’s parameters are in ﬂatter regions of the loss landscape. It would be valuable for future\n",
      "work to explore this hypothesis, as well as test other generalization measures.\n",
      "Figure 7: Networks trained on the S5composition objective appear to only grok in relatively ﬂat\n",
      "regions of the loss landscape.\n",
      "10\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.5970477\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m , long after severely overﬁtting, validation accuracy sometimes suddenly\n",
      "begins to increase from cha, long after severely overﬁtting, validation accuracy sometimes suddenly\n",
      "begins to increase from chance level toward perfect generalization. We call this phenomenon\n",
      "‘grokking’. An example is shown in Figure 1.\n",
      "• We present the data efﬁciency curves for a variety of binary operations.\n",
      "•We show empirically that the amount of optimization required for generalization quickly\n",
      "increases as the dataset size decreases.\n",
      "•We compare various optimization details to measure their impact on data efﬁciency. We ﬁnd\n",
      "that weight decay is particularly effective at improving generalization on the tasks we study.\n",
      "•We visualize the symbol embeddings learned by these networks and ﬁnd that they sometimes\n",
      "uncover recognizable structure of the mathematical objects represented by the symbols.\n",
      "2 M ETHOD\n",
      "All of our experiments used a small transformer trained on datasets of equations of the form a◦b=c,\n",
      "where each of “ a”, “◦”, “b”, “=”, and “c” is a separate token. Details of the operations studied, the\n",
      "architect\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.5149054\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m quire an extremely large number of samples\n",
      "to master.\n",
      "In Jiang et al. (2019) they studied a large nuquire an extremely large number of samples\n",
      "to master.\n",
      "In Jiang et al. (2019) they studied a large number of generalization or complexity measures on\n",
      "convolutional neural networks to see which, if any, are predictive of generalization performance.\n",
      "They ﬁnd that ﬂatness based measures that aim to quantify the sensitivity of the trained neural network\n",
      "to parameter perturbations are the most predictive. We conjectured that the grokking phenomena\n",
      "8we report in this work may be due to the noise from SGD driving the optimization to ﬂatter/simpler\n",
      "solutions that generalize better and hope to investigate in future work whether any of these measures\n",
      "are predictive of grokking.\n",
      "Zhang et al. (2016) ﬁnds that neural networks of sizes typically used in deep learning can interpolate\n",
      "arbitrary training data, and yet generalize when trained with semantically meaningful labels using\n",
      "appropriate optimization procedures. Our work shows a related phenomenon where neural networks\n",
      "can interpolate a small alg\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.4754515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the raw text\n",
    "# Split to then use it to update the vector store\n",
    "raw_text = DirectoryReader('data/').load_data()\n",
    "\n",
    "# Craete a tecxt splitter\n",
    "text_splitter = CharacterSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# Split the documents recursivly\n",
    "data = text_splitter.split_documents(raw_text)\n",
    "\n",
    "print(data[-2])\n",
    "print(\"\\n____________________________________________________\\n\")\n",
    "print(data[-1])\n",
    "\n",
    "# Create the vecotr store from documents\n",
    "db = VectorStore(model_name=\"nomic-ai/nomic-embed-text-v1.5\")\n",
    "\n",
    "# Create the vector store\n",
    "db.create_vector_store(data)\n",
    "\n",
    "# Define a querry and searcah for it in my vector stor\n",
    "query = \"When does grooking happen ?\"\n",
    "similar_vectors = db.query_similar_vectors(query, top_n = 3)\n",
    "db.print_similar_vectors(similar_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm assuming you meant \"grocking,\" which is a term coined by Cal Newport in his book \"Deep Work\" to describe the focused, productive study session where one fully engages with a complex concept or skill. Groking typically occurs during dedicated blocks of uninterrupted time, often in a quiet and distraction-free environment. The specific timing can vary depending on the individual's learning style and the complexity of the material being studied. Some people may find that they grok best early in the morning, while others may prefer to study late at night. Ultimately, the goal is to create an environment where you can fully concentrate on the task at hand and immerse yourself in the material until you achieve a deep understanding of it."
     ]
    }
   ],
   "source": [
    "stream = ollama.chat(\n",
    "    model='mistral',\n",
    "    messages=[{'role': 'user', 'content': f\"{query}\"}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The term \"grokking\" is not explicitly defined in the given text, but it can be inferred from context that it refers to a phenomenon where neural networks generalize better after being trained on small algorithmically generated datasets. The text suggests that this may occur in relatively flat regions of the loss landscape and that ﬂatness based measures could be predictive of grokking. However, further research is needed to explore this hypothesis.\n",
      "\n",
      "The process of grokking seems to be related to the networks learning to interpolate a small amount of data while still generalizing to new examples. The authors also mention that they hope to investigate whether any of the measures studied in Jiang et al. (2019) for predicting generalization performance are also predictive of grokking.\n",
      "\n",
      "It's important to note that the term \"grokking\" is not a widely used or standard term in machine learning and deep learning literature, so further research would be needed to understand its implications and potential significance."
     ]
    }
   ],
   "source": [
    "# Get only the text and put it all together\n",
    "context = ' '.join([text for text, _ in similar_vectors])\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model='mistral',\n",
    "    messages=[{'role': 'user', 'content': f\"Question: {query} \\n Context: {context}\"}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv-kernel",
   "language": "python",
   "name": "local-venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "139e7c84632f54486abb9d698f2a5412a324e85ce1b1331ea63d3255168fb27f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
