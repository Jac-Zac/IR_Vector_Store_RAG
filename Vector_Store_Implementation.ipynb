{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store Implementation + RAG\n",
    "\n",
    "The following code is an implementation made for the final exam of the Information Retrival course.\n",
    "\n",
    "**Author:** *Jacopo Zacchigna*\n",
    "\n",
    "<img src=\"https://images.contentstack.io/v3/assets/bltefdd0b53724fa2ce/blt185ef72de6dc0e43/6466a9a1f21a3540facf75ac/vector-search-diagram-cropped-white-space.png\" width=\"75%\" height=\"75%\">\n",
    "\n",
    "---\n",
    "\n",
    "The notebook is an implementation of a vector store.\n",
    "The code is structured in multiple class:\n",
    "\n",
    "- Index\n",
    "- VectorStore\n",
    "\n",
    "Furtheremore I also implemented some additional classes to test the vector store and also for the RAG:\n",
    "\n",
    "- TextLoader\n",
    "- DirectoryReader\n",
    "- TextSplitter\n",
    "\n",
    "---\n",
    "\n",
    "##### The text for the RAG demo is taken from:\n",
    "\n",
    "- Grokking Paper: https://arxiv.org/pdf/2201.02177.pdf\n",
    "- Attention Is All You Need: https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Imports external libraries for the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Index (Helper class)\n",
    "\n",
    "Implementation of an Helper class index which is going to be used in my vector Store\n",
    "\n",
    "- **Add items:** to add all of the vectors with the relative indices to the stored_vectors dictionary\n",
    "- **knn_query:** to get the `top_n` most similar vectors inside a vector store with the relative\n",
    "- **_cosine_similarity:** helper function to compute the cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index:\n",
    "    def __init__(self, dim=None):\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Dictionary to store the vectors\n",
    "        self.stored_vectors = {}\n",
    "\n",
    "    def add_items(self, vectors, vectors_id: int):\n",
    "        \"\"\"\n",
    "        Update the indexing structure for the vector store\n",
    "        \"\"\"\n",
    "        for vector_id, vector in zip(vectors_id, vectors):\n",
    "            if vector.shape != (self.dim,):\n",
    "                raise ValueError(\"Vectors must have shape (dim,)\")\n",
    "            self.stored_vectors[vector_id] = vector\n",
    "\n",
    "    def knn_query(self, query_vector: np.ndarray, top_n: int = 5):\n",
    "        \"\"\"\n",
    "        Find the top n similar vectors to the query vector using cosine similarity.\n",
    "\n",
    "        Args:\n",
    "            query_vector (numpy.ndarray): The query vector.\n",
    "            top_n (int): The number of top similar vectors to return.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of two numpy arrays: the first array contains the indices of the top n similar vectors,\n",
    "            and the second array contains the corresponding cosine similarity scores.\n",
    "        \"\"\"\n",
    "        # For every vector in the vector store compute the similarity and create a tuple with the relative index (int)\n",
    "        similarities = [(index, self._cosine_similarity(query_vector, vector)) for index, vector in self.stored_vectors.items()]\n",
    "\n",
    "        # Sort based on the similarity (second element of the vector) and take the top_n most similar vectors\n",
    "        \n",
    "        # Then zip: [(index, similarity), (index, similarity) ...] -> ([index, index, ...], [similarity, similarity, ...])\n",
    "        top_n_indices, top_n_similarities = zip(*sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n])\n",
    "\n",
    "        return top_n_indices, top_n_similarities\n",
    "        \n",
    "    def _cosine_similarity(self, query_vector, vector) -> float:\n",
    "        \"\"\"\n",
    "        Compute the similarity between two vectors\n",
    "\n",
    "        Args:\n",
    "            query_vector (numpy.ndarray): The query vector\n",
    "            vector (numpy.ndarray): The vector to compare\n",
    "\n",
    "        Returns:\n",
    "            The dot product of the vectors, normalized by the product of their norms\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute the dot product between the two vectors\n",
    "        dot_product = np.dot(query_vector, vector)\n",
    "\n",
    "        # Normalization values\n",
    "        query_vector_norm = np.linalg.norm(query_vector)\n",
    "        vector_norm = np.linalg.norm(vector)\n",
    "\n",
    "        # Return the similarity\n",
    "        return dot_product / (query_vector_norm * vector_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Vector Store class\n",
    "\n",
    "This is the main part of the code which implements the vector store.\n",
    "\n",
    "*It focueses on implementing the following function with some additional functionality and some basic error handling:*\n",
    "\n",
    "- **_load_vector_store:** loads the index and sentences\n",
    "\n",
    "- **save_vector_store:** saves the index and sentences to the specified directory\n",
    "\n",
    "- **create_vector_store:** adds vectors to the vector store\n",
    "\n",
    "- **update_vector_store:** updates the existing vector store with new vectors\n",
    "\n",
    "- **delete_vector_store:** deletes a persistent vector store\n",
    "\n",
    "- **query_similar_vectors:** finds similar vectors to the query vector based on cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, model_name=\"nomic-ai/nomic-embed-text-v1.5\", persist=True, persist_path=\"vector_store\"):\n",
    "        self.persist = persist\n",
    "        self.persist_path = persist_path\n",
    "        \n",
    "        # Initialize our index our index\n",
    "        self.model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "        \n",
    "        # Counter to then store the ids of vectors\n",
    "        self.id_counter = 0\n",
    "\n",
    "        # Dictionary to store chunk corresponding to vectors\n",
    "        self.text_chunks = {}\n",
    "\n",
    "    def save_vector_store(self):\n",
    "        # In the case the vector store was created without persistence\n",
    "        if not self.persist:\n",
    "            # Set it to be persitentxw\n",
    "            self.persist = True\n",
    "    \n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(self.persist_path, exist_ok=True)\n",
    "    \n",
    "        # Serialize and save the index\n",
    "        with open(os.path.join(self.persist_path, \"index.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.index, f)\n",
    "    \n",
    "        # Serialize and save the text_chunks\n",
    "        with open(os.path.join(self.persist_path, \"text_chunks.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.text_chunks, f)\n",
    "\n",
    "    def create_vector_store(self, text_chunks):\n",
    "        # Get the embeddings\n",
    "        embeddings = self.model.encode(text_chunks)\n",
    "        self.embeddings_dimension = len(embeddings[0])\n",
    "\n",
    "        # Create the index with the dimensionality of the vector I got\n",
    "        self.index = Index(dim=self.embeddings_dimension)\n",
    "\n",
    "        # Create a dictionary with the documents and the relative embeddings\n",
    "        chunks_embeddings = {text_chunks[i]: embeddings[i] for i in range(len(text_chunks))}\n",
    "        \n",
    "        try:\n",
    "            vectors = []\n",
    "            ids = []\n",
    "            \n",
    "            for chunk, vector in chunks_embeddings.items():\n",
    "                # Append the new vector\n",
    "                vectors.append(vector)\n",
    "                # Assign a unique integer id to every vector\n",
    "                ids.append(self.id_counter)\n",
    "                # Store the text chunks\n",
    "                self.text_chunks[self.id_counter] = chunk\n",
    "                # Increment the counter for the next vector\n",
    "                self.id_counter += 1\n",
    "                \n",
    "            # Adding the items to the index\n",
    "            self.index.add_items(vectors, ids)\n",
    "\n",
    "            if self.persist:\n",
    "                self.save_vector_store()\n",
    "\n",
    "            print(\"\\033[32mVector store created successfully\\033[0m\", end=\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def update_vector_store(self, text_chunks):\n",
    "        \"\"\"\n",
    "        Update the existing vector store with new documents\n",
    "\n",
    "        documents: List of documents to add to my vector store\n",
    "        \"\"\"\n",
    "        embeddings = self.model.encode(text_chunks)\n",
    "        chunks_embeddings = {text_chunks[i]: embeddings[i] for i in range(len(text_chunks))}\n",
    "\n",
    "        try:\n",
    "            if self.persist:\n",
    "                # Load existing index and text_chunks\n",
    "                self.index, self.text_chunks = self._load_vector_store()\n",
    "\n",
    "            # Get the max for the counter and start from the next one\n",
    "            self.id_counter = max(self.text_chunks.keys()) + 1\n",
    "\n",
    "            # Add new vectors to the index and text_chunks\n",
    "            vectors = []\n",
    "            ids = []\n",
    "            for chunk, vector in chunks_embeddings.items():\n",
    "                vectors.append(vector)\n",
    "                ids.append(self.id_counter)\n",
    "                self.text_chunks[self.id_counter] = chunk\n",
    "                self.id_counter += 1\n",
    "\n",
    "            # Adding the vectors, index to the our index\n",
    "            self.index.add_items(vectors, ids)\n",
    "\n",
    "            print(\"\\033[32mVector store updated successfully\\033[0m\", end=\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def delete_vector_store(self) -> None:\n",
    "        \"\"\"\n",
    "        Delete a persistent vector store that was craeted\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if the directory exists\n",
    "            if os.path.exists(self.persist_path):\n",
    "                # Delete index and text_chunks files\n",
    "                os.remove(os.path.join(self.persist_path, \"index.pkl\"))\n",
    "                os.remove(os.path.join(self.persist_path, \"text_chunks.pkl\"))\n",
    "                os.rmdir(self.persist_path)\n",
    "                print(\"\\033[32mVector store deleted successfully\\033[0m\", end=\"\\n\\n\")\n",
    "            else:\n",
    "                print(\"Vector store does not exist\", end=\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def query_similar_vectors(self, query: str, top_n=5):\n",
    "        \"\"\"\n",
    "        Find similar vectors to the query\n",
    "\n",
    "        Args:\n",
    "            query (str): The query that is going to be searched for inside my vector store\n",
    "            num_results (int): The number of similar vectors to return\n",
    "\n",
    "        Returns:\n",
    "            A list of tuples, each containing a document and its similarity to the query vector\n",
    "        \"\"\"\n",
    "        if self.persist:\n",
    "            # Load existing index and text_chunks\n",
    "            self._load_vector_store()\n",
    "\n",
    "        # Use the same model to encode the query\n",
    "        query_vector = self.model.encode(query)\n",
    "        \n",
    "        # Querry for the top_n most similar vectors to my querry vector\n",
    "        top_n_indices, top_n_similarities = self.index.knn_query(query_vector, top_n=top_n)\n",
    "\n",
    "        # Return the most similar documents in a list of tuples with (text_chunks, similarity_score)\n",
    "        return [(self.text_chunks[index], similarities) for index, similarities in zip(top_n_indices, top_n_similarities)]\n",
    "        \n",
    "    def _load_vector_store(self):\n",
    "        index_file = os.path.join(self.persist_path, \"index.pkl\")\n",
    "        text_chunks_file = os.path.join(self.persist_path, \"text_chunks.pkl\")\n",
    "\n",
    "        if not os.path.exists(index_file) or not os.path.exists(text_chunks_file):\n",
    "            raise FileNotFoundError(\"Index and text_chunks files not found in the specified directory.\")\n",
    "\n",
    "        with open(index_file, \"rb\") as f:\n",
    "            self.index = pickle.load(f)\n",
    "        with open(text_chunks_file, \"rb\") as f:\n",
    "            self.text_chunks = pickle.load(f)\n",
    "\n",
    "        return self.index, self.text_chunks\n",
    "        \n",
    "    def print_similar_vectors(self, similar_vectors) -> None:\n",
    "        \"\"\"\n",
    "        Helper function to print the most similar vector with the relative similarity score in a nice way\n",
    "        \"\"\"\n",
    "        print(\"\\033[1mSimilar Text Retrived:\\033[0m\")\n",
    "        print(\"___________________________________\\n\")\n",
    "        for chunk, similarity_score in similar_vectors:\n",
    "            print(\"\\033[1m- Retrieved Text:\\033[0m\", chunk)\n",
    "            print(\"\\033[1m    Similarity Score:\\033[0m\", similarity_score)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Demo of The vector store\n",
    "\n",
    "Using nomic embed for the demo and a custom index. The demo showcase how to update a vector store and make queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLoader:\n",
    "    \"\"\"\n",
    "    Class to laod a csv file and split it into two splits\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load the data from a CSV file\n",
    "        return pd.read_csv(self.data_path, delimiter=\";\")\n",
    "\n",
    "    def split_data(self, split_ratio=0.8, random_state=1337):\n",
    "        # Load the data\n",
    "        data = self.load_data()\n",
    "\n",
    "        # Split them in two splits\n",
    "        data_1 = data.sample(frac=split_ratio, random_state=random_state)\n",
    "        data_2 = data.drop(data_1.index)\n",
    "\n",
    "        # Return them as a tuple of text lists\n",
    "        return data_1[\"text\"].tolist(), data_2[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store example\n",
    "\n",
    "Creation of the vector store, and simple query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mVector store created successfully\u001b[0m\n",
      "\n",
      "\u001b[1mSimilar Text Retrived:\u001b[0m\n",
      "___________________________________\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m A vintage car gleamed in the sunlight, its polished chrome catching the eye of passersby.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.5708912\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m The sleek, silver sports car raced down the winding mountain road, its engine roaring with power.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.52294475\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m A family sedan cruised along the highway, its occupants singing along to their favorite songs on the radio.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.5017636\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the raw text\n",
    "# Split to then use it to update the vector store\n",
    "data_1, data_2 = TextLoader('data/sample.csv').split_data()\n",
    "\n",
    "# Create an instance of a vector store from documents\n",
    "db = VectorStore(model_name=\"nomic-ai/nomic-embed-text-v1.5\", persist=True, persist_path=\"demo\")\n",
    "\n",
    "# Create the vector store with some of the data\n",
    "db.create_vector_store(data_1)\n",
    "\n",
    "# Define a querry\n",
    "query = \"I want to buy a car\"\n",
    "\n",
    "# Search for it in my vector store and return the 3 most similar results\n",
    "similar_vectors = db.query_similar_vectors(query, top_n=3)\n",
    "\n",
    "# Pritty print the most similar vectors with relative similarity score\n",
    "db.print_similar_vectors(similar_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the vetor store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mVector store updated successfully\u001b[0m\n",
      "\n",
      "\u001b[1mSimilar Text Retrived:\u001b[0m\n",
      "___________________________________\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m And as the sun rises once again, the cycle begins anew, a testament to the beauty and resilience of life.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.53241175\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m The Harley-Davidson motorcycle rumbled to life, its deep, throaty growl announcing its presence on the road.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.5212596\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m A vintage car gleamed in the sunlight, its polished chrome catching the eye of passersby.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.4940513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update the vector store\n",
    "db.update_vector_store(data_2)\n",
    "\n",
    "# Query the vector store\n",
    "query = \"I want to buy a cycle\"\n",
    "\n",
    "similar_vectors = db.query_similar_vectors(query, top_n=3)\n",
    "db.print_similar_vectors(similar_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mVector store deleted successfully\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Delete saved vector store\n",
    "db.delete_vector_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG (Retrival Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Imports\n",
    "\n",
    "- **Pypdf:** to read pdf files\n",
    "- **ollama:** to run mistral 7B (not MoE) locally fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DirectoryReader\n",
    "\n",
    "Simple implementation of a class to load all of the test present in a directorly fro `pdf` and `txt` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectoryReader:\n",
    "    \"\"\"\n",
    "    Class to load all of the text from a directory with different filetypes\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "\n",
    "    # This supports pdfs and txt but can be extended if needed\n",
    "    # Furtheremore we could store the metadata for the source documents and not put all of the text together\n",
    "    def load_data(self):\n",
    "        # List all files in the data directory\n",
    "        files = os.listdir(self.data_path)\n",
    "        self.text = ''\n",
    "    \n",
    "        # Read the contents of each file\n",
    "        for file in files:\n",
    "            # Get the file path\n",
    "            file_path = os.path.join(self.data_path, file)\n",
    "            \n",
    "            if file.endswith('.pdf'):\n",
    "                # load the pdf inside the text attribute\n",
    "                self._load_pfd(file_path)\n",
    "            elif file.endswith('.txt'):\n",
    "                # load the txt inside the text attribute\n",
    "                self._load_txt(file_path)\n",
    "            else:\n",
    "                print(f\"File type not supported for: {file}\")\n",
    "    \n",
    "        return self.text\n",
    "\n",
    "    def _load_pfd(self, file_path):\n",
    "        with open(file_path, 'rb') as f:  # Open the file in binary mode\n",
    "            pdf = PdfReader(f)  # Create a PdfReader object\n",
    "            for page in pdf.pages:\n",
    "                self.text += page.extract_text()\n",
    "        \n",
    "    def _load_txt(self, file_path):\n",
    "        with open(file_path, 'r') as txt_file:  # Open the file in text mode\n",
    "            self.text += txt_file.read()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RecursiveCharacterTextSplitter\n",
    "\n",
    "Implementation of a class that recursivly split the text starting from `\\n\\n` as separator getting to `''` whilest the chunk is smaller then the chunk size defined. By also allowing for overlap of different chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveCharacterTextSplitter:\n",
    "    \"\"\"\n",
    "    Class that split text into chunks that can be used to create a vector store\n",
    "    \"\"\"\n",
    "    def __init__(self, chunk_size=100, chunk_overlap=0):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        # Define the different levels of separators\n",
    "        self._separators = ['\\n\\n', '\\n', ' ', '']\n",
    "\n",
    "    def _get_separator(self, text: str) -> str:\n",
    "        \"\"\"Generator that returns the different separators\"\"\"\n",
    "        for separator in self._separators:\n",
    "            if separator in text:\n",
    "                return separator\n",
    "        return ''\n",
    "\n",
    "    def _split_text_by_separator(self, text: str):\n",
    "        \"\"\"Split the text by the first separator found.\"\"\"\n",
    "        separator = self._get_separator(text)\n",
    "        return text.split(separator) if separator else list(text)\n",
    "\n",
    "    def _merge_splits(self, splits, separator: str):\n",
    "        \"\"\"Merge the splits if the cumulative length is less than the chunk size.\"\"\"\n",
    "        merged_text = []\n",
    "        current_text = ''\n",
    "        for s in splits:\n",
    "            if len(current_text) + len(s) > self.chunk_size:\n",
    "                merged_text.append(current_text)\n",
    "                current_text = s\n",
    "            else:\n",
    "                current_text += separator + s\n",
    "        if current_text:\n",
    "            merged_text.append(current_text)\n",
    "        return merged_text\n",
    "\n",
    "    def split_text(self, text: str):\n",
    "        \"\"\"Split incoming text and return chunks.\"\"\"\n",
    "        final_chunks = [] \n",
    "\n",
    "        splits = self._split_text_by_separator(text)\n",
    "        good_splits = []\n",
    "\n",
    "        for s in splits:\n",
    "            if len(s) <= self.chunk_size:\n",
    "                good_splits.append(s)\n",
    "\n",
    "        if good_splits:\n",
    "            merged_text = self._merge_splits(good_splits, separator=self._get_separator(''.join(good_splits)))\n",
    "            final_chunks.extend(merged_text)\n",
    "\n",
    "        return final_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the vector store and showcase of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File type not supported for: sample.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mVector store created successfully\u001b[0m\n",
      "\n",
      "\u001b[1mSimilar Text Retrived:\u001b[0m\n",
      "___________________________________\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m are predictive of grokking.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.7055642\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m −0.79548 (signiﬁcant with p<0.000014 ). This is suggestive that grokking may only happen after\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.66416645\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m ‘grokking’. An example is shown in Figure 1.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.6611806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all of the raw text \n",
    "raw_text = DirectoryReader('data/').load_data()\n",
    "\n",
    "# Characther spillter initializatoin\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# Split the documents into chunks\n",
    "data = text_splitter.split_text(raw_text)\n",
    "\n",
    "# Initialize an instance of a vector store without persitence\n",
    "db = VectorStore(model_name=\"nomic-ai/nomic-embed-text-v1.5\", persist=False)\n",
    "\n",
    "# Create the vector store\n",
    "db.create_vector_store(data)\n",
    "\n",
    "# Define a querry and search for it in my vector store\n",
    "query = \"When does grooking happen ?\"\n",
    "similar_vectors = db.query_similar_vectors(query, top_n = 3)\n",
    "db.print_similar_vectors(similar_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral without RAG\n",
    "\n",
    "Running the small model on the querry gives us poor results that are not grounded in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm assuming you meant \"grokking\" instead of \"grooking,\" which is a term coined by Robert A. Heinlein in his science fiction novel \"Stranger in a Strange Land.\" In the context of the novel, \"Grok\" refers to the ability to empathize and understand another being or culture completely.\n",
      "\n",
      "As for when \"grokking\" happens, it's important to note that this is a fictional concept from a work of science fiction. In real life, there isn't a specific moment or time when one can say they have completely understood another person or culture. Understanding and empathy are ongoing processes that develop over time through interaction, observation, and open-mindedness.\n",
      "\n",
      "So, to answer your question directly, \"grokking\" doesn't happen at a particular moment or time in real life. Instead, it's an ideal state of understanding and connection that we strive for in our interactions with others."
     ]
    }
   ],
   "source": [
    "stream = ollama.chat(\n",
    "    model='mistral',\n",
    "    messages=[{'role': 'user', 'content': f\"{query}\"}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral with RAG\n",
    "\n",
    "Running the model by passing to it the context provide us much more accurate results. This is what we were looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " According to the sources provided, grokking is a phenomenon where validation accuracy suddenly increases from chance level toward perfect generalization, even after severely overfitting. It is suggested that this may occur in relatively flat regions of the loss landscape (Source 0). The process of grokking is studied in detail in the paper by Alethea Power et al. (Source 1), where they show that neural networks can learn a pattern in small algorithmically generated datasets and improve generalization performance beyond chance level, even after overfitting. This improvement in generalization is found to depend on dataset size and requires increasing optimization as the dataset size decreases (Source 1). Therefore, grokking is likely to happen well past the point of overfitting.\n",
      "\n",
      "Regarding your question about when does grooking happen, based on the sources provided, it appears that grokking can happen after the network has severely overfit and entered relatively flat regions of the loss landscape. However, further research is needed to explore this hypothesis in more detail and test other generalization measures."
     ]
    }
   ],
   "source": [
    "# Get only the text and put it all together with spaces in the middle\n",
    "context = '\\n\\n'.join([f\"Source {i}: {text}\" for i, (text, _) in enumerate(similar_vectors)])\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model='mistral',\n",
    "    messages=[{'role': 'user', 'content': f\"Question: {query}\\n\\n Context: {context}\"}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv-kernel",
   "language": "python",
   "name": "local-venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "139e7c84632f54486abb9d698f2a5412a324e85ce1b1331ea63d3255168fb27f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
