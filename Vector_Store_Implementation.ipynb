{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store Implementation\n",
    "\n",
    "The following code is an implementation made for the final exam of the Information Retrival course.\n",
    "\n",
    "**Author:** *Jacopo Zacchigna*\n",
    "\n",
    "---\n",
    "\n",
    "The notebook is an implementation of a vector store.\n",
    "The code is structured in multiple class:\n",
    "\n",
    "- Index\n",
    "- VectorStore\n",
    "\n",
    "And there is also the implementation of additional classes that are helpfull to load the data and retrive interesting informations:\n",
    "\n",
    "- TextSplitter\n",
    "- Retriever\n",
    "\n",
    "---\n",
    "\n",
    "##### The text for the demo is from:\n",
    "\n",
    "http://ir.dcs.gla.ac.uk/resources/test_collections/time/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Imports external libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Pritty print\n",
    "import termcolor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index (Helper class)\n",
    "\n",
    "Implementation of an Helper class index which is going to be used in my vector Store\n",
    "\n",
    "- **Add items:** to add all of the vectors with the relative indices to the stored_vectors dictionary\n",
    "- **knn_query:** to get the `top_n` most similar vectors inside a vector store with the relative\n",
    "- **_cosine_similarity:** helper function to compute the cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index:\n",
    "    def __init__(self, dim=None):\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Dictionary to store the vectors\n",
    "        self.stored_vectors = {}\n",
    "\n",
    "    def add_items(self, vectors, vectors_id: int):\n",
    "        \"\"\"\n",
    "        Update the indexing structure for the vector store\n",
    "        \"\"\"\n",
    "        for vector_id, vector in zip(vectors_id, vectors):\n",
    "            if vector.shape != (self.dim,):\n",
    "                raise ValueError(\"Vectors must have shape (dim,)\")\n",
    "            self.stored_vectors[vector_id] = vector\n",
    "\n",
    "    def knn_query(self, query_vector: np.ndarray, top_n: int = 5):\n",
    "        \"\"\"\n",
    "        Find the top n similar vectors to the query vector using cosine similarity.\n",
    "\n",
    "        Args:\n",
    "            query_vector (numpy.ndarray): The query vector.\n",
    "            top_n (int): The number of top similar vectors to return.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of two numpy arrays: the first array contains the indices of the top n similar vectors,\n",
    "            and the second array contains the corresponding cosine similarity scores.\n",
    "        \"\"\"\n",
    "        similarities = [(index, self._cosine_similarity(query_vector, vector)) for index, vector in self.stored_vectors.items()]\n",
    "\n",
    "        # Sort based on the similarity (second element of the vector) and take the first top_n elements\n",
    "        # Then unpack it into indices and distances\n",
    "        top_n_indices, top_n_similarities = zip(*sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n])\n",
    "\n",
    "        return top_n_indices, top_n_similarities\n",
    "        \n",
    "    def _cosine_similarity(self, query_vector, vector) -> float:\n",
    "        \"\"\"\n",
    "        Compute the similarity between two vectors\n",
    "\n",
    "        Args:\n",
    "            query_vector (numpy.ndarray): The query vector\n",
    "            vector (numpy.ndarray): The vector to compare\n",
    "\n",
    "        Returns:\n",
    "            The dot product of the vectors, normalized by the product of their norms\n",
    "        \"\"\"\n",
    "\n",
    "        dot_product = np.dot(query_vector, vector)\n",
    "        \n",
    "        query_vector_norm = np.linalg.norm(query_vector)\n",
    "        vector_norm = np.linalg.norm(vector)\n",
    "\n",
    "        # Return the similarity\n",
    "        return dot_product / (query_vector_norm * vector_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store class\n",
    "\n",
    "This is the main part of the code which implements the vector store.\n",
    "\n",
    "*It focueses on implementing the following function with some additional functionality and some basic error handling:*\n",
    "\n",
    "- **_load_vector_store:** loads the index and sentences\n",
    "\n",
    "- **save_vector_store:** saves the index and sentences to the specified directory\n",
    "\n",
    "- **create_vector_store:** adds vectors to the vector store\n",
    "\n",
    "- **update_vector_store:** updates the existing vector store with new vectors\n",
    "\n",
    "- **delete_vector_store:** deletes a persistent vector store\n",
    "\n",
    "- **get_similar_vectors:** finds similar vectors to the query vector based on cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, vector_dimension=None, persist=True, persist_path=\"vector_store\"):\n",
    "\n",
    "        if vector_dimension is None:\n",
    "            raise ValueError(\"You should pass the vector size\")\n",
    "\n",
    "        # Initialize our index our index\n",
    "        self.index = Index(dim=vector_dimension)\n",
    "        self.persist = persist\n",
    "        self.persist_path = persist_path\n",
    "        self.vector_dimension = vector_dimension\n",
    "    \n",
    "        # Counter to then store the ids of vectors\n",
    "        self.id_counter = 0\n",
    "        \n",
    "        # Dictionary to store sentences corresponding to vectors\n",
    "        self.sentences = {}\n",
    "\n",
    "    def _load_vector_store(self):\n",
    "        index_file = os.path.join(self.persist_path, \"index.pkl\")\n",
    "        sentences_file = os.path.join(self.persist_path, \"sentences.pkl\")\n",
    "        \n",
    "        if not os.path.exists(index_file) or not os.path.exists(sentences_file):\n",
    "            raise FileNotFoundError(\"Index and sentences files not found in the specified directory.\")\n",
    "\n",
    "        with open(index_file, \"rb\") as f:\n",
    "            self.index = pickle.load(f)\n",
    "        with open(sentences_file, \"rb\") as f:\n",
    "            self.sentences = pickle.load(f)\n",
    "\n",
    "        return self.index, self.sentences\n",
    "\n",
    "    def save_vector_store(self):\n",
    "        \"\"\"\n",
    "        Save the index and corresponding sentences\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(self.persist_path, exist_ok=True)\n",
    "        \n",
    "        # Serialize and save the index\n",
    "        with open(os.path.join(self.persist_path, \"index.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.index, f)\n",
    "\n",
    "        # Serialize and save the sentences\n",
    "        with open(os.path.join(self.persist_path, \"sentences.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.sentences, f)\n",
    "            \n",
    "    def create_vector_store(self, new_sentence_vectors):\n",
    "        \"\"\"\n",
    "        Add vectors to the vector store\n",
    "\n",
    "        id: the unique id for the vector\n",
    "        vecotor: the vector to be added\n",
    "        \"\"\"\n",
    "        try:\n",
    "            vectors = []\n",
    "            ids = []\n",
    "            for sentence, vector in new_sentence_vectors.items():\n",
    "                # Append the new vector\n",
    "                vectors.append(vector)\n",
    "                # Assign a unique integer id to every vector\n",
    "                ids.append(self.id_counter)\n",
    "                # Store the sentence\n",
    "                self.sentences[self.id_counter] = sentence\n",
    "                # Increment the counter for the next vector\n",
    "                self.id_counter += 1\n",
    "\n",
    "            # Adding the items to the index\n",
    "            self.index.add_items(vectors, ids)\n",
    "    \n",
    "            if self.persist:                \n",
    "                self.save_vector_store()\n",
    "                \n",
    "            print(\"Vector store created successfully\", end=\"\\n\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def update_vector_store(self, new_sentence_vectors):\n",
    "        \"\"\"\n",
    "        Update the existing vector store with new vectors\n",
    "\n",
    "        new_id_vectors: Dictionary containing new vectors to be added\n",
    "        persist_path: Path to the directory where the existing vector store is saved\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load existing index and sentences\n",
    "            self.index, self.sentences = self._load_vector_store()\n",
    "\n",
    "            # Update the id counter\n",
    "            self.id_counter = max(self.sentences.keys()) + 1\n",
    "\n",
    "            # Add new vectors to the index and sentences\n",
    "            vectors = []\n",
    "            ids = []\n",
    "            for sentence, vector in new_sentence_vectors.items():\n",
    "                vectors.append(vector)\n",
    "                ids.append(self.id_counter)\n",
    "                self.sentences[self.id_counter] = sentence\n",
    "                self.id_counter += 1\n",
    "                \n",
    "            # Adding the vectors, index to the our index\n",
    "            self.index.add_items(vectors, ids)\n",
    "            \n",
    "            print(\"Vector store updated successfully\", end=\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def delete_vector_store(self) -> None:\n",
    "        \"\"\"\n",
    "        Delete a persistent vector store that was craeted\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Check if the directory exists\n",
    "            if os.path.exists(self.persist_path):\n",
    "                # Delete index and sentences files\n",
    "                os.remove(os.path.join(self.persist_path, \"index.pkl\"))\n",
    "                os.remove(os.path.join(self.persist_path, \"sentences.pkl\"))\n",
    "                print(\"Vector store deleted successfully\", end=\"\\n\\n\")\n",
    "            else:\n",
    "                print(\"Vector store does not exist\", end=\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def get_similar_vectors(self, query_vector, top_n=5) -> list:\n",
    "        \"\"\"\n",
    "        Find similar vectors to the query vector\n",
    "\n",
    "        Args:\n",
    "            query_vector (numpy.ndarray): The query vector to compare with the vecotr in the store\n",
    "            num_results (int): The number of similar vectors to return\n",
    "\n",
    "        Returns:\n",
    "            A list of tuples, each containing a vector id and its similarity to the query vector\n",
    "        \"\"\"\n",
    "        if self.persist:\n",
    "            # Load existing index and sentences\n",
    "            self._load_vector_store()\n",
    "        \n",
    "        result = []\n",
    "        labels, distances = self.index.knn_query(query_vector, top_n=top_n)\n",
    "\n",
    "        similar_vectors = [(self.sentences[label], distance) for label, distance in zip(labels, distances)]\n",
    "        \n",
    "        return similar_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo of The vector store\n",
    "\n",
    "Using nomic embed for the demo and a custom index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextSplitter and Retriver class\n",
    "\n",
    "Implementation of the text splitter with different options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSplitter:\n",
    "    def __init__(self, data_path, split_ratio=0.8, random_state=200):\n",
    "        self.data = pd.read_csv(data_path, delimiter=\";\")\n",
    "        self.data_1 = self.data.sample(frac=split_ratio, random_state=random_state)\n",
    "        self.data_2 = self.data.drop(self.data_1.index)\n",
    "\n",
    "    # text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "    # Function to split text into chunks...\n",
    "\n",
    "    # Function to split in 80/20\n",
    "    def get_data_split(self):\n",
    "        return self.data_1[\"text\"].tolist(), self.data_2[\"text\"].tolist()\n",
    "\n",
    "    \"\"\"\n",
    "    # Split the text into chunks\n",
    "    texts = text_splitter.split_text(pdf_text)\n",
    "\n",
    "    # Create a metadata for each chunk\n",
    "    metadatas = [{\"source\": f\"{i}-pl\"} for i in range(len(texts))]\n",
    "\n",
    "    # Create a Chroma vector store\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    docsearch = await cl.make_async(Chroma.from_texts)(\n",
    "        texts, embeddings, metadatas=metadatas\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self, model_name=\"nomic-ai/nomic-embed-text-v1.5\", persist=True):\n",
    "        self.model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "        self.persist = persist\n",
    "\n",
    "    def create_vector_store(self, data):\n",
    "        vectors = self.model.encode(data)\n",
    "        vector_dimension = len(vectors[0])\n",
    "        \n",
    "        # You can also change the persit path\n",
    "        self.vector_store = VectorStore(vector_dimension=vector_dimension, persist=self.persist)\n",
    "        new_sentence_vectors = {data[i]: vectors[i] for i in range(len(data))}\n",
    "        self.vector_store.create_vector_store(new_sentence_vectors)\n",
    "\n",
    "    def update_vector_store_data(self, data):\n",
    "        vectors = self.model.encode(data)\n",
    "        new_sentence_vectors = {data[i]: vectors[i] for i in range(len(data))}\n",
    "        self.vector_store.update_vector_store(new_sentence_vectors)\n",
    "        return new_sentence_vectors\n",
    "\n",
    "    def query_similar_vectors(self, query, top_n=5):\n",
    "        query_vector = self.model.encode(query)\n",
    "        similar_vectors = self.vector_store.get_similar_vectors(query_vector, top_n=top_n)\n",
    "        return similar_vectors\n",
    "\n",
    "    def print_similar_vectors(self, similar_vectors):\n",
    "        print(\"Similarity Vectors:\")\n",
    "        for sentence, similarity_score in similar_vectors:\n",
    "            print(termcolor.colored(f\"- Sentence: {sentence}\", \"green\", \"on_grey\", [\"bold\"]))\n",
    "            print(termcolor.colored(f\"  Similarity Score: {similarity_score}\", \"yellow\", \"on_grey\", [\"bold\"]))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created successfully\n",
      "\n",
      "Similarity Vectors:\n",
      "\u001b[1m\u001b[40m\u001b[32m- Sentence: The sleek, silver sports car raced down the winding mountain road, its engine roaring with power.\u001b[0m\n",
      "\u001b[1m\u001b[40m\u001b[33m  Similarity Score: 0.5229447484016418\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[40m\u001b[32m- Sentence: A family sedan cruised along the highway, its occupants singing along to their favorite songs on the radio.\u001b[0m\n",
      "\u001b[1m\u001b[40m\u001b[33m  Similarity Score: 0.5017635822296143\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[40m\u001b[32m- Sentence: A group of friends revved their engines, ready to hit the open road and leave the city behind.\u001b[0m\n",
      "\u001b[1m\u001b[40m\u001b[33m  Similarity Score: 0.4948986768722534\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[40m\u001b[32m- Sentence: The Harley-Davidson motorcycle rumbled to life, its deep, throaty growl announcing its presence on the road.\u001b[0m\n",
      "\u001b[1m\u001b[40m\u001b[33m  Similarity Score: 0.4918859899044037\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[40m\u001b[32m- Sentence: The sound of engines filled the air, a symphony of power and speed that echoed through the streets.\u001b[0m\n",
      "\u001b[1m\u001b[40m\u001b[33m  Similarity Score: 0.4788406491279602\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write a function  to get the everything as text from csv\n",
    "\n",
    "text_splitter = TextSplitter(\"data.csv\")\n",
    "data_1, data_2 = text_splitter.get_data_split()\n",
    "\n",
    "retriever = Retriever()\n",
    "retriever.create_vector_store(data_1)\n",
    "\n",
    "query = \"I want to buy a car\"\n",
    "similar_vectors = retriever.query_similar_vectors(query)\n",
    "\n",
    "retriever.print_similar_vectors(similar_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding update of the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store updated successfully\n",
      "\n",
      "Similarity Vectors:\n",
      "\u001b[1m\u001b[40m\u001b[32m- Sentence: And as the sun rises once again, the cycle begins anew, a testament to the beauty and resilience of life.\u001b[0m\n",
      "\u001b[1m\u001b[40m\u001b[33m  Similarity Score: 0.5324117541313171\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[40m\u001b[32m- Sentence: The Harley-Davidson motorcycle rumbled to life, its deep, throaty growl announcing its presence on the road.\u001b[0m\n",
      "\u001b[1m\u001b[40m\u001b[33m  Similarity Score: 0.521259605884552\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[40m\u001b[32m- Sentence: In the city, motorcyclists weaved through traffic with ease, their nimble machines darting between cars and buses.\u001b[0m\n",
      "\u001b[1m\u001b[40m\u001b[33m  Similarity Score: 0.48292413353919983\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update the vector store\n",
    "new_sentence_vectors_2 = retriever.update_vector_store_data(data_2)\n",
    "\n",
    "# Query the vector store\n",
    "query = \"I want to buy a cycle\"\n",
    "\n",
    "similar_vectors = retriever.query_similar_vectors(query, top_n=3)\n",
    "retriever.print_similar_vectors(similar_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding persistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store deleted successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Delete saved vector store\n",
    "retriever.vector_store.delete_vector_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the system on a set of test queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv-kernel",
   "language": "python",
   "name": "local-venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "139e7c84632f54486abb9d698f2a5412a324e85ce1b1331ea63d3255168fb27f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
