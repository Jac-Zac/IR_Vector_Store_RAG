{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store Implementation + RAG\n",
    "\n",
    "The following code is an implementation made for the final exam of the Information Retrival course.\n",
    "\n",
    "**Author:** *Jacopo Zacchigna*\n",
    "\n",
    "<img src=\"https://images.contentstack.io/v3/assets/bltefdd0b53724fa2ce/blt185ef72de6dc0e43/6466a9a1f21a3540facf75ac/vector-search-diagram-cropped-white-space.png\" width=\"75%\" height=\"75%\">\n",
    "\n",
    "---\n",
    "\n",
    "The notebook is an implementation of a vector store.\n",
    "The code is structured in multiple class:\n",
    "\n",
    "- Index\n",
    "- VectorStore\n",
    "\n",
    "Furtheremore I also implemented some additional classes to test the vector store and also for the RAG:\n",
    "\n",
    "- TextLoader\n",
    "- DirectoryReader\n",
    "- TextSplitter\n",
    "\n",
    "---\n",
    "\n",
    "##### The text for the RAG demo is taken from:\n",
    "\n",
    "- Grokking Paper: https://arxiv.org/pdf/2201.02177.pdf\n",
    "- Attention Is All You Need: https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Imports external libraries for the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Index (Helper class)\n",
    "\n",
    "Implementation of an Helper class index which is going to be used in my vector Store\n",
    "\n",
    "- **Add items:** to add all of the vectors with the relative indices to the stored_vectors dictionary\n",
    "- **knn_query:** to get the `top_n` most similar vectors inside a vector store with the relative\n",
    "- **_cosine_similarity:** helper function to compute the cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index:\n",
    "    def __init__(self, dim=None):\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Dictionary to store the vectors\n",
    "        self.stored_vectors = {}\n",
    "\n",
    "    def add_items(self, vectors, vectors_id: int):\n",
    "        \"\"\"\n",
    "        Update the indexing structure for the vector store\n",
    "        \"\"\"\n",
    "        for vector_id, vector in zip(vectors_id, vectors):\n",
    "            if vector.shape != (self.dim,):\n",
    "                raise ValueError(\"Vectors must have shape (dim,)\")\n",
    "            self.stored_vectors[vector_id] = vector\n",
    "\n",
    "    def knn_query(self, query_vector: np.ndarray, top_n: int = 5):\n",
    "        \"\"\"\n",
    "        Find the top n similar vectors to the query vector using cosine similarity.\n",
    "\n",
    "        Args:\n",
    "            query_vector (numpy.ndarray): The query vector.\n",
    "            top_n (int): The number of top similar vectors to return.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of two numpy arrays: the first array contains the indices of the top n similar vectors,\n",
    "            and the second array contains the corresponding cosine similarity scores.\n",
    "        \"\"\"\n",
    "        # For every vector in the vector store compute the similarity and create a tuple with the relative index (int)\n",
    "        similarities = [(index, self._cosine_similarity(query_vector, vector)) for index, vector in self.stored_vectors.items()]\n",
    "\n",
    "        # Sort based on the similarity (second element of the vector) and take the top_n most similar vectors\n",
    "        \n",
    "        # Then zip: [(index, similarity), (index, similarity) ...] -> ([index, index, ...], [similarity, similarity, ...])\n",
    "        top_n_indices, top_n_similarities = zip(*sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n])\n",
    "\n",
    "        return top_n_indices, top_n_similarities\n",
    "        \n",
    "    def _cosine_similarity(self, query_vector, vector) -> float:\n",
    "        \"\"\"\n",
    "        Compute the similarity between two vectors\n",
    "\n",
    "        Args:\n",
    "            query_vector (numpy.ndarray): The query vector\n",
    "            vector (numpy.ndarray): The vector to compare\n",
    "\n",
    "        Returns:\n",
    "            The dot product of the vectors, normalized by the product of their norms\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute the dot product between the two vectors\n",
    "        dot_product = np.dot(query_vector, vector)\n",
    "\n",
    "        # Normalization values\n",
    "        query_vector_norm = np.linalg.norm(query_vector)\n",
    "        vector_norm = np.linalg.norm(vector)\n",
    "\n",
    "        # Return the similarity\n",
    "        return dot_product / (query_vector_norm * vector_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store class\n",
    "\n",
    "This is the main part of the code which implements the vector store.\n",
    "\n",
    "*It focueses on implementing the following function with some additional functionality and some basic error handling:*\n",
    "\n",
    "- **_load_vector_store:** loads the index and sentences\n",
    "\n",
    "- **save_vector_store:** saves the index and sentences to the specified directory\n",
    "\n",
    "- **create_vector_store:** adds vectors to the vector store\n",
    "\n",
    "- **update_vector_store:** updates the existing vector store with new vectors\n",
    "\n",
    "- **delete_vector_store:** deletes a persistent vector store\n",
    "\n",
    "- **query_similar_vectors:** finds similar vectors to the query vector based on cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, model_name=\"nomic-ai/nomic-embed-text-v1.5\", persist=True, persist_path=\"vector_store\"):\n",
    "        self.persist = persist\n",
    "        self.persist_path = persist_path\n",
    "        \n",
    "        # Initialize our index our index\n",
    "        self.model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "        \n",
    "        # Counter to then store the ids of vectors\n",
    "        self.id_counter = 0\n",
    "\n",
    "        # Dictionary to store chunk corresponding to vectors\n",
    "        self.text_chunks = {}\n",
    "\n",
    "    def save_vector_store(self):\n",
    "        # In the case the vector store was created without persistence\n",
    "        if not self.persist:\n",
    "            # Set it to be persitentxw\n",
    "            self.persist = True\n",
    "    \n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(self.persist_path, exist_ok=True)\n",
    "    \n",
    "        # Serialize and save the index\n",
    "        with open(os.path.join(self.persist_path, \"index.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.index, f)\n",
    "    \n",
    "        # Serialize and save the text_chunks\n",
    "        with open(os.path.join(self.persist_path, \"text_chunks.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.text_chunks, f)\n",
    "\n",
    "    def create_vector_store(self, text_chunks):\n",
    "        # Get the embeddings\n",
    "        embeddings = self.model.encode(text_chunks)\n",
    "        self.embeddings_dimension = len(embeddings[0])\n",
    "\n",
    "        # Create the index with the dimensionality of the vector I got\n",
    "        self.index = Index(dim=self.embeddings_dimension)\n",
    "\n",
    "        # Create a dictionary with the documents and the relative embeddings\n",
    "        chunks_embeddings = {text_chunks[i]: embeddings[i] for i in range(len(text_chunks))}\n",
    "        \n",
    "        try:\n",
    "            vectors = []\n",
    "            ids = []\n",
    "            \n",
    "            for chunk, vector in chunks_embeddings.items():\n",
    "                # Append the new vector\n",
    "                vectors.append(vector)\n",
    "                # Assign a unique integer id to every vector\n",
    "                ids.append(self.id_counter)\n",
    "                # Store the text chunks\n",
    "                self.text_chunks[self.id_counter] = chunk\n",
    "                # Increment the counter for the next vector\n",
    "                self.id_counter += 1\n",
    "                \n",
    "            # Adding the items to the index\n",
    "            self.index.add_items(vectors, ids)\n",
    "\n",
    "            if self.persist:\n",
    "                self.save_vector_store()\n",
    "\n",
    "            print(\"\\033[32mVector store created successfully\\033[0m\", end=\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def update_vector_store(self, text_chunks):\n",
    "        \"\"\"\n",
    "        Update the existing vector store with new documents\n",
    "\n",
    "        documents: List of documents to add to my vector store\n",
    "        \"\"\"\n",
    "        embeddings = self.model.encode(text_chunks)\n",
    "        chunks_embeddings = {text_chunks[i]: embeddings[i] for i in range(len(text_chunks))}\n",
    "\n",
    "        try:\n",
    "            if self.persist:\n",
    "                # Load existing index and text_chunks\n",
    "                self.index, self.text_chunks = self._load_vector_store()\n",
    "\n",
    "            # Get the max for the counter and start from the next one\n",
    "            self.id_counter = max(self.text_chunks.keys()) + 1\n",
    "\n",
    "            # Add new vectors to the index and text_chunks\n",
    "            vectors = []\n",
    "            ids = []\n",
    "            for chunk, vector in chunks_embeddings.items():\n",
    "                vectors.append(vector)\n",
    "                ids.append(self.id_counter)\n",
    "                self.text_chunks[self.id_counter] = chunk\n",
    "                self.id_counter += 1\n",
    "\n",
    "            # Adding the vectors, index to the our index\n",
    "            self.index.add_items(vectors, ids)\n",
    "\n",
    "            print(\"\\033[32mVector store updated successfully\\033[0m\", end=\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def delete_vector_store(self) -> None:\n",
    "        \"\"\"\n",
    "        Delete a persistent vector store that was craeted\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if the directory exists\n",
    "            if os.path.exists(self.persist_path):\n",
    "                # Delete index and text_chunks files\n",
    "                os.remove(os.path.join(self.persist_path, \"index.pkl\"))\n",
    "                os.remove(os.path.join(self.persist_path, \"text_chunks.pkl\"))\n",
    "                os.rmdir(self.persist_path)\n",
    "                print(\"\\033[32mVector store deleted successfully\\033[0m\", end=\"\\n\\n\")\n",
    "            else:\n",
    "                print(\"Vector store does not exist\", end=\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def query_similar_vectors(self, query: str, top_n=5):\n",
    "        \"\"\"\n",
    "        Find similar vectors to the query\n",
    "\n",
    "        Args:\n",
    "            query (str): The query that is going to be searched for inside my vector store\n",
    "            num_results (int): The number of similar vectors to return\n",
    "\n",
    "        Returns:\n",
    "            A list of tuples, each containing a document and its similarity to the query vector\n",
    "        \"\"\"\n",
    "        if self.persist:\n",
    "            # Load existing index and text_chunks\n",
    "            self._load_vector_store()\n",
    "\n",
    "        # Use the same model to encode the query\n",
    "        query_vector = self.model.encode(query)\n",
    "        \n",
    "        # Querry for the top_n most similar vectors to my querry vector\n",
    "        top_n_indices, top_n_similarities = self.index.knn_query(query_vector, top_n=top_n)\n",
    "\n",
    "        # Return the most similar documents in a list of tuples with (text_chunks, similarity_score)\n",
    "        return [(self.text_chunks[index], similarities) for index, similarities in zip(top_n_indices, top_n_similarities)]\n",
    "        \n",
    "    def _load_vector_store(self):\n",
    "        index_file = os.path.join(self.persist_path, \"index.pkl\")\n",
    "        text_chunks_file = os.path.join(self.persist_path, \"text_chunks.pkl\")\n",
    "\n",
    "        if not os.path.exists(index_file) or not os.path.exists(text_chunks_file):\n",
    "            raise FileNotFoundError(\"Index and text_chunks files not found in the specified directory.\")\n",
    "\n",
    "        with open(index_file, \"rb\") as f:\n",
    "            self.index = pickle.load(f)\n",
    "        with open(text_chunks_file, \"rb\") as f:\n",
    "            self.text_chunks = pickle.load(f)\n",
    "\n",
    "        return self.index, self.text_chunks\n",
    "        \n",
    "    def print_similar_vectors(self, similar_vectors) -> None:\n",
    "        \"\"\"\n",
    "        Helper function to print the most similar vector with the relative similarity score in a nice way\n",
    "        \"\"\"\n",
    "        print(\"\\033[1mSimilar Text Retrived:\\033[0m\")\n",
    "        print(\"___________________________________\\n\")\n",
    "        for chunk, similarity_score in similar_vectors:\n",
    "            print(\"\\033[1m- Retrieved Text:\\033[0m\", chunk)\n",
    "            print(\"\\033[1m    Similarity Score:\\033[0m\", similarity_score)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Demo of The vector store\n",
    "\n",
    "Using nomic embed for the demo and a custom index. The demo showcase how to update a vector store and make queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLoader:\n",
    "    \"\"\"\n",
    "    Class to laod a csv file and split it into two splits\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load the data from a CSV file\n",
    "        return pd.read_csv(self.data_path, delimiter=\";\")\n",
    "\n",
    "    def split_data(self, split_ratio=0.8, random_state=1337):\n",
    "        # Load the data\n",
    "        data = self.load_data()\n",
    "\n",
    "        # Split them in two splits\n",
    "        data_1 = data.sample(frac=split_ratio, random_state=random_state)\n",
    "        data_2 = data.drop(data_1.index)\n",
    "\n",
    "        # Return them as a tuple of text lists\n",
    "        return data_1[\"text\"].tolist(), data_2[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store example\n",
    "\n",
    "Creation of the vector store, and simple query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mVector store created successfully\u001b[0m\n",
      "\n",
      "\u001b[1mSimilar Text Retrived:\u001b[0m\n",
      "___________________________________\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m A vintage car gleamed in the sunlight, its polished chrome catching the eye of passersby.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.5708912\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m The sleek, silver sports car raced down the winding mountain road, its engine roaring with power.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.52294475\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m A family sedan cruised along the highway, its occupants singing along to their favorite songs on the radio.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.5017636\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the raw text\n",
    "# Split to then use it to update the vector store\n",
    "data_1, data_2 = TextLoader('data/sample.csv').split_data()\n",
    "\n",
    "# Create an instance of a vector store from documents\n",
    "db = VectorStore(model_name=\"nomic-ai/nomic-embed-text-v1.5\", persist=True, persist_path=\"demo\")\n",
    "\n",
    "# Create the vector store with some of the data\n",
    "db.create_vector_store(data_1)\n",
    "\n",
    "# Define a querry\n",
    "query = \"I want to buy a car\"\n",
    "\n",
    "# Search for it in my vector store and return the 3 most similar results\n",
    "similar_vectors = db.query_similar_vectors(query, top_n=3)\n",
    "\n",
    "# Pritty print the most similar vectors with relative similarity score\n",
    "db.print_similar_vectors(similar_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the vetor store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mVector store updated successfully\u001b[0m\n",
      "\n",
      "\u001b[1mSimilar Text Retrived:\u001b[0m\n",
      "___________________________________\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m And as the sun rises once again, the cycle begins anew, a testament to the beauty and resilience of life.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.53241175\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m The Harley-Davidson motorcycle rumbled to life, its deep, throaty growl announcing its presence on the road.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.5212596\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m A vintage car gleamed in the sunlight, its polished chrome catching the eye of passersby.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.4940513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update the vector store\n",
    "db.update_vector_store(data_2)\n",
    "\n",
    "# Query the vector store\n",
    "query = \"I want to buy a cycle\"\n",
    "\n",
    "similar_vectors = db.query_similar_vectors(query, top_n=3)\n",
    "db.print_similar_vectors(similar_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mVector store deleted successfully\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Delete saved vector store\n",
    "db.delete_vector_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG (Retrival Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Imports\n",
    "\n",
    "- **Pypdf:** to read pdf files\n",
    "- **ollama:** to run mistral 7B (not MoE) locally fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DirectoryReader, CharacterSplitter\n",
    "\n",
    "Simple implementation of classes to load the text for the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectoryReader:\n",
    "    \"\"\"\n",
    "    Class to load all of the text from a directory with different filetypes\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "\n",
    "    # This supports pdfs and txt but can be extended if needed\n",
    "    # Furtheremore we could store the metadata for the source documents and not put all of the text together\n",
    "    def load_data(self):\n",
    "        # List all files in the data directory\n",
    "        files = os.listdir(self.data_path)\n",
    "        self.text = ''\n",
    "    \n",
    "        # Read the contents of each file\n",
    "        for file in files:\n",
    "            # Get the file path\n",
    "            file_path = os.path.join(self.data_path, file)\n",
    "            \n",
    "            if file.endswith('.pdf'):\n",
    "                # load the pdf inside the text attribute\n",
    "                self._load_pfd(file_path)\n",
    "            elif file.endswith('.txt'):\n",
    "                # load the txt inside the text attribute\n",
    "                self._load_txt(file_path)\n",
    "            else:\n",
    "                print(f\"File type not supported for: {file}\")\n",
    "    \n",
    "        return self.text\n",
    "\n",
    "    def _load_pfd(self, file_path):\n",
    "        with open(file_path, 'rb') as f:  # Open the file in binary mode\n",
    "            pdf = PdfReader(f)  # Create a PdfReader object\n",
    "            for page in pdf.pages:\n",
    "                self.text += page.extract_text()\n",
    "        \n",
    "    def _load_txt(self, file_path):\n",
    "        with open(file_path, 'r') as txt_file:  # Open the file in text mode\n",
    "            self.text += txt_file.read()\n",
    "        \n",
    "class CharacterSplitter:\n",
    "    \"\"\"\n",
    "    Class that split text into chunks that can be used to create a vector store\n",
    "    \"\"\"\n",
    "    def __init__(self, chunk_size=100, chunk_overlap=0):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def split_documents(self, raw_documents):\n",
    "        # Get the rappresentation for every charcter\n",
    "        self.data = list(raw_documents)\n",
    "        # Slit the text\n",
    "        self.split_text()\n",
    "        return self.chunks\n",
    "\n",
    "    def split_text(self):\n",
    "        self.chunks = []\n",
    "        chunk_size = self.chunk_size - self.chunk_overlap  # Adjust chunk size to account for overlap\n",
    "    \n",
    "        for i in range(0, len(self.data), chunk_size):\n",
    "            # Ensure the chunk size doesn't exceed the length of the data\n",
    "            if i + self.chunk_size > len(self.data):\n",
    "                chunk = self.data[i:]\n",
    "            else:\n",
    "                chunk = self.data[i:i + self.chunk_size]\n",
    "\n",
    "            # Join the characters together\n",
    "            self.chunks.append(\"\".join(chunk))\n",
    "    \n",
    "        # Adjust the last chunk to include the overlap\n",
    "        if self.chunk_overlap > 0:\n",
    "            for i in range(1, len(self.chunks)):\n",
    "                self.chunks[i] = self.chunks[i-1][-self.chunk_overlap:] + self.chunks[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the vector store and showcase of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File type not supported for: sample.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mVector store created successfully\u001b[0m\n",
      "\n",
      "\u001b[1mSimilar Text Retrived:\u001b[0m\n",
      "___________________________________\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m 8 (signiﬁcant with p<0.000014 ). This is suggestive that grokking may only happen after\n",
      "the network’8 (signiﬁcant with p<0.000014 ). This is suggestive that grokking may only happen after\n",
      "the network’s parameters are in ﬂatter regions of the loss landscape. It would be valuable for future\n",
      "work to explore this hypothesis, as well as test other generalization measures.\n",
      "Figure 7: Networks trained on the S5composition objective appear to only grok in relatively ﬂat\n",
      "regions of the loss landscape.\n",
      "10\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.5970477\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m , long after severely overﬁtting, validation accuracy sometimes suddenly\n",
      "begins to increase from cha, long after severely overﬁtting, validation accuracy sometimes suddenly\n",
      "begins to increase from chance level toward perfect generalization. We call this phenomenon\n",
      "‘grokking’. An example is shown in Figure 1.\n",
      "• We present the data efﬁciency curves for a variety of binary operations.\n",
      "•We show empirically that the amount of optimization required for generalization quickly\n",
      "increases as the dataset size decreases.\n",
      "•We compare various optimization details to measure their impact on data efﬁciency. We ﬁnd\n",
      "that weight decay is particularly effective at improving generalization on the tasks we study.\n",
      "•We visualize the symbol embeddings learned by these networks and ﬁnd that they sometimes\n",
      "uncover recognizable structure of the mathematical objects represented by the symbols.\n",
      "2 M ETHOD\n",
      "All of our experiments used a small transformer trained on datasets of equations of the form a◦b=c,\n",
      "where each of “ a”, “◦”, “b”, “=”, and “c” is a separate token. Details of the operations studied, the\n",
      "architect\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.5149054\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m quire an extremely large number of samples\n",
      "to master.\n",
      "In Jiang et al. (2019) they studied a large nuquire an extremely large number of samples\n",
      "to master.\n",
      "In Jiang et al. (2019) they studied a large number of generalization or complexity measures on\n",
      "convolutional neural networks to see which, if any, are predictive of generalization performance.\n",
      "They ﬁnd that ﬂatness based measures that aim to quantify the sensitivity of the trained neural network\n",
      "to parameter perturbations are the most predictive. We conjectured that the grokking phenomena\n",
      "8we report in this work may be due to the noise from SGD driving the optimization to ﬂatter/simpler\n",
      "solutions that generalize better and hope to investigate in future work whether any of these measures\n",
      "are predictive of grokking.\n",
      "Zhang et al. (2016) ﬁnds that neural networks of sizes typically used in deep learning can interpolate\n",
      "arbitrary training data, and yet generalize when trained with semantically meaningful labels using\n",
      "appropriate optimization procedures. Our work shows a related phenomenon where neural networks\n",
      "can interpolate a small alg\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.4754515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all of the raw text \n",
    "raw_text = DirectoryReader('data/').load_data()\n",
    "\n",
    "# Characther spillter initializatoin\n",
    "text_splitter = CharacterSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# Split the documents into chunks\n",
    "data = text_splitter.split_documents(raw_text)\n",
    "\n",
    "#print(data[-2])\n",
    "#print(\"\\n____________________________________________________\\n\")\n",
    "# print(data[-1])\n",
    "\n",
    "# Initialize an instance of a vector store without persitence\n",
    "db = VectorStore(model_name=\"nomic-ai/nomic-embed-text-v1.5\", persist=False)\n",
    "\n",
    "# Create the vector store\n",
    "db.create_vector_store(data)\n",
    "\n",
    "# Define a querry and search for it in my vector store\n",
    "query = \"When does grooking happen ?\"\n",
    "similar_vectors = db.query_similar_vectors(query, top_n = 3)\n",
    "db.print_similar_vectors(similar_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral without RAG\n",
    "\n",
    "Running the small model on the querry gives us poor results that are not grounded in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm assuming you meant \"growing\" instead of \"grooking.\" Growth is a continuous process that happens throughout the lifespan of an organism, from the smallest microbe to the largest tree or human being. It can refer to physical growth in size or weight, as well as growth in knowledge, skills, or experience.\n",
      "\n",
      "Growth can occur at different rates and in different ways depending on the specific organism and its environment. For example, a plant may grow rapidly during the spring and summer months when it has access to plenty of sunlight, water, and nutrients, while a human being may experience more rapid growth during childhood than during adulthood.\n",
      "\n",
      "In general, however, growth is a lifelong process that happens constantly, both in nature and in human development."
     ]
    }
   ],
   "source": [
    "stream = ollama.chat(\n",
    "    model='mistral',\n",
    "    messages=[{'role': 'user', 'content': f\"{query}\"}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral with RAG\n",
    "\n",
    "Running the model by passing to it the context provide us much more accurate results. This is what we were looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The term \"grokking\" refers to a sudden increase in validation accuracy of a neural network after severely overfitting on the training data, which is suggestive of generalization from the learned representations. According to the text provided, this phenomenon seems to occur when the network's parameters are in relatively flat regions of the loss landscape, which can be long after the network has overfitted significantly.\n",
      "\n",
      "However, it's important to note that there isn't a definitive answer to when grokking happens exactly, as it appears to be an empirical observation. The text suggests that further work is needed to explore this hypothesis and test other generalization measures in order to gain a better understanding of the phenomenon. Additionally, the text mentions that flatness-based measures that quantify the sensitivity of neural networks to parameter perturbations are predictive of generalization performance, and it's conjectured that the noise from stochastic gradient descent (SGD) during optimization may drive the optimization towards flatter/simpler solutions that generalize better.\n",
      "\n",
      "Overall, more research is needed to fully understand the nature of grokking and its relationship to neural network training and generalization performance."
     ]
    }
   ],
   "source": [
    "# Get only the text and put it all together\n",
    "context = ' '.join([text for text, _ in similar_vectors])\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model='mistral',\n",
    "    messages=[{'role': 'user', 'content': f\"Question: {query} \\n Context: {context}\"}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv-kernel",
   "language": "python",
   "name": "local-venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "139e7c84632f54486abb9d698f2a5412a324e85ce1b1331ea63d3255168fb27f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
