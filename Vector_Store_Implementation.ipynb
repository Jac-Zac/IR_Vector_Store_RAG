{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store Implementation\n",
    "\n",
    "This is an implementation of a vector store that can leverage embedding models to create our vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "- Numpy\n",
    "- Pands\n",
    "\n",
    "##### Using files from:\n",
    "\n",
    "http://ir.dcs.gla.ac.uk/resources/test_collections/time/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index (Helper class)\n",
    "\n",
    "Implementation of an Helper class index which is going to be used in my vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index:\n",
    "    def __init__(self, dim = None):\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Dictionary to store the vectors\n",
    "        self.stored_vectors = {}\n",
    "\n",
    "    def add_items(self, vectors, vectors_id) -> None:\n",
    "        \"\"\"\n",
    "        Update the indexing structure for the vector store\n",
    "        \"\"\"\n",
    "        for vector_id, vector in zip(vectors_id, vectors):\n",
    "            self.stored_vectors[vector_id] = vector\n",
    "\n",
    "    def find_similar_vectors(self, query_vector, top_n):\n",
    "        \"\"\"\n",
    "        Compute the top n similar vectors to the query vector\n",
    "\n",
    "        Args:\n",
    "            query_vector (numpy.ndarray): The query vector\n",
    "            top_n (int): The number of top similar vectors to return\n",
    "\n",
    "        Returns:\n",
    "            A list of the top n similar vectors\n",
    "        \"\"\"\n",
    "        # Add option for different similarity measures\n",
    "        similarities = self._cosine_similarity(query_vector)\n",
    "        \n",
    "        top_n_indices = np.argsort(similarities[0])[-top_n:][::-1]\n",
    "        return top_n_indices\n",
    "        # return [self.vectors[i] for i in top_n_indices]\n",
    "\n",
    "    def _cosine_similarity(self, query_vector) -> float:\n",
    "        \"\"\"\n",
    "        Compute the similarity between two vectors\n",
    "\n",
    "        Args:\n",
    "            query_vector (numpy.ndarray): The query vector\n",
    "            vector (numpy.ndarray): The vector to compare\n",
    "\n",
    "        Returns:\n",
    "            The dot product of the vectors, normalized by the product of their norms\n",
    "        \"\"\"\n",
    "\n",
    "        vectors = np.array(list(self.stored_vectors.values()))\n",
    "        \n",
    "        dot_product = np.dot(query_vector, vectors.T)\n",
    "        query_vector_norm = np.linalg.norm(query_vector)\n",
    "        vectors_norm = np.linalg.norm(vectors, axis=1)\n",
    "        similarities = dot_product / (query_vector_norm * vectors_norm)\n",
    "        \n",
    "        return similarities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, vector_dimension = None, persist=True, persist_path=\"vector_store\", metric=\"cosine\"):\n",
    "\n",
    "        if vector_dimension is None:\n",
    "            print(\"You should pass the vector size\")\n",
    "\n",
    "        # Initialize our index our index\n",
    "        self.index = Index(dim=vector_dimension)\n",
    "\n",
    "        # Persistence\n",
    "        self.persist = persist\n",
    "        self.persist_path = persist_path\n",
    "        \n",
    "        self.vector_dimension = vector_dimension\n",
    "        \n",
    "        # A dictionary for indexing structure for retrieval\n",
    "        self.similarity_index = {}\n",
    "        \n",
    "        # Counter to then store the ids of vectors\n",
    "        self.id_counter : int = 0\n",
    "        \n",
    "        # Dictionary to store sentences corresponding to vectors\n",
    "        self.sentences = {}\n",
    "\n",
    "\n",
    "    def _load_vector_store(self):\n",
    "        index_file = os.path.join(self.persist_path, \"index.pkl\")\n",
    "        sentences_file = os.path.join(self.persist_path, \"sentences.pkl\")\n",
    "        \n",
    "        if not (os.path.exists(index_file) and os.path.exists(sentences_file)):\n",
    "            raise FileNotFoundError(\"Index and sentences files not found in the specified directory.\")\n",
    "\n",
    "        with open(index_file, \"rb\") as f:\n",
    "            self.index = pickle.load(f)\n",
    "        with open(sentences_file, \"rb\") as f:\n",
    "            self.sentences = pickle.load(f)\n",
    "\n",
    "        return self.index, self.sentences\n",
    "\n",
    "    def _save_vector_store(self):\n",
    "        \"\"\"\n",
    "        Save the index and corresponding sentences\n",
    "        \"\"\"\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(self.persist_path, exist_ok=True)\n",
    "        \n",
    "        # Serialize and save the index\n",
    "        with open(os.path.join(self.persist_path, \"index.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.index, f)\n",
    "\n",
    "        # Serialize and save the sentences\n",
    "        with open(os.path.join(self.persist_path, \"sentences.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.sentences, f)\n",
    "    \n",
    "    def update_vector_store(self, new_sentence_vectors):\n",
    "        \"\"\"\n",
    "        Update the existing vector store with new vectors\n",
    "\n",
    "        new_id_vectors: Dictionary containing new vectors to be added\n",
    "        persist_path: Path to the directory where the existing vector store is saved\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # load existing index and sentences\n",
    "            self.index, self.sentences = self._load_vector_store()\n",
    "\n",
    "            # Update the id counter\n",
    "            self.id_counter = max(self.sentences.keys()) + 1\n",
    "\n",
    "            # Add new vectors to the index and sentences\n",
    "            vectors = []\n",
    "            ids = []\n",
    "            for sentence, vector in new_sentence_vectors.items():\n",
    "                vectors.append(vector)\n",
    "                ids.append(self.id_counter)\n",
    "                self.sentences[self.id_counter] = sentence\n",
    "                self.id_counter += 1\n",
    "                \n",
    "            # Adding the vectors, idnex to the our index\n",
    "            self.index.add_items(vectors, ids)\n",
    "            \n",
    "            print(\"Vector store updated successfully\", end=\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def create_vector_store(self, new_sentence_vectors):\n",
    "        \"\"\"\n",
    "        Add vectors to the vector store\n",
    "\n",
    "        id: the unique id for the vector\n",
    "        vecotor: the vector to be added\n",
    "        \"\"\"\n",
    "        try:\n",
    "            vectors = []\n",
    "            ids = []\n",
    "            for sentence, vector in new_sentence_vectors.items():\n",
    "                # Append the new vector\n",
    "                vectors.append(vector)\n",
    "                # Assing a unique integer id to every vecotr\n",
    "                ids.append(self.id_counter)\n",
    "                # Store the sentence\n",
    "                self.sentences[self.id_counter] = sentence\n",
    "                # Incremeant the counter for the next vector\n",
    "                self.id_counter += 1\n",
    "\n",
    "            # Adding the items to the index\n",
    "            self.index.add_items(vectors, ids)\n",
    "    \n",
    "            if self.persist:       \n",
    "                 self._save_vector_store()\n",
    "                \n",
    "            print(\"Vector store created successfully\", end=\"\\n\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def get_similar_vectors(self, query_vector, num_results=5) -> list:\n",
    "        \"\"\"\n",
    "        Find similar vectors to the query vector\n",
    "\n",
    "        Args:\n",
    "            query_vector (numpy.ndarray): The query vector to compare with the vecotr in the store\n",
    "            num_results (int): The number of similar vectors to return\n",
    "            persist_path: Path to the directory where the existing vector store is saved\n",
    "\n",
    "        Returns:\n",
    "            A list of tuples, each containing a vector id and its similarity to the query vector\n",
    "        \"\"\"\n",
    "        # load existing index and sentences\n",
    "        self.index, self.sentences = self._load_vector_store()\n",
    "        \n",
    "        result = []\n",
    "        # labels, distances = self.index.find_similar_vectors(query_vector, k=top_n)\n",
    "        labels = self.index.find_similar_vectors(query_vector, top_n=num_results)\n",
    "\n",
    "        return labels\n",
    "        # similar_vectors = [(self.sentences[label], distance) for label, distance in zip(labels[0], distances[0])]\n",
    "        \n",
    "        \"\"\" \n",
    "        for vector_id, vector in self.stored_vectors.items():\n",
    "            similarity = self._compute_similarity(query_vector, vector)\n",
    "            result.append((vector_id, similarity))\n",
    "            \n",
    "        # Sort by the similarity in descending order\n",
    "        result.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return the num_results most similar ones\n",
    "        return result[:num_results]\n",
    "        \"\"\"\n",
    "        \n",
    "        # return similar_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo of The vector store\n",
    "\n",
    "Using nomic embed for the demo and a custom index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created successfully\n",
      "\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the Embedding Model\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
    "\n",
    "# Get data from csv\n",
    "data = pd.read_csv(\"data.csv\", delimiter=\";\")\n",
    "\n",
    "# Take 80% of the data to create the vector store, remaining 20% to update the vector store\n",
    "# To showcase the ability of updating the vector store\n",
    "data_1 = data.sample(frac=0.8, random_state=200)\n",
    "data_2 = data.drop(data_1.index)\n",
    "\n",
    "# Convert data to list\n",
    "# Instead of doing this I could decide to split documents in a smarter way with overlap or something\n",
    "\n",
    "data_1 = data_1[\"text\"].tolist()\n",
    "\n",
    "# Encode the data, have data and vector in dictionary, data keys, vector values\n",
    "vectors_1 = model.encode(data_1)\n",
    "\n",
    "# Get the vector dimension\n",
    "vector_dimension = len(vectors_1[0])\n",
    "\n",
    "# Create a dictionary with id and vectors\n",
    "new_sentence_vectors_1 = {data_1[i]: vectors_1[i] for i in range(len(data_1))}\n",
    "\n",
    "# Create a vector store. Select metric 'cosine' is the one I implemented (other possibility 'l1' or 'ip')\n",
    "vector_store = VectorStore(vector_dimension, metric=\"cosine\")\n",
    "\n",
    "# Create the vector store, set persist to True to save the vector store on disk\n",
    "# Implment persitence in the future\n",
    "vector_store.create_vector_store(new_sentence_vectors_1)\n",
    "\n",
    "# Query the vector store\n",
    "query = \"I want to buy a car\"\n",
    "query_vector = model.encode(query)\n",
    "# similar_vectors = vector_store.get_similar_vectors(query_vector, num_results=5)\n",
    "index = vector_store.get_similar_vectors(query_vector, num_results=5)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Visualize the most similar vectors\n",
    "print(\"Similarity Vectors:\")\n",
    "for sentence, similarity_score in similar_vectors:\n",
    "    print(f\"- Sentence: {sentence}\")\n",
    "    print(f\"  Similarity Score: {similarity_score}\")\n",
    "    print()\n",
    "\"\"\"\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding update of the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'persist_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m vectors_2 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(data_2)\n\u001b[1;32m      4\u001b[0m new_sentence_vectors_2 \u001b[38;5;241m=\u001b[39m {data_2[i]: vectors_2[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data_2))}\n\u001b[0;32m----> 5\u001b[0m \u001b[43mvector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_vector_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_sentence_vectors_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Query the vector store\u001b[39;00m\n\u001b[1;32m      8\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI want to buy a cycle\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 83\u001b[0m, in \u001b[0;36mVectorStore.update_vector_store\u001b[0;34m(self, new_sentence_vectors)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVector store updated successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[3], line 64\u001b[0m, in \u001b[0;36mVectorStore.update_vector_store\u001b[0;34m(self, new_sentence_vectors)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mUpdate the existing vector store with new vectors\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mnew_id_vectors: Dictionary containing new vectors to be added\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03mpersist_path: Path to the directory where the existing vector store is saved\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# load existing index and sentences\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_vector_store(\u001b[43mpersist_path\u001b[49m)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Update the id counter\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentences\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'persist_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Update the vector store\n",
    "data_2 = data_2[\"text\"].tolist()\n",
    "vectors_2 = model.encode(data_2)\n",
    "new_sentence_vectors_2 = {data_2[i]: vectors_2[i] for i in range(len(data_2))}\n",
    "vector_store.update_vector_store(new_sentence_vectors_2)\n",
    "\n",
    "# Query the vector store\n",
    "query = \"I want to buy a cycle\"\n",
    "query_vector = model.encode(query)\n",
    "similar_vectors = vector_store.get_similar_vectors(query_vector, top_n=5)\n",
    "print(\"Similarity Vectors:\")\n",
    "for sentence, similarity_score in similar_vectors:\n",
    "    print(f\"- Sentence: {sentence}\")\n",
    "    print(f\"  Similarity Score: {similarity_score}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding persistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the system on a set of test queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old way of using it\n",
    "\n",
    "Might be helpfull as a look up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VectorStore instance\n",
    "vector_store = VectorStore()\n",
    "\n",
    "# Define your sentences\n",
    "sentences = [\n",
    "    \"I eat mango\",\n",
    "    \"mango is my favorite fruit\",\n",
    "    \"mango, apple, oranges are fruits\",\n",
    "    \"fruits are good for health\",\n",
    "]\n",
    "\n",
    "# Tokenization and Vocabulary Creation\n",
    "vocabulary = set()\n",
    "for sentence in sentences:\n",
    "    tokens = sentence.lower().split()\n",
    "    vocabulary.update(tokens)\n",
    "\n",
    "# Assign unique indices to words in the vocabulary\n",
    "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "\n",
    "# Vectorization\n",
    "sentence_vectors = {}\n",
    "for sentence in sentences:\n",
    "    tokens = sentence.lower().split()\n",
    "    vector = np.zeros(len(vocabulary))\n",
    "    for token in tokens:\n",
    "        vector[word_to_index[token]] += 1\n",
    "    sentence_vectors[sentence] = vector\n",
    "\n",
    "# Storing in VectorStore\n",
    "for sentence, vector in sentence_vectors.items():\n",
    "    vector_store.add_vector(sentence, vector)\n",
    "\n",
    "# Searching for Similarity\n",
    "query_sentence = \"Mango is the best fruit\"\n",
    "query_vector = np.zeros(len(vocabulary))\n",
    "query_tokens = query_sentence.lower().split()\n",
    "for token in query_tokens:\n",
    "    if token in word_to_index:\n",
    "        query_vector[word_to_index[token]] += 1\n",
    "\n",
    "similar_sentences = vector_store.find_similar_vectors(query_vector, num_results=2)\n",
    "\n",
    "# Print similar sentences\n",
    "print(\"Query Sentence:\", query_sentence)\n",
    "print(\"Similar Sentences:\")\n",
    "for sentence, similarity in similar_sentences:\n",
    "    print(f\"{sentence}: Similarity = {similarity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv-kernel",
   "language": "python",
   "name": "local-venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "139e7c84632f54486abb9d698f2a5412a324e85ce1b1331ea63d3255168fb27f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
