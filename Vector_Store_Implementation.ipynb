{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store Implementation + RAG\n",
    "\n",
    "The following code is an implementation made for the final exam of the Information Retrival course.\n",
    "\n",
    "**Author:** *Jacopo Zacchigna*\n",
    "\n",
    "<img src=\"https://images.contentstack.io/v3/assets/bltefdd0b53724fa2ce/blt185ef72de6dc0e43/6466a9a1f21a3540facf75ac/vector-search-diagram-cropped-white-space.png\" width=\"75%\" height=\"75%\">\n",
    "\n",
    "---\n",
    "\n",
    "The notebook is an implementation of a vector store.\n",
    "The code is structured in multiple class:\n",
    "\n",
    "- Index\n",
    "- VectorStore\n",
    "\n",
    "Furtheremore I also implemented some additional classes to test the vector store and also for the RAG:\n",
    "\n",
    "- TextLoader\n",
    "- DirectoryReader\n",
    "- TextSplitter\n",
    "\n",
    "---\n",
    "\n",
    "##### The text for the RAG demo is taken from:\n",
    "\n",
    "- Grokking Paper: https://arxiv.org/pdf/2201.02177.pdf\n",
    "- Attention Is All You Need: https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Imports external libraries for the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Index (Helper class)\n",
    "\n",
    "Implementation of an Helper class index which is going to be used in my vector Store\n",
    "\n",
    "- **Add items:** to add all of the vectors with the relative indices to the stored_vectors dictionary\n",
    "- **knn_query:** to get the `top_n` most similar vectors inside a vector store with the relative\n",
    "- **_cosine_similarity:** helper function to compute the cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index:\n",
    "    def __init__(self, dim=None):\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Dictionary to store the vectors\n",
    "        self.stored_vectors = {}\n",
    "\n",
    "    def add_items(self, vectors, vectors_id: int):\n",
    "        \"\"\"\n",
    "        Update the indexing structure for the vector store\n",
    "        \"\"\"\n",
    "        for vector_id, vector in zip(vectors_id, vectors):\n",
    "            if vector.shape != (self.dim,):\n",
    "                raise ValueError(\"Vectors must have shape (dim,)\")\n",
    "            self.stored_vectors[vector_id] = vector\n",
    "\n",
    "    def knn_query(self, query_vector: np.ndarray, top_n: int = 5):\n",
    "        \"\"\"\n",
    "        Find the top n similar vectors to the query vector using cosine similarity.\n",
    "\n",
    "        Args:\n",
    "            query_vector (numpy.ndarray): The query vector.\n",
    "            top_n (int): The number of top similar vectors to return.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of two numpy arrays: the first array contains the indices of the top n similar vectors,\n",
    "            and the second array contains the corresponding cosine similarity scores.\n",
    "        \"\"\"\n",
    "        # For every vector in the vector store compute the similarity and create a tuple with the relative index (int)\n",
    "        similarities = [(index, self._cosine_similarity(query_vector, vector)) for index, vector in self.stored_vectors.items()]\n",
    "\n",
    "        # Sort based on the similarity (second element of the vector) and take the top_n most similar vectors\n",
    "        \n",
    "        # Then zip: [(index, similarity), (index, similarity) ...] -> ([index, index, ...], [similarity, similarity, ...])\n",
    "        top_n_indices, top_n_similarities = zip(*sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n])\n",
    "\n",
    "        return top_n_indices, top_n_similarities\n",
    "        \n",
    "    def _cosine_similarity(self, query_vector, vector) -> float:\n",
    "        \"\"\"\n",
    "        Compute the similarity between two vectors\n",
    "\n",
    "        Args:\n",
    "            query_vector (numpy.ndarray): The query vector\n",
    "            vector (numpy.ndarray): The vector to compare\n",
    "\n",
    "        Returns:\n",
    "            The dot product of the vectors, normalized by the product of their norms\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute the dot product between the two vectors\n",
    "        dot_product = np.dot(query_vector, vector)\n",
    "\n",
    "        # Normalization values\n",
    "        query_vector_norm = np.linalg.norm(query_vector)\n",
    "        vector_norm = np.linalg.norm(vector)\n",
    "\n",
    "        # Return the similarity\n",
    "        return dot_product / (query_vector_norm * vector_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Vector Store class\n",
    "\n",
    "This is the main part of the code which implements the vector store.\n",
    "\n",
    "*It focueses on implementing the following function with some additional functionality and some basic error handling:*\n",
    "\n",
    "- **_load_vector_store:** loads the index and sentences\n",
    "\n",
    "- **save_vector_store:** saves the index and sentences to the specified directory\n",
    "\n",
    "- **create_vector_store:** adds vectors to the vector store\n",
    "\n",
    "- **update_vector_store:** updates the existing vector store with new vectors\n",
    "\n",
    "- **delete_vector_store:** deletes a persistent vector store\n",
    "\n",
    "- **query_similar_vectors:** finds similar vectors to the query vector based on cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, model_name=\"nomic-ai/nomic-embed-text-v1.5\", persist=True, persist_path=\"vector_store\"):\n",
    "        self.persist = persist\n",
    "        self.persist_path = persist_path\n",
    "        \n",
    "        # Initialize our index our index\n",
    "        self.model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "        \n",
    "        # Counter to then store the ids of vectors\n",
    "        self.id_counter = 0\n",
    "\n",
    "        # Dictionary to store chunk corresponding to vectors\n",
    "        self.text_chunks = {}\n",
    "\n",
    "    def save_vector_store(self):\n",
    "        # In the case the vector store was created without persistence\n",
    "        if not self.persist:\n",
    "            # Set it to be persitentxw\n",
    "            self.persist = True\n",
    "    \n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(self.persist_path, exist_ok=True)\n",
    "    \n",
    "        # Serialize and save the index\n",
    "        with open(os.path.join(self.persist_path, \"index.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.index, f)\n",
    "    \n",
    "        # Serialize and save the text_chunks\n",
    "        with open(os.path.join(self.persist_path, \"text_chunks.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.text_chunks, f)\n",
    "\n",
    "    def create_vector_store(self, text_chunks):\n",
    "        # Get the embeddings\n",
    "        embeddings = self.model.encode(text_chunks)\n",
    "        self.embeddings_dimension = len(embeddings[0])\n",
    "\n",
    "        # Create the index with the dimensionality of the vector I got\n",
    "        self.index = Index(dim=self.embeddings_dimension)\n",
    "\n",
    "        # Create a dictionary with the documents and the relative embeddings\n",
    "        chunks_embeddings = {text_chunks[i]: embeddings[i] for i in range(len(text_chunks))}\n",
    "        \n",
    "        try:\n",
    "            vectors = []\n",
    "            ids = []\n",
    "            \n",
    "            for chunk, vector in chunks_embeddings.items():\n",
    "                # Append the new vector\n",
    "                vectors.append(vector)\n",
    "                # Assign a unique integer id to every vector\n",
    "                ids.append(self.id_counter)\n",
    "                # Store the text chunks\n",
    "                self.text_chunks[self.id_counter] = chunk\n",
    "                # Increment the counter for the next vector\n",
    "                self.id_counter += 1\n",
    "                \n",
    "            # Adding the items to the index\n",
    "            self.index.add_items(vectors, ids)\n",
    "\n",
    "            if self.persist:\n",
    "                self.save_vector_store()\n",
    "\n",
    "            print(\"\\033[32mVector store created successfully\\033[0m\", end=\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def update_vector_store(self, text_chunks):\n",
    "        \"\"\"\n",
    "        Update the existing vector store with new documents\n",
    "\n",
    "        documents: List of documents to add to my vector store\n",
    "        \"\"\"\n",
    "        embeddings = self.model.encode(text_chunks)\n",
    "        chunks_embeddings = {text_chunks[i]: embeddings[i] for i in range(len(text_chunks))}\n",
    "\n",
    "        try:\n",
    "            if self.persist:\n",
    "                # Load existing index and text_chunks\n",
    "                self.index, self.text_chunks = self._load_vector_store()\n",
    "\n",
    "            # Get the max for the counter and start from the next one\n",
    "            self.id_counter = max(self.text_chunks.keys()) + 1\n",
    "\n",
    "            # Add new vectors to the index and text_chunks\n",
    "            vectors = []\n",
    "            ids = []\n",
    "            for chunk, vector in chunks_embeddings.items():\n",
    "                vectors.append(vector)\n",
    "                ids.append(self.id_counter)\n",
    "                self.text_chunks[self.id_counter] = chunk\n",
    "                self.id_counter += 1\n",
    "\n",
    "            # Adding the vectors, index to the our index\n",
    "            self.index.add_items(vectors, ids)\n",
    "\n",
    "            print(\"\\033[32mVector store updated successfully\\033[0m\", end=\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def delete_vector_store(self) -> None:\n",
    "        \"\"\"\n",
    "        Delete a persistent vector store that was craeted\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if the directory exists\n",
    "            if os.path.exists(self.persist_path):\n",
    "                # Delete index and text_chunks files\n",
    "                os.remove(os.path.join(self.persist_path, \"index.pkl\"))\n",
    "                os.remove(os.path.join(self.persist_path, \"text_chunks.pkl\"))\n",
    "                os.rmdir(self.persist_path)\n",
    "                print(\"\\033[32mVector store deleted successfully\\033[0m\", end=\"\\n\\n\")\n",
    "            else:\n",
    "                print(\"Vector store does not exist\", end=\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def query_similar_vectors(self, query: str, top_n=5):\n",
    "        \"\"\"\n",
    "        Find similar vectors to the query\n",
    "\n",
    "        Args:\n",
    "            query (str): The query that is going to be searched for inside my vector store\n",
    "            num_results (int): The number of similar vectors to return\n",
    "\n",
    "        Returns:\n",
    "            A list of tuples, each containing a document and its similarity to the query vector\n",
    "        \"\"\"\n",
    "        if self.persist:\n",
    "            # Load existing index and text_chunks\n",
    "            self._load_vector_store()\n",
    "\n",
    "        # Use the same model to encode the query\n",
    "        query_vector = self.model.encode(query)\n",
    "        \n",
    "        # Querry for the top_n most similar vectors to my querry vector\n",
    "        top_n_indices, top_n_similarities = self.index.knn_query(query_vector, top_n=top_n)\n",
    "\n",
    "        # Return the most similar documents in a list of tuples with (text_chunks, similarity_score)\n",
    "        return [(self.text_chunks[index], similarities) for index, similarities in zip(top_n_indices, top_n_similarities)]\n",
    "        \n",
    "    def _load_vector_store(self):\n",
    "        index_file = os.path.join(self.persist_path, \"index.pkl\")\n",
    "        text_chunks_file = os.path.join(self.persist_path, \"text_chunks.pkl\")\n",
    "\n",
    "        if not os.path.exists(index_file) or not os.path.exists(text_chunks_file):\n",
    "            raise FileNotFoundError(\"Index and text_chunks files not found in the specified directory.\")\n",
    "\n",
    "        with open(index_file, \"rb\") as f:\n",
    "            self.index = pickle.load(f)\n",
    "        with open(text_chunks_file, \"rb\") as f:\n",
    "            self.text_chunks = pickle.load(f)\n",
    "\n",
    "        return self.index, self.text_chunks\n",
    "        \n",
    "    def print_similar_vectors(self, similar_vectors) -> None:\n",
    "        \"\"\"\n",
    "        Helper function to print the most similar vector with the relative similarity score in a nice way\n",
    "        \"\"\"\n",
    "        print(\"\\033[1mSimilar Text Retrived:\\033[0m\")\n",
    "        print(\"___________________________________\\n\")\n",
    "        for chunk, similarity_score in similar_vectors:\n",
    "            print(\"\\033[1m- Retrieved Text:\\033[0m\", chunk)\n",
    "            print(\"\\033[1m    Similarity Score:\\033[0m\", similarity_score)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo of The vector store\n",
    "\n",
    "Using nomic embed for the demo and a custom index. The demo showcase how to update a vector store and make queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLoader:\n",
    "    \"\"\"\n",
    "    Class to laod a csv file and split it into two splits\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load the data from a CSV file\n",
    "        return pd.read_csv(self.data_path, delimiter=\";\")\n",
    "\n",
    "    def split_data(self, split_ratio=0.8, random_state=1337):\n",
    "        # Load the data\n",
    "        data = self.load_data()\n",
    "\n",
    "        # Split them in two splits\n",
    "        data_1 = data.sample(frac=split_ratio, random_state=random_state)\n",
    "        data_2 = data.drop(data_1.index)\n",
    "\n",
    "        # Return them as a tuple of text lists\n",
    "        return data_1[\"text\"].tolist(), data_2[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store example\n",
    "\n",
    "Creation of the vector store, and simple query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mVector store created successfully\u001b[0m\n",
      "\n",
      "\u001b[1mSimilar Text Retrived:\u001b[0m\n",
      "___________________________________\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m A vintage car gleamed in the sunlight, its polished chrome catching the eye of passersby.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.5708912\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m The sleek, silver sports car raced down the winding mountain road, its engine roaring with power.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.52294475\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m A family sedan cruised along the highway, its occupants singing along to their favorite songs on the radio.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.5017636\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the raw text\n",
    "# Split to then use it to update the vector store\n",
    "data_1, data_2 = TextLoader('data/sample.csv').split_data()\n",
    "\n",
    "# Create an instance of a vector store from documents\n",
    "db = VectorStore(model_name=\"nomic-ai/nomic-embed-text-v1.5\", persist=True, persist_path=\"demo\")\n",
    "\n",
    "# Create the vector store with some of the data\n",
    "db.create_vector_store(data_1)\n",
    "\n",
    "# Define a querry\n",
    "query = \"I want to buy a car\"\n",
    "\n",
    "# Search for it in my vector store and return the 3 most similar results\n",
    "similar_vectors = db.query_similar_vectors(query, top_n=3)\n",
    "\n",
    "# Pritty print the most similar vectors with relative similarity score\n",
    "db.print_similar_vectors(similar_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Updating the vetor store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mVector store updated successfully\u001b[0m\n",
      "\n",
      "\u001b[1mSimilar Text Retrived:\u001b[0m\n",
      "___________________________________\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m And as the sun rises once again, the cycle begins anew, a testament to the beauty and resilience of life.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.53241175\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m The Harley-Davidson motorcycle rumbled to life, its deep, throaty growl announcing its presence on the road.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.5212596\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m A vintage car gleamed in the sunlight, its polished chrome catching the eye of passersby.\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.4940513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update the vector store\n",
    "db.update_vector_store(data_2)\n",
    "\n",
    "# Query the vector store\n",
    "query = \"I want to buy a cycle\"\n",
    "\n",
    "similar_vectors = db.query_similar_vectors(query, top_n=3)\n",
    "db.print_similar_vectors(similar_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Deleting the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mVector store deleted successfully\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Delete saved vector store\n",
    "db.delete_vector_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG (Retrival Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Additional Imports\n",
    "\n",
    "- **Pypdf:** to read pdf files\n",
    "- **ollama:** to run mistral 7B (not MoE) locally fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DirectoryReader\n",
    "\n",
    "Simple implementation of a class to load all of the test present in a directorly fro `pdf` and `txt` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectoryReader:\n",
    "    \"\"\"\n",
    "    Class to load all of the text from a directory with different filetypes\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "\n",
    "    # This supports pdfs and txt but can be extended if needed\n",
    "    # Furtheremore we could store the metadata for the source documents and not put all of the text together\n",
    "    def load_data(self):\n",
    "        # List all files in the data directory\n",
    "        files = os.listdir(self.data_path)\n",
    "        self.text = ''\n",
    "    \n",
    "        # Read the contents of each file\n",
    "        for file in files:\n",
    "            # Get the file path\n",
    "            file_path = os.path.join(self.data_path, file)\n",
    "            \n",
    "            if file.endswith('.pdf'):\n",
    "                # load the pdf inside the text attribute\n",
    "                self._load_pfd(file_path)\n",
    "            elif file.endswith('.txt'):\n",
    "                # load the txt inside the text attribute\n",
    "                self._load_txt(file_path)\n",
    "            else:\n",
    "                print(f\"File type not supported for: {file}\")\n",
    "    \n",
    "        return self.text\n",
    "\n",
    "    def _load_pfd(self, file_path):\n",
    "        with open(file_path, 'rb') as f:  # Open the file in binary mode\n",
    "            pdf = PdfReader(f)  # Create a PdfReader object\n",
    "            for page in pdf.pages:\n",
    "                self.text += page.extract_text()\n",
    "        \n",
    "    def _load_txt(self, file_path):\n",
    "        with open(file_path, 'r') as txt_file:  # Open the file in text mode\n",
    "            self.text += txt_file.read()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### RecursiveCharacterTextSplitter\n",
    "\n",
    "Implementation of a class that recursivly split the text starting from `\\n\\n` as separator getting to `''` whilest the chunk is smaller then the chunk size defined. This doesn't allow for overlap between chunks but that could potentially be implemented in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveTextSplitter:\n",
    "    \"\"\"\n",
    "    Class that splits text into chunks that can be used to create a vector store\n",
    "    \"\"\"\n",
    "    def __init__(self, chunk_size=100):\n",
    "        self.chunk_size = chunk_size\n",
    "        self._separators = [\"\\n\\n\", '\\n', \" \", \".\", \",\", \"\"]\n",
    "\n",
    "    def split_text(self, text: str):\n",
    "        \"\"\"Split incoming text and return chunks.\"\"\"\n",
    "        chunks = []\n",
    "\n",
    "        separator = self._get_separator(text)\n",
    "\n",
    "        # Split the text with the separator if empty use list to split the char\n",
    "        splits = text.split(separator) if separator else list(text)\n",
    "\n",
    "        # Find good splits\n",
    "        good_splits = []\n",
    "        for split in splits:\n",
    "            # if the split is good because it is smaller then chunk size\n",
    "            if len(split) <= self.chunk_size:\n",
    "                good_splits.append(split)\n",
    "            else:\n",
    "                # Recursively split the chunk if it was larger with another separator\n",
    "                merged_chunks = self.split_text(split)\n",
    "                chunks.extend(merged_chunks)\n",
    "\n",
    "        # Try to merge splits together to still be smaller then the chunk size\n",
    "        if good_splits:\n",
    "            merged_chunks = self._merge_splits(good_splits)\n",
    "            chunks.extend(merged_chunks)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def _get_separator(self, text: str) -> str:\n",
    "        \"\"\"Generator that returns the different separators\"\"\"\n",
    "        return next((s for s in self._separators if s in text), \"\")\n",
    "\n",
    "    def _merge_splits(self, splits):\n",
    "        \"\"\"Merge the splits if the cumulative length is less than the chunk size.\"\"\"\n",
    "        merged_chunks = []\n",
    "        current_chunk = ''\n",
    "\n",
    "        for split in splits:\n",
    "            if len(current_chunk) + len(split) > self.chunk_size:\n",
    "                merged_chunks.append(current_chunk)\n",
    "                current_chunk = split\n",
    "            else:\n",
    "                current_chunk += self._get_separator(current_chunk) + split\n",
    "\n",
    "        # Append the new chunk\n",
    "        merged_chunks.append(current_chunk)\n",
    "        return merged_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the vector store and showcase of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File type not supported for: sample.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mVector store created successfully\u001b[0m\n",
      "\n",
      "\u001b[1mSimilar Text Retrived:\u001b[0m\n",
      "___________________________________\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m approximately half of them achieved high validation accuracy. We then used the method described in Keskar et al. (2016) to calculate the sharpness approximation value, φ. We found that the validation accuracy and the φscore across our trained networks had Spearman correlation coefﬁcient of −0.79548 (signiﬁcant with p<0.000014 ). This is suggestive that grokking may only happen after the network’s parameters are in ﬂatter regions of the loss landscape. It would be valuable for future work to explore this hypothesis, as well as test other generalization measures. Figure 7: Networks trained on the S5composition objective appear to only grok in relatively ﬂat regions of the loss landscape. 10\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.5271473\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m binary op tables. •We show that, long after severely overﬁtting, validation accuracy sometimes suddenly begins to increase from chance level toward perfect generalization. We call this phenomenon ‘grokking’. An example is shown in Figure 1. • We present the data efﬁciency curves for a variety of binary operations. •We show empirically that the amount of optimization required for generalization quickly increases as the dataset size decreases. •We compare various optimization details to measure their impact on data efﬁciency. We ﬁnd that weight decay is particularly effective at improving generalization on the tasks we study. •We visualize the symbol embeddings learned by these networks and ﬁnd that they sometimes uncover recognizable structure of the mathematical objects represented by the symbols. 2 M ETHOD All of our experiments used a small transformer trained on datasets of equations of the form a◦b=c,\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.48582044\n",
      "\n",
      "\u001b[1m- Retrieved Text:\u001b[0m sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks. 15GROKKING : G ENERALIZATION BEYOND OVERFIT - TING ON SMALL ALGORITHMIC DATASETS Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin OpenAIVedant Misra∗ Google ABSTRACT In this paper we propose to study generalization of neural networks on small al- gorithmically generated datasets. In this setting, questions about data efﬁciency, memorization, generalization, and speed of learning can be studied in great de- tail. In some situations we show that neural networks learn through a process of “grokking” a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in general- ization can happen well past the point of overﬁtting. We also study generalization as a function of dataset size and ﬁnd that smaller datasets require increasing amounts\n",
      "\u001b[1m    Similarity Score:\u001b[0m 0.47691286\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all of the raw text \n",
    "raw_text = DirectoryReader('data/').load_data()\n",
    "\n",
    "# Characther spillter initialization\n",
    "text_splitter = RecursiveTextSplitter(chunk_size=1000)\n",
    "\n",
    "# Split the documents into chunks\n",
    "data = text_splitter.split_text(raw_text)\n",
    "\n",
    "# Initialize an instance of a vector store without persitence\n",
    "db = VectorStore(model_name=\"nomic-ai/nomic-embed-text-v1.5\", persist=False)\n",
    "\n",
    "# Create the vector store\n",
    "db.create_vector_store(data)\n",
    "\n",
    "# Define a querry and search for it in my vector store\n",
    "query = \"When does grooking happen ?\"\n",
    "similar_vectors = db.query_similar_vectors(query, top_n = 3)\n",
    "db.print_similar_vectors(similar_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral without RAG\n",
    "\n",
    "Running the small model on the querry gives us poor results that are not grounded in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think there may be a bit of confusion here!\n",
      "\n",
      "Grooking is not actually a real thing that happens. It seems you might have made a typo or created a made-up word.\n",
      "\n",
      "If you meant to ask about something else, feel free to clarify or rephrase your question, and I'll do my best to help!"
     ]
    }
   ],
   "source": [
    "stream = ollama.chat(\n",
    "    model='llama3',\n",
    "    messages=[{'role': 'user', 'content': f\"{query}\"}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral with RAG\n",
    "\n",
    "Running the model by passing to it the context provide us much more accurate results. This is what we were looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grokking happens when a neural network suddenly begins to increase its validation accuracy from chance level towards perfect generalization, even after severe overfitting. This phenomenon was observed in the studies referenced, particularly in Figure 1, where an example of this sudden improvement is shown."
     ]
    }
   ],
   "source": [
    "# Get only the text and put it all together with spaces in the middle\n",
    "context = '\\n\\n'.join([f\"Source {i}: {text}\" for i, (text, _) in enumerate(similar_vectors)])\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model='llama3',\n",
    "    messages=[{'role': 'user', 'content': f\"Question: {query}\\n\\n Context: {context}\"}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the context provided in the sources you have given, it appears that \"grokking\" is a phenomenon where validation accuracy suddenly increases toward perfect generalization after severely overfitting the data. The exact timing of when this happens is not explicitly stated in the sources, but they suggest that it may occur long after the network's parameters are in flatter regions of the loss landscape and require optimization for generalization to become effective. However, further research is needed to explore this hypothesis and test other generalization measures as suggested by the authors."
     ]
    }
   ],
   "source": [
    "# Get only the text and put it all together with spaces in the middle\n",
    "context = '\\n\\n'.join([f\"Source {i}: {text}\" for i, (text, _) in enumerate(similar_vectors)])\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model='mistral',\n",
    "    messages=[{'role': 'user', 'content': f\"Question: {query}\\n\\n Context: {context}\"}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv-kernel",
   "language": "python",
   "name": "local-venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "139e7c84632f54486abb9d698f2a5412a324e85ce1b1331ea63d3255168fb27f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
